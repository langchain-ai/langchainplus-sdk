{"messages": [{"role": "user", "content": "How do I use a RecursiveUrlLoader to load content from a page?"}]}
{"messages": [{"role": "user", "content": "How do I use a RecursiveUrlLoader to load content from a page?"}]}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [], "answer": "", "query": "", "num_response_attempts": 0}}
{"input": [{"role": "system", "content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful."}, {"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "How do I use a RecursiveUrlLoader to load content from a page?", "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_QVF5RSFXkFAO4UG2GKDlX8bS", "function": {"arguments": "{\"steps\":[\"Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.\",\"Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.\",\"Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-521dde06-84f7-4bbf-ae8a-dfb4af6a538b", "example": false, "tool_calls": [{"name": "Plan", "args": {"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}, "id": "call_QVF5RSFXkFAO4UG2GKDlX8bS", "type": "tool_call"}], "invalid_tool_calls": []}}
{"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": "delete", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": [], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"question": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader."}
{"question": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader."}
{"input": {"question": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_TAc7T2pGmw37TvLwrxfs5SFq", "function": {"arguments": "{\"queries\":[\"RecursiveUrlLoader functionality documentation\",\"RecursiveUrlLoader use cases and examples\",\"Understanding RecursiveUrlLoader in Conceptual docs\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-0bea5009-22ad-4bad-905b-9432ba8246d7", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}, "id": "call_TAc7T2pGmw37TvLwrxfs5SFq", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}
{"input": {"question": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"], "documents": []}}
{"input": {"query": "RecursiveUrlLoader functionality documentation"}}
{"query": "RecursiveUrlLoader functionality documentation"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"input": {"query": "RecursiveUrlLoader use cases and examples"}}
{"query": "RecursiveUrlLoader use cases and examples"}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"input": {"query": "Understanding RecursiveUrlLoader in Conceptual docs"}}
{"query": "Understanding RecursiveUrlLoader in Conceptual docs"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}], "steps": ["Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"question": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page."}
{"question": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page."}
{"input": {"question": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_i8alqc3m8ua1MbkaAqL546Ee", "function": {"arguments": "{\"queries\":[\"RecursiveUrlLoader how-to guide\",\"RecursiveUrlLoader code examples\",\"Using RecursiveUrlLoader to load web content\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-315664d7-2800-4856-b20e-3fc1295f5a18", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}, "id": "call_i8alqc3m8ua1MbkaAqL546Ee", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}
{"input": {"question": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"], "documents": []}}
{"input": {"query": "RecursiveUrlLoader how-to guide"}}
{"query": "RecursiveUrlLoader how-to guide"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}]}
{"input": {"query": "RecursiveUrlLoader code examples"}}
{"query": "RecursiveUrlLoader code examples"}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}]}
{"input": {"query": "Using RecursiveUrlLoader to load web content"}}
{"query": "Using RecursiveUrlLoader to load web content"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}], "steps": ["Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"question": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."}
{"question": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."}
{"input": {"question": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_MlxoUaokud5JSHcOyld8pisp", "function": {"arguments": "{\"queries\":[\"RecursiveUrlLoader integration documentation\",\"dependencies for RecursiveUrlLoader\",\"configuration settings for RecursiveUrlLoader usage\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-e0b0e8ca-d3cf-4bde-a514-9e2a66b53aa6", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}, "id": "call_MlxoUaokud5JSHcOyld8pisp", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}
{"input": {"question": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader.", "queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"], "documents": []}}
{"input": {"query": "RecursiveUrlLoader integration documentation"}}
{"query": "RecursiveUrlLoader integration documentation"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}]}
{"input": {"query": "dependencies for RecursiveUrlLoader"}}
{"query": "dependencies for RecursiveUrlLoader"}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"input": {"query": "configuration settings for RecursiveUrlLoader usage"}}
{"query": "configuration settings for RecursiveUrlLoader usage"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "steps": []}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "answer": "", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are an expert programmer and problem-solver, tasked with answering any question about LangChain.\n\nGenerate a comprehensive and informative answer for the given question based solely on the provided search results (URL and content). Do NOT ramble, and adjust your response length based on the question. If they ask a question that can be answered in one sentence, do that. If 5 paragraphs of detail is needed, do that. You must only use information from the provided search results. Use an unbiased and journalistic tone. Combine search results together into a coherent answer. Do not repeat text. Cite search results using [${number}] notation. Only cite the most relevant results that answer the question accurately. Place these citations at the end of the individual sentence or paragraph that reference them. Do not put them all at the end, but rather sprinkle them throughout. If different results refer to different entities within the same name, write separate answers for each entity.\n\nYou should use bullet points in your answer for readability. Put citations where they apply rather than putting them all at the end. DO NOT PUT THEM ALL THAT END, PUT THEM IN THE BULLET POINTS.\n\nIf there is nothing in the context relevant to the question at hand, do NOT make up an answer. Rather, tell them why you're unsure and ask for any additional information that may help you answer better.\n\nSometimes, what a user is asking may NOT be possible. Do NOT tell them that things are possible if you don't see evidence for it in the context below. If you don't see based in the information below that something is possible, do NOT say that it is - instead say that you're not sure.\n\nAnything between the following `context` html blocks is retrieved from a knowledge bank, not part of the conversation with the user.\n\n<context>\n    <documents>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46'>\nFor detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)\n</document>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='84143e77-2eef-5124-804b-be76667edaf2'>\nRecursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b\n</document>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='fb260844-e344-5e15-beb8-d242dae38764'>\n## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.\n</document>\n<document  description='This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/url/' title='URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/url/' uuid='005a63b4-8392-5045-82f2-b81186f74af5'>\n**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output\n</document>\n<document  language='en' description='Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/nuclia/' title='Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/nuclia/' uuid='0b22f469-33c4-5bab-909f-44f40db838a3'>\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)\n</document>\n<document  language='en' description='Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/' title='Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/' uuid='303552cf-c14a-5615-b73b-6072a5ac090b'>\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)\n</document>\n<document  language='en' description='AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/async_html/' title='AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/async_html/' uuid='5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8'>\nAsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)\n</document>\n<document  description='This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/web_base/' title='WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/web_base/' uuid='9e6977c1-8cd3-5e48-a465-a937d97029c2'>\nWebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```\n</document>\n<document  description='Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/' title='PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/' uuid='1cca1842-3793-59d9-babc-5daf57e13dbf'>\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)\n</document>\n<document  description='Within the settings of the LangSmith playground, you can save your model configuration for later use.' language='en' changefreq=None source='https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration' title='Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='b31f2ef0-7681-5425-b4fd-ab095f0ae993'>\nSave settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n</documents>\n<context/>", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "How do I use a RecursiveUrlLoader to load content from a page?", "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085"}}]]}
{"messages": [{"content": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c009b756-1f8f-4fad-b2aa-cebae7aab05f", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "num_response_attempts": 1}
{"input": {"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}, {"content": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c009b756-1f8f-4fad-b2aa-cebae7aab05f", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "answer": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."}, {"role": "human", "content": "Set of facts: \n\n <documents>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46'>\nFor detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)\n</document>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='84143e77-2eef-5124-804b-be76667edaf2'>\nRecursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b\n</document>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='fb260844-e344-5e15-beb8-d242dae38764'>\n## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.\n</document>\n<document  description='This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/url/' title='URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/url/' uuid='005a63b4-8392-5045-82f2-b81186f74af5'>\n**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output\n</document>\n<document  language='en' description='Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/nuclia/' title='Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/nuclia/' uuid='0b22f469-33c4-5bab-909f-44f40db838a3'>\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)\n</document>\n<document  language='en' description='Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/' title='Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/' uuid='303552cf-c14a-5615-b73b-6072a5ac090b'>\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)\n</document>\n<document  language='en' description='AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/async_html/' title='AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/async_html/' uuid='5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8'>\nAsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)\n</document>\n<document  description='This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/web_base/' title='WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/web_base/' uuid='9e6977c1-8cd3-5e48-a465-a937d97029c2'>\nWebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```\n</document>\n<document  description='Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/' title='PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/' uuid='1cca1842-3793-59d9-babc-5daf57e13dbf'>\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)\n</document>\n<document  description='Within the settings of the LangSmith playground, you can save your model configuration for later use.' language='en' changefreq=None source='https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration' title='Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='b31f2ef0-7681-5425-b4fd-ab095f0ae993'>\nSave settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n</documents> \n\n LLM generation: To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2]."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Set of facts: \n\n <documents>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46'>\nFor detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)\n</document>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='84143e77-2eef-5124-804b-be76667edaf2'>\nRecursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b\n</document>\n<document  description='The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' title='Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/recursive_url/' uuid='fb260844-e344-5e15-beb8-d242dae38764'>\n## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.\n</document>\n<document  description='This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/url/' title='URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/url/' uuid='005a63b4-8392-5045-82f2-b81186f74af5'>\n**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output\n</document>\n<document  language='en' description='Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/nuclia/' title='Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/nuclia/' uuid='0b22f469-33c4-5bab-909f-44f40db838a3'>\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)\n</document>\n<document  language='en' description='Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/' title='Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/' uuid='303552cf-c14a-5615-b73b-6072a5ac090b'>\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)\n</document>\n<document  language='en' description='AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/async_html/' title='AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/async_html/' uuid='5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8'>\nAsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)\n</document>\n<document  description='This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/web_base/' title='WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/web_base/' uuid='9e6977c1-8cd3-5e48-a465-a937d97029c2'>\nWebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```\n</document>\n<document  description='Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/' title='PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/' uuid='1cca1842-3793-59d9-babc-5daf57e13dbf'>\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)\n</document>\n<document  description='Within the settings of the LangSmith playground, you can save your model configuration for later use.' language='en' changefreq=None source='https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration' title='Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='b31f2ef0-7681-5425-b4fd-ab095f0ae993'>\nSave settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n</documents> \n\n LLM generation: To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_XhVw2v9JmIJOfflcqh7Mi9Nb", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2ff2fc50-9c13-46a5-8183-3faef04fef22", "example": false, "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_XhVw2v9JmIJOfflcqh7Mi9Nb", "type": "tool_call"}], "invalid_tool_calls": []}}
