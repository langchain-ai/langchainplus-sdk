{"messages": [{"role": "user", "content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto"}]}
{"messages": [{"role": "user", "content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto"}]}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "2aab5b84-b0cd-5c90-b403-a08118c7c40c"}, "page_content": "the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.</p>\\n<p>Task decomposition can be done (1) by LLM with simple prompting like <code>&quot;Steps for XYZ.\\\\n1.&quot;</code>, <code>&quot;What are the subgoals for achieving XYZ?&quot;</code>, (2) by using task-specific instructions; e.g. <code>&quot;Write a story outline.&quot;</code> for writing a novel, or (3) with human inputs.</p>\\n<p>Another quite distinct approach, <strong>LLM+P</strong> (<a href=\"https://arxiv.org/abs/2304.11477\">Liu et al. 2023</a>), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into &ldquo;Problem PDDL&rdquo;, then (2) requests a classical planner to generate a PDDL plan based on an existing &ldquo;Domain PDDL&rdquo;, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.</p>\\n<h2 id=\"self-reflection\">Self-Reflection<a hidden class=\"anchor\" aria-hidden=\"true\" href=\"#self-reflection\">#</a></h2>\\n<p>Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.</p>\\n<p><a name=\"react\"></a><strong>ReAct</strong> (<a href=\"https://arxiv.org/abs/2210.03629\">Yao et al. 2023</a>) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.</p>\\n<p>The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:</p>\\n<pre tabindex=\"0\"><code>Thought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n</code></pre><img src=\"react.png\" style=\"width: 100%;\" class=\"center\" />\\n<figcaption>Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: <a href=\"https://arxiv.org/abs/2210.03629\" target=\"_blank\">Yao et al. 2023</a>).</figcaption>\\n<p>In both experiments on knowledge-intensive tasks and decision-making tasks, <code>ReAct</code> works better than the <code>Act</code>-only baseline where <code>Thought: \u2026</code> step is removed.</p>\\n<p><strong>Reflexion</strong> (<a href=\"https://arxiv.org/abs/2303.11366\">Shinn &amp; Labash 2023</a>) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may <em>decide to reset</em> the environment to start a new trial depending on the self-reflection results.</p>\\n<img src=\"reflexion.png\" style=\"width: 80%;\" class=\"center\" />\\n<figcaption>Fig. 3. Illustration of the Reflexion framework. (Image source: <a href=\"https://arxiv.org/abs/2303.11366\" target=\"_blank\">Shinn & Labash, 2023</a>)</figcaption>\\n<p>The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/", "title": "Agent architectures | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/", "uuid": "08ad3f07-bc6e-5fd9-b58c-12faeebe8c96"}, "page_content": "Evaluating task completion and correctness\nProviding feedback for iterative improvement\nEnabling self-correction and learning\n\nWhile often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.\nBy leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.\nComments", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "e40ad861-c8f7-5738-a0c8-92f735b3d0c2"}, "page_content": "fast retrieval. Tool use The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more. Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\\\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \\\\\"Steps for XYZ.\\\\\\\\n1.\\\\\", \\\\\"What are the subgoals for achieving XYZ?\\\\\", (2) by using task-specific instructions; e.g. \\\\\"Write a story outline.\\\\\" for writing a novel, or (3) with human inputs.\\\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into \u201cProblem PDDL\u201d, then (2) requests a classical planner to generate a PDDL plan based on an existing \u201cDomain PDDL\u201d, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\\\nSelf-Reflection Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\\\nThought: ... Action: ... Observation: ... ... (Repeated many times) Fig. 2. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023). In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: \u2026 step is removed.\\\\nReflexion (Shinn \\\\u0026 Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\\\nFig. 3. Illustration of the Reflexion framework.", "type": "Document"}, {"metadata": {"description": "In this tutorial, we will walk through 3 evaluation strategies LLM agents, building on the conceptual points shared in our evaluation guide.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/agents", "title": "Evaluate an agent | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "be7de036-4a6c-5506-905d-eebf6ba7d6fe"}, "page_content": "Evaluate an agent | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsEvaluate an agentOn this pageEvaluate an agent\nIn this tutorial, we will walk through 3 evaluation strategies LLM agents, building on the conceptual points shared in our evaluation guide.\n\nFinal Response: Evaluate the agent's final response.\nSingle step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).\nTrajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.", "type": "Document"}, {"metadata": {"language": "en", "description": "Comparing documents through embeddings has the benefit of working across multiple languages. \"Harrison says hello\" and \"Harrison dice hola\" will occupy similar positions in the vector space because they have the same meaning semantically.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document/", "title": "Doctran: language translation | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document/", "uuid": "b5d8329f-ab93-5031-aa04-df45a380e45b"}, "page_content": "Proyectos de Investigaci\u00f3n y Desarrollo\nEn nuestra b\u00fasqueda de innovaci\u00f3n, nuestro departamento de investigaci\u00f3n y desarrollo ha estado trabajando incansablemente en varios proyectos. Me gustar\u00eda reconocer el trabajo excepcional de David Rodriguez (email: david.rodriguez@example.com) en su rol como l\u00edder de proyecto. Las contribuciones de David al desarrollo de nuestra tecnolog\u00eda de vanguardia han sido fundamentales. Adem\u00e1s, recordamos a todos que compartan sus ideas y sugerencias para posibles nuevos proyectos durante nuestra sesi\u00f3n mensual de lluvia de ideas de I+D, programada para el 10 de Julio.\n\nPor favor, traten la informaci\u00f3n en este documento con la m\u00e1xima confidencialidad y aseg\u00farense de que no sea compartida con personas no autorizadas. Si tienen alguna pregunta o inquietud sobre los temas discutidos, por favor no duden en comunicarse directamente conmigo.\n\nGracias por su atenci\u00f3n, y sigamos trabajando juntos para alcanzar nuestros objetivos.\n\nSaludos cordiales,\n\nJason Fan\nCofundador y CEO\nPsychic\njason@psychic.dev\n```\n\n## Output using the Async version\u200b\n\nAfter translating a document, the result will be returned as a new document with the page_content translated into the target language. The async version will improve performance when the documents are chunked in multiple parts. It will also make sure to return the output in the correct order.\n\n```python\nimport asyncio\n```\n\n```python\nresult = await qa_translator.atransform_documents(documents)\n```\n\n```python\nprint(result[0].page_content)\n```\n\n```output\nDocumento Confidencial - Solo para Uso Interno\n\nFecha: 1 de Julio de 2023\n\nAsunto: Actualizaciones y Discusiones sobre Varios Temas\n\nEstimado Equipo,\n\nEspero que este correo electr\u00f3nico les encuentre bien. En este documento, me gustar\u00eda proporcionarles algunas actualizaciones importantes y discutir varios temas que requieren nuestra atenci\u00f3n. Por favor, traten la informaci\u00f3n contenida aqu\u00ed como altamente confidencial.\n\nMedidas de Seguridad y Privacidad\nComo parte de nuestro compromiso continuo de garantizar la seguridad y privacidad de los datos de nuestros clientes, hemos implementado medidas s\u00f3lidas en todos nuestros sistemas. Nos gustar\u00eda elogiar a John Doe (email: john.doe@example.com) del departamento de TI por su trabajo diligente en mejorar nuestra seguridad de red. En adelante, recordamos amablemente a todos que se adhieran estrictamente a nuestras pol\u00edticas y pautas de protecci\u00f3n de datos. Adem\u00e1s, si encuentran alg\u00fan riesgo o incidente de seguridad potencial, por favor rep\u00f3rtenlo inmediatamente a nuestro equipo dedicado en security@example.com.\n\nActualizaciones de Recursos Humanos y Beneficios para Empleados\nRecientemente, dimos la bienvenida a varios nuevos miembros del equipo que han hecho contribuciones significativas a sus respectivos departamentos. Me gustar\u00eda reconocer a Jane Smith (SSN: 049-45-5928) por su destacado desempe\u00f1o en servicio al cliente. Jane ha recibido consistentemente comentarios positivos de nuestros clientes. Adem\u00e1s, recuerden que el per\u00edodo de inscripci\u00f3n abierta para nuestro programa de beneficios para empleados se acerca r\u00e1pidamente. Si tienen alguna pregunta o requieren asistencia, por favor contacten a nuestro representante de Recursos Humanos, Michael Johnson (tel\u00e9fono: 418-492-3850, email: michael.johnson@example.com).\n\nIniciativas y Campa\u00f1as de Marketing\nNuestro equipo de marketing ha estado trabajando activamente en el desarrollo de nuevas estrategias para aumentar el conocimiento de la marca y fomentar la participaci\u00f3n de los clientes. Nos gustar\u00eda agradecer a Sarah Thompson (tel\u00e9fono: 415-555-1234) por sus esfuerzos excepcionales en la gesti\u00f3n de nuestras plataformas de redes sociales. Sarah ha aumentado con \u00e9xito nuestra base de seguidores en un 20% solo en el \u00faltimo mes. Adem\u00e1s, marquen sus calendarios para el pr\u00f3ximo evento de lanzamiento de productos el 15 de Julio. Animamos a todos los miembros del equipo a asistir y apoyar este emocionante hito para nuestra empresa.", "type": "Document"}, {"metadata": {"description": "Comparing documents through embeddings has the benefit of working across multiple languages. \"Harrison says hello\" and \"Harrison dice hola\" will occupy similar positions in the vector space because they have the same meaning semantically.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document/", "title": "Doctran: language translation | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document/", "uuid": "d01e1d53-f102-5abd-95ef-033f2a4e67d3"}, "page_content": "Proyectos de Investigaci\u00f3n y Desarrollo\nEn nuestra b\u00fasqueda de innovaci\u00f3n, nuestro departamento de investigaci\u00f3n y desarrollo ha estado trabajando incansablemente en varios proyectos. Me gustar\u00eda reconocer el trabajo excepcional de David Rodriguez (email: david.rodriguez@example.com) en su rol como l\u00edder de proyecto. Las contribuciones de David al desarrollo de nuestra tecnolog\u00eda de vanguardia han sido fundamentales. Adem\u00e1s, recordamos a todos que compartan sus ideas y sugerencias para posibles nuevos proyectos durante nuestra sesi\u00f3n mensual de lluvia de ideas de I+D, programada para el 10 de Julio.\n\nPor favor, traten la informaci\u00f3n en este documento con la m\u00e1xima confidencialidad y aseg\u00farense de que no sea compartida con personas no autorizadas. Si tienen alguna pregunta o inquietud sobre los temas discutidos, por favor no duden en comunicarse directamente conmigo.\n\nGracias por su atenci\u00f3n, y sigamos trabajando juntos para alcanzar nuestros objetivos.\n\nSaludos cordiales,\n\nJason Fan\nCofundador y CEO\nPsychic\njason@psychic.dev\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output using Sync version](#output-using-sync-version)\n\n- [Output using the Async version](#output-using-the-async-version)", "type": "Document"}, {"metadata": {"description": "Spreedly is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways and third party APIs. It does this by simultaneously providing a card tokenization/vault service as well as a gateway and receiver integration service. Payment methods tokenized by Spreedly are stored at Spreedly, allowing you to independently store a card and then pass that card to different end points based on your business requirements.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/spreedly/", "title": "Spreedly | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/spreedly/", "uuid": "be8ce1f5-2d6f-5220-b9b5-0e72e27e29ee"}, "page_content": "```output\n[Document(page_content='installment_grace_period_duration\\nreference_data_code\\ninvoice_number\\ntax_management_indicator\\noriginal_amount\\ninvoice_amount\\nvat_tax_rate\\nmobile_remote_payment_type\\ngratuity_amount\\nmdd_field_1\\nmdd_field_2\\nmdd_field_3\\nmdd_field_4\\nmdd_field_5\\nmdd_field_6\\nmdd_field_7\\nmdd_field_8\\nmdd_field_9\\nmdd_field_10\\nmdd_field_11\\nmdd_field_12\\nmdd_field_13\\nmdd_field_14\\nmdd_field_15\\nmdd_field_16\\nmdd_field_17\\nmdd_field_18\\nmdd_field_19\\nmdd_field_20\\nsupported_countries: US\\nAE\\nBR\\nCA\\nCN\\nDK\\nFI\\nFR\\nDE\\nIN\\nJP\\nMX\\nNO\\nSE\\nGB\\nSG\\nLB\\nPK\\nsupported_cardtypes: visa\\nmaster\\namerican_express\\ndiscover\\ndiners_club\\njcb\\ndankort\\nmaestro\\nelo\\nregions: asia_pacific\\neurope\\nlatin_america\\nnorth_america\\nhomepage: http://www.cybersource.com\\ndisplay_api_url: https://ics2wsa.ic3.com/commerce/1.x/transactionProcessor\\ncompany_name: CyberSource', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'}),\n Document(page_content='BG\\nBH\\nBI\\nBJ\\nBM\\nBN\\nBO\\nBR\\nBS\\nBT\\nBW\\nBY\\nBZ\\nCA\\nCC\\nCF\\nCH\\nCK\\nCL\\nCM\\nCN\\nCO\\nCR\\nCV\\nCX\\nCY\\nCZ\\nDE\\nDJ\\nDK\\nDO\\nDZ\\nEC\\nEE\\nEG\\nEH\\nES\\nET\\nFI\\nFJ\\nFK\\nFM\\nFO\\nFR\\nGA\\nGB\\nGD\\nGE\\nGF\\nGG\\nGH\\nGI\\nGL\\nGM\\nGN\\nGP\\nGQ\\nGR\\nGT\\nGU\\nGW\\nGY\\nHK\\nHM\\nHN\\nHR\\nHT\\nHU\\nID\\nIE\\nIL\\nIM\\nIN\\nIO\\nIS\\nIT\\nJE\\nJM\\nJO\\nJP\\nKE\\nKG\\nKH\\nKI\\nKM\\nKN\\nKR\\nKW\\nKY\\nKZ\\nLA\\nLC\\nLI\\nLK\\nLS\\nLT\\nLU\\nLV\\nMA\\nMC\\nMD\\nME\\nMG\\nMH\\nMK\\nML\\nMN\\nMO\\nMP\\nMQ\\nMR\\nMS\\nMT\\nMU\\nMV\\nMW\\nMX\\nMY\\nMZ\\nNA\\nNC\\nNE\\nNF\\nNG\\nNI\\nNL\\nNO\\nNP\\nNR\\nNU\\nNZ\\nOM\\nPA\\nPE\\nPF\\nPH\\nPK\\nPL\\nPN\\nPR\\nPT\\nPW\\nPY\\nQA\\nRE\\nRO\\nRS\\nRU\\nRW\\nSA\\nSB\\nSC\\nSE\\nSG\\nSI\\nSK\\nSL\\nSM\\nSN\\nST\\nSV\\nSZ\\nTC\\nTD\\nTF\\nTG\\nTH\\nTJ\\nTK\\nTM\\nTO\\nTR\\nTT\\nTV\\nTW\\nTZ\\nUA\\nUG\\nUS\\nUY\\nUZ\\nVA\\nVC\\nVE\\nVI\\nVN\\nVU\\nWF\\nWS\\nYE\\nYT\\nZA\\nZM\\nsupported_cardtypes: visa\\nmaster\\namerican_express\\ndiscover\\njcb\\nmaestro\\nelo\\nnaranja\\ncabal\\nunionpay\\nregions: asia_pacific\\neurope\\nmiddle_east\\nnorth_america\\nhomepage: http://worldpay.com\\ndisplay_api_url: https://secure.worldpay.com/jsp/merchant/xml/paymentService.jsp\\ncompany_name: WorldPay', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'}),\n Document(page_content='gateway_specific_fields: receipt_email\\nradar_session_id\\nskip_radar_rules\\napplication_fee\\nstripe_account\\nmetadata\\nidempotency_key\\nreason\\nrefund_application_fee\\nrefund_fee_amount\\nreverse_transfer\\naccount_id\\ncustomer_id\\nvalidate\\nmake_default\\ncancellation_reason\\ncapture_method\\nconfirm\\nconfirmation_method\\ncustomer\\ndescription\\nmoto\\noff_session\\non_behalf_of\\npayment_method_types\\nreturn_email\\nreturn_url\\nsave_payment_method\\nsetup_future_usage\\nstatement_descriptor\\nstatement_descriptor_suffix\\ntransfer_amount\\ntransfer_destination\\ntransfer_group\\napplication_fee_amount\\nrequest_three_d_secure\\nerror_on_requires_action\\nnetwork_transaction_id\\nclaim_without_transaction_id\\nfulfillment_date\\nevent_type\\nmodal_challenge\\nidempotent_request\\nmerchant_reference\\ncustomer_reference\\nshipping_address_zip\\nshipping_from_zip\\nshipping_amount\\nline_items\\nsupported_countries: AE\\nAT\\nAU\\nBE\\nBG\\nBR\\nCA\\nCH\\nCY\\nCZ\\nDE\\nDK\\nEE\\nES\\nFI\\nFR\\nGB\\nGR\\nHK\\nHU\\nIE\\nIN\\nIT\\nJP\\nLT\\nLU\\nLV\\nMT\\nMX\\nMY\\nNL\\nNO\\nNZ\\nPL\\nPT\\nRO\\nSE\\nSG\\nSI\\nSK\\nUS\\nsupported_cardtypes: visa', metadata={'source': 'https://core.spreedly.com/v1/gateways_options.json'}),", "type": "Document"}, {"metadata": {"language": "en", "description": "Comparing documents through embeddings has the benefit of working across multiple languages. \"Harrison says hello\" and \"Harrison dice hola\" will occupy similar positions in the vector space because they have the same meaning semantically.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document/", "title": "Doctran: language translation | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_translate_document/", "uuid": "168a8284-6f0e-59e3-82b7-33225aa81bdd"}, "page_content": "Research and Development Projects\nIn our pursuit of innovation, our research and development department has been working tirelessly on various projects. I would like to acknowledge the exceptional work of David Rodriguez (email: david.rodriguez@example.com) in his role as project lead. David's contributions to the development of our cutting-edge technology have been instrumental. Furthermore, we would like to remind everyone to share their ideas and suggestions for potential new projects during our monthly R&D brainstorming session, scheduled for July 10th.\n\nPlease treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n\"\"\"\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nqa_translator = DoctranTextTranslator(language=\"spanish\")\n```\n\n## Output using Sync version\u200b\n\nAfter translating a document, the result will be returned as a new document with the page_content translated into the target language\n\n```python\ntranslated_document = qa_translator.transform_documents(documents)\n```\n\n```python\nprint(translated_document[0].page_content)\n```\n\n```output\nDocumento Confidencial - Solo para Uso Interno\n\nFecha: 1 de Julio de 2023\n\nAsunto: Actualizaciones y Discusiones sobre Varios Temas\n\nEstimado Equipo,\n\nEspero que este correo electr\u00f3nico los encuentre bien. En este documento, me gustar\u00eda proporcionarles algunas actualizaciones importantes y discutir varios temas que requieren nuestra atenci\u00f3n. Por favor, traten la informaci\u00f3n contenida aqu\u00ed como altamente confidencial.\n\nMedidas de Seguridad y Privacidad\nComo parte de nuestro compromiso continuo para garantizar la seguridad y privacidad de los datos de nuestros clientes, hemos implementado medidas s\u00f3lidas en todos nuestros sistemas. Nos gustar\u00eda elogiar a John Doe (email: john.doe@example.com) del departamento de TI por su trabajo diligente en mejorar nuestra seguridad de red. En adelante, recordamos amablemente a todos que se adhieran estrictamente a nuestras pol\u00edticas y pautas de protecci\u00f3n de datos. Adem\u00e1s, si encuentran alg\u00fan riesgo o incidente de seguridad potencial, por favor rep\u00f3rtenlo inmediatamente a nuestro equipo dedicado en security@example.com.\n\nActualizaciones de Recursos Humanos y Beneficios para Empleados\nRecientemente, dimos la bienvenida a varios nuevos miembros del equipo que han hecho contribuciones significativas a sus respectivos departamentos. Me gustar\u00eda reconocer a Jane Smith (SSN: 049-45-5928) por su destacado desempe\u00f1o en servicio al cliente. Jane ha recibido consistentemente comentarios positivos de nuestros clientes. Adem\u00e1s, recuerden que el per\u00edodo de inscripci\u00f3n abierta para nuestro programa de beneficios para empleados se acerca r\u00e1pidamente. Si tienen alguna pregunta o requieren asistencia, por favor contacten a nuestro representante de Recursos Humanos, Michael Johnson (tel\u00e9fono: 418-492-3850, email: michael.johnson@example.com).\n\nIniciativas y Campa\u00f1as de Marketing\nNuestro equipo de marketing ha estado trabajando activamente en el desarrollo de nuevas estrategias para aumentar el conocimiento de la marca y fomentar la participaci\u00f3n de los clientes. Nos gustar\u00eda agradecer a Sarah Thompson (tel\u00e9fono: 415-555-1234) por sus esfuerzos excepcionales en la gesti\u00f3n de nuestras plataformas de redes sociales. Sarah ha aumentado con \u00e9xito nuestra base de seguidores en un 20% solo en el \u00faltimo mes. Adem\u00e1s, marquen sus calendarios para el pr\u00f3ximo evento de lanzamiento de productos el 15 de Julio. Animamos a todos los miembros del equipo a asistir y apoyar este emocionante hito para nuestra empresa.", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/extraction/retries/", "title": "Complex data extraction with function calling | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/extraction/retries/", "uuid": "a5c006d8-a806-5f7c-89e8-ec0fe351cea7"}, "page_content": "And it works!\u00b6\nRetries are an easy way to reduce function calling failures. While retrying may become unnecessary with more powerful LLMs, data validation is important to control how LLMs interact with the rest of your software stack.\nIf you notice high retry rates (using an observability tool like LangSmith), you can set up a rule to send the failure cases to a dataset alongside the corrected values and then automatically program those into your prompts or schemas (or use them as few-shots to have semantically relevant demonstrations).\nComments", "type": "Document"}, {"metadata": {"description": null, "language": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/", "title": "LangChain Python API Reference Documentation.", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "b0661b7d-cd15-5e35-b230-e78252baa687"}, "page_content": "LangChain Python API Reference Documentation.\n\nYou will be automatically redirected to the new location of this page.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/chains/langchain.chains.api.base.APIChain.html", "title": "langchain.chains.api.base.APIChain \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "3b65b9f3-4004-52be-9c80-4be14444e78a"}, "page_content": "langchain.chains.api.base.APIChain \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17\n\nThis is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\n\nLangChain\n\nCore\n\nCommunity\n\nExperimental\n\nText splitters\n\nai21\n\nairbyte\n\nanthropic\n\nastradb\n\naws\n\nazure-dynamic-sessions\n\nbox\n\nchroma\n\ncohere\n\ncouchbase\n\nelasticsearch\n\nexa\n\nfireworks\n\ngoogle-community\n\ngoogle-genai\n\ngoogle-vertexai\n\ngroq\n\nhuggingface\n\nibm\n\nmilvus\n\nmistralai\n\nmongodb\n\nnomic\n\nnvidia-ai-endpoints\n\nollama\n\nopenai\n\npinecone\n\npostgres\n\nprompty\n\nqdrant\n\nrobocorp\n\ntogether\n\nunstructured\n\nvoyageai\n\nweaviate\n\nPartner libs\n\nai21\nairbyte\nanthropic\nastradb\naws\nazure-dynamic-sessions\nbox\nchroma\ncohere\ncouchbase\nelasticsearch\nexa\nfireworks\ngoogle-community\ngoogle-genai\ngoogle-vertexai\ngroq\nhuggingface\nibm\nmilvus\nmistralai\nmongodb\nnomic\nnvidia-ai-endpoints\nollama\nopenai\npinecone\npostgres\nprompty\nqdrant\nrobocorp\ntogether\nunstructured\nvoyageai\nweaviate\n\nDocs\n\nToggle Menu\n\nlangchain.chains.api.base.APIChain\nAPIChain\nAPIChain.api_answer_chain\nAPIChain.api_docs\nAPIChain.api_request_chain\nAPIChain.callback_manager\nAPIChain.callbacks\nAPIChain.limit_to_domains\nAPIChain.memory\nAPIChain.metadata\nAPIChain.requests_wrapper\nAPIChain.tags\nAPIChain.verbose\nAPIChain.__call__()\nAPIChain.abatch()\nAPIChain.abatch_as_completed()\nAPIChain.acall()\nAPIChain.ainvoke()\nAPIChain.apply()\nAPIChain.aprep_inputs()\nAPIChain.aprep_outputs()\nAPIChain.arun()\nAPIChain.as_tool()\nAPIChain.astream()\nAPIChain.astream_events()\nAPIChain.batch()\nAPIChain.batch_as_completed()\nAPIChain.configurable_alternatives()\nAPIChain.configurable_fields()\nAPIChain.from_llm_and_api_docs()\nAPIChain.invoke()\nAPIChain.prep_inputs()\nAPIChain.prep_outputs()\nAPIChain.run()\nAPIChain.save()\nAPIChain.stream()\nAPIChain.to_json()\n\nlangchain.chains.api.base.APIChain\u00b6\n\nNote\nAPIChain implements the standard Runnable Interface. \ud83c\udfc3\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\n\nclass langchain.chains.api.base.APIChain[source]\u00b6\nBases: Chain\n\nDeprecated since version 0.2.13: This class is deprecated and will be removed in langchain 1.0. See API reference for replacement: https://api.python.langchain.com/en/latest/chains/langchain.chains.api.base.APIChain.html\n\nChain that makes API calls and summarizes the responses to answer a question.\n\nSecurity Note: This API chain uses the requests toolkitto make GET, POST, PATCH, PUT, and DELETE requests to an API.\nExercise care in who is allowed to use this chain. If exposing\nto end users, consider that users will be able to make arbitrary\nrequests on behalf of the server hosting the code. For example,\nusers could ask the server to make a request to a private API\nthat is only accessible from the server.\nControl access to who can submit issue requests using this toolkit and\nwhat network access it has.\nSee https://python.langchain.com/docs/security for more information.\n\nNote: this class is deprecated. See below for a replacement implementation\nusing LangGraph. The benefits of this implementation are:\n\nUses LLM tool calling features to encourage properly-formatted API requests;\nSupport for both token-by-token and step-by-step streaming;\nSupport for checkpointing and memory of chat history;\nEasier to modify or extend (e.g., with additional tools, structured responses, etc.)\n\nInstall LangGraph with:\npip install -U langgraph\n\nfrom typing import Annotated, Sequence\nfrom typing_extensions import TypedDict", "type": "Document"}, {"metadata": {"description": "Docusaurus is a static-site generator which provides out-of-the-box documentation features.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "title": "Docusaurus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "uuid": "8ca8dfbc-51cb-5b5a-9870-a1203e464dfc"}, "page_content": "of knowledge or computation. This can include Python REPLs, embeddings, search engines, and more. LangChain provides a large collection of common utils to use in your application.\\\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\nChat: Chat models are a variation on Language Models that expose a different API - rather than working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\\\n\\\\nAgents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\\\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\\\nData Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\\\nQuestion Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\\\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\\\nGenerate similar examples: Generating similar examples to a given input. This is a common use case for many applications, and LangChain provides some prompts/chains for assisting in this.\\\\nCompare models: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain\u2019s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Ecosystem#\\\\nGuides for how other companies/products can be used with LangChain\\\\n\\\\nLangChain Ecosystem\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAdditional Resources#\\\\nAdditional collection of resources we think may be useful as you develop your application!\\\\n\\\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for", "type": "Document"}, {"metadata": {"description": "Docusaurus is a static-site generator which provides out-of-the-box documentation features.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "title": "Docusaurus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "uuid": "e1a60685-daa6-5c67-ae41-b5d020b140fa"}, "page_content": "Below are some of the common use cases LangChain supports.\\\\n\\\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\\\nExtraction: Extract structured information from text.\\\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain\u2019s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Ecosystem#\\\\nGuides for how other companies/products can be used with LangChain\\\\n\\\\nLangChain Ecosystem\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAdditional Resources#\\\\nAdditional collection of resources we think may be useful as you develop your application!\\\\n\\\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\nDiscord: Join us on our Discord to discuss all things LangChain!\\\\nProduction Support: As you move your LangChains into production, we\u2019d love to offer more comprehensive support. Please fill out this form and we\u2019ll set up a dedicated support Slack channel.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnext\\\\nQuickstart Guide\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Contents\\\\n  \\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy Harrison Chase\\\\n\\\\n\\\\n\\\\n\\\\n    \\\\n      \u00a9 Copyright 2023, Harrison Chase.\\\\n      \\\\n\\\\n\\\\n\\\\n\\\\n  Last updated on Mar 27, 2023.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\', lookup_str=\\'\\', metadata={\\'source\\': \\'https://python.langchain.com/en/latest/\\', \\'loc\\': \\'https://python.langchain.com/en/latest/\\', \\'lastmod\\': \\'2023-03-27T22:50:49.790324+00:00\\', \\'changefreq\\': \\'daily\\', \\'priority\\': \\'0.9\\'}, lookup_index=0)Add custom scraping rules\\u200bThe SitemapLoader uses beautifulsoup4 for the scraping process, and it scrapes every element on the page by default. The SitemapLoader constructor accepts a custom scraping function. This feature can be helpful to tailor the scraping process to your specific needs; for example, you might want to avoid scraping headers or navigation elements. The following example shows how to develop and use a custom function to avoid navigation and header elements.Import the beautifulsoup4 library and define the", "type": "Document"}, {"metadata": {"description": "This quick start will get you up and running with our evaluation SDK and Experiments UI.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation", "title": "Evaluation quick start | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "d9f6f3cc-b044-587a-8c1c-ea1aefbe00a7"}, "page_content": "Next steps\u200b\nFor conceptual explanations see the Conceptual guide.\nSee the How-to guides for answers to \u201cHow do I\u2026.?\u201d format questions.\nFor end-to-end walkthroughs see Tutorials.\nFor comprehensive descriptions of every class and function see the API reference.Was this page helpful?You can leave detailed feedback on GitHub.PreviousAdd observability to your LLM applicationNextQuick Start1. Install LangSmith2. Create an API key3. Set up your environment4. Run your evaluation5. View Experiments UINext stepsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "Create an account", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/create_account_api_key", "title": "Create an account and API key | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "74a68fda-1ba2-5be1-b04b-e420ddf04109"}, "page_content": "Create an account and API key | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationConceptual GuideHow-to GuidesSetupCreate an account and API keySet up an organizationSet up a workspaceSet up billing for your LangSmith accountUpdate invoice email, tax id and, business informationManage your organization using the APISet up access controlSet up resource tagsSAML SSOTutorialsOptimize tracing spend on LangSmithPricingSelf-hostingReferenceAdministrationHow-to GuidesSetupCreate an account and API keyOn this pageCreate an account and API key\nCreate an account\u200b\nTo get started with LangSmith, you need to create an account. You can sign up for a free account here.\nWe support logging in with Google, GitHub, Discord, and email.\n\nAPI keys\u200b\nLangSmith supports two types of API keys: Service Keys and Personal Access Tokens.\nBoth types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.\nRead more about the differences between Service Keys and Personal Access Tokens under admin concepts\nCreate an API key\u200b\nTo log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests.\nCurrently, an API key is scoped to a workspace, so you will need to create an API key for each workspace you want to use.\nTo create either type of API key head to the Settings page, then scroll to the API Keys section. Then click Create API Key.\nnoteThe API key will be shown only once, so make sure to copy it and store it in a safe place.\n\nConfigure the SDK\u200b\nYou may set the following environment variables in addition to LANGCHAIN_API_KEY (or equivalently LANGSMITH_API_KEY).\nThese are only required if using the EU instance.\ninfoLANGCHAIN_HUB_API_URL is only required if using the legacy langchainhub sdk\nLANGCHAIN_ENDPOINT=https://api.smith.langchain.com\nLANGCHAIN_HUB_API_URL=https://api.smith.langchain.comWas this page helpful?You can leave detailed feedback on GitHub.PreviousAdministration how-to guidesNextSet up an organizationCreate an accountAPI keysCreate an API keyConfigure the SDKCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "Docusaurus is a static-site generator which provides out-of-the-box documentation features.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "title": "Docusaurus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "uuid": "4ee6a328-d1fb-53df-bf93-f8401e28a54d"}, "page_content": "Hugging Face Datasets\\\\nLLM Math\\\\nQuestion Answering Benchmarking: Paul Graham Essay\\\\nQuestion Answering Benchmarking: State of the Union Address\\\\nQA Generation\\\\nQuestion Answering\\\\nSQL Question Answering Benchmarking: Chinook\\\\n\\\\n\\\\n\\\\nReference\\\\n\\\\nInstallation\\\\nIntegrations\\\\nAPI References\\\\nPrompts\\\\nPromptTemplates\\\\nExample Selector\\\\n\\\\n\\\\nUtilities\\\\nPython REPL\\\\nSerpAPI\\\\nSearxNG Search\\\\nDocstore\\\\nText Splitter\\\\nEmbeddings\\\\nVectorStores\\\\n\\\\n\\\\nChains\\\\nAgents\\\\n\\\\n\\\\n\\\\nEcosystem\\\\n\\\\nLangChain Ecosystem\\\\nAI21 Labs\\\\nAtlasDB\\\\nBanana\\\\nCerebriumAI\\\\nChroma\\\\nCohere\\\\nDeepInfra\\\\nDeep Lake\\\\nForefrontAI\\\\nGoogle Search Wrapper\\\\nGoogle Serper Wrapper\\\\nGooseAI\\\\nGraphsignal\\\\nHazy Research\\\\nHelicone\\\\nHugging Face\\\\nMilvus\\\\nModal\\\\nNLPCloud\\\\nOpenAI\\\\nOpenSearch\\\\nPetals\\\\nPGVector\\\\nPinecone\\\\nPromptLayer\\\\nQdrant\\\\nRunhouse\\\\nSearxNG Search API\\\\nSerpAPI\\\\nStochasticAI\\\\nUnstructured\\\\nWeights & Biases\\\\nWeaviate\\\\nWolfram Alpha Wrapper\\\\nWriter\\\\n\\\\n\\\\n\\\\nAdditional Resources\\\\n\\\\nLangChainHub\\\\nGlossary\\\\nLangChain Gallery\\\\nDeployments\\\\nTracing\\\\nDiscord\\\\nProduction Support\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n.rst\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n.pdf\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain\\\\n\\\\n\\\\n\\\\n\\\\n Contents \\\\n\\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain#\\\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\\\n\\\\nBe data-aware: connect a language model to other sources of data\\\\nBe agentic: allow a language model to interact with its environment\\\\n\\\\nThe LangChain framework is designed with the above principles in mind.\\\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\\\n\\\\nGetting Started#\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\n\\\\nGetting Started Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere are several main modules that LangChain provides support for.\\\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\\\nThese modules are, in increasing order of complexity:\\\\n\\\\nModels: The various model types and model integrations LangChain supports.\\\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\\\n\\\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge", "type": "Document"}, {"metadata": {"language": "en", "description": "Retrieval is a common technique chatbots use to augment their responses with data outside a chat model's training data. This section will cover how to implement retrieval in the context of chatbots, but it's worth noting that retrieval is a very subtle and deep topic - we encourage you to explore other parts of the documentation that go into greater depth!", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "title": "How to add retrieval to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "uuid": "79c8ddaa-c99c-5d83-82fb-0c43263d133c"}, "page_content": "Invoking this chain combines both steps outlined above:\n\n```python\nretrieval_chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n        ],\n    }\n)\n```\n\n```output\n{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n 'context': [Document(page_content='Skip to main content\ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith'}),\n  Document(page_content='LangSmith Overview and User Guide | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith'}),\n  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith'}),\n  Document(page_content=\"does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this\", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith'})],\n 'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}\n```\n\nLooks good!\n\n## Query transformation\u200b", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/", "title": "How to Edit State of a Deployed Graph | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/", "uuid": "a8ca74b2-1a23-5f21-a8b6-4dc98c17a6d7"}, "page_content": "How to Edit State of a Deployed Graph\n\nHow to Edit State of a Deployed Graph\u00b6\nWhen creating LangGraph agents, it is often nice to add a human-in-the-loop component. This can be helpful when giving them access to tools. Often in these situations you may want to edit the graph state before continuing (for example, to edit what tool is being called, or how it is being called).\nThis can be in several ways, but the primary supported way is to add an \"interrupt\" before a node is executed. This interrupts execution at that node. You can then use update_state to update the state, and then resume from that spot to continue.\nSetup\u00b6\nWe are not going to show the full code for the graph we are hosting, but you can see it here if you want to. Once this graph is hosted, we are ready to invoke it and wait for user input.\nSDK initialization\u00b6\nFirst, we need to setup our client so that we can communicate with our hosted graph:\nPythonJavascriptCURL\n\nfrom langgraph_sdk import get_client\nclient = get_client(url=<DEPLOYMENT_URL>)\n# Using the graph deployed with the name \"agent\"\nassistant_id = \"agent\"\nthread = await client.threads.create()\n\nimport { Client } from \"@langchain/langgraph-sdk\";\n\nconst client = new Client({ apiUrl: <DEPLOYMENT_URL> });\n// Using the graph deployed with the name \"agent\"\nconst assistantId = \"agent\";\nconst thread = await client.threads.create();\n\ncurl --request POST \\\n  --url <DEPLOYMENT_URL>/threads \\\n  --header 'Content-Type: application/json' \\\n  --data '{}'\n\nEditing state\u00b6\nInitial invocation\u00b6\nNow let's invoke our graph, making sure to interrupt before the action node.\nPythonJavascriptCURL\n\ninput = { 'messages':[{ \"role\":\"user\", \"content\":\"search for weather in SF\" }] }\n\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id,\n    input=input,\n    stream_mode=\"updates\",\n    interrupt_before=[\"action\"],\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst input = { messages: [{ role: \"human\", content: \"search for weather in SF\" }] };\n\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: input,\n    streamMode: \"updates\",\n    interruptBefore: [\"action\"],\n  }\n);\n\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}\n\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"input\\\": {\\\"messages\\\": [{\\\"role\\\": \\\"human\\\", \\\"content\\\": \\\"search for weather in SF\\\"}]},\n   \\\"interrupt_before\\\": [\\\"action\\\"],\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll search for the current weather in San Francisco for you using the search function. Here's how I'll do that:\", 'type': 'text'}, {'id': 'toolu_01KEJMBFozSiZoS4mAcPZeqQ', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-6dbb0167-f8f6-4e2a-ab68-229b2d1fbb64', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_01KEJMBFozSiZoS4mAcPZeqQ'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nEdit the state\u00b6\nNow, let's assume we actually meant to search for the weather in Sidi Frej (another city with the initials SF). We can edit the state to properly reflect that:\nPythonJavascriptCURL", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/", "title": "How to view and update past graph state | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/", "uuid": "94ac2def-8c3c-52fc-9fbf-cfd5e8320fb4"}, "page_content": "Replay a state\u00b6\nWe can go back to any of these states and restart the agent from there! Let's go back to right before the tool call gets executed.\nto_replay = all_states[2]\n\nto_replay.values\n\n{'messages': [HumanMessage(content=\"Can you play Taylor Swift's most popular song?\", id='7e32f0f3-75f5-48e1-a4ae-d38ccc15973b'),\n  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_uhGY6Fv6Mr4ZOhSokintuoD7', 'function': {'arguments': '{\"song\":\"Anti-Hero by Taylor Swift\"}', 'name': 'play_song_on_apple'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 80, 'total_tokens': 102}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-af077bc4-f03c-4afe-8d92-78bdae394412-0', tool_calls=[{'name': 'play_song_on_apple', 'args': {'song': 'Anti-Hero by Taylor Swift'}, 'id': 'call_uhGY6Fv6Mr4ZOhSokintuoD7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 22, 'total_tokens': 102})]}\n\nto_replay.next\n\n('action',)\n\nTo replay from this place we just need to pass its config back to the agent. Notice that it just resumes from right where it left all - making a tool call.\nfor event in app.stream(None, to_replay.config):\n    for v in event.values():\n        print(v)\n\n{'messages': [ToolMessage(content='Succesfully played Anti-Hero by Taylor Swift on Apple Music!', name='play_song_on_apple', tool_call_id='call_uhGY6Fv6Mr4ZOhSokintuoD7')]}\n{'messages': [AIMessage(content='I\\'ve started playing \"Anti-Hero\" by Taylor Swift on Apple Music! Enjoy the music!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 126, 'total_tokens': 146}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-dc338bbd-d623-40bb-b824-5d2307954b57-0', usage_metadata={'input_tokens': 126, 'output_tokens': 20, 'total_tokens': 146})]}\n\nBranch off a past state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to instead of playing the song on Apple to play it on Spotify:\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = to_replay.values[\"messages\"][-1]\n\n# Let's now update the tool we are calling\nlast_message.tool_calls[0][\"name\"] = \"play_song_on_spotify\"\n\nbranch_config = app.update_state(\n    to_replay.config,\n    {\"messages\": [last_message]},\n)\n\nWe can then invoke with this new branch_config to resume running from here with changed state. We can see from the log that the tool was called with different input.\nfor event in app.stream(None, branch_config):\n    for v in event.values():\n        print(v)\n\n{'messages': [ToolMessage(content='Succesfully played Anti-Hero by Taylor Swift on Spotify!', name='play_song_on_spotify', tool_call_id='call_uhGY6Fv6Mr4ZOhSokintuoD7')]}\n{'messages': [AIMessage(content='I\\'ve started playing \"Anti-Hero\" by Taylor Swift on Spotify. Enjoy the music!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 125, 'total_tokens': 144}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-7d8d5094-7029-4da3-9e0e-ef9d18b63615-0', usage_metadata={'input_tokens': 125, 'output_tokens': 19, 'total_tokens': 144})]}\n\nAlternatively, we could update the state to not even call a tool!\nfrom langchain_core.messages import AIMessage", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/", "title": "How to view and update past graph state | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/", "uuid": "3c927f33-98f3-5c0f-a07a-0d4b72dca526"}, "page_content": "Alternatively, we could update the state to not even call a tool!\nfrom langchain_core.messages import AIMessage\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = to_replay.values[\"messages\"][-1]\n\n# Let's now get the ID for the last message, and create a new message with that ID.\nnew_message = AIMessage(\n    content=\"It's quiet hours so I can't play any music right now!\", id=last_message.id\n)\n\nbranch_config = app.update_state(\n    to_replay.config,\n    {\"messages\": [new_message]},\n)\n\nAPI Reference:\nAIMessage\n\nbranch_state = app.get_state(branch_config)\n\nbranch_state.values\n\n{'messages': [HumanMessage(content=\"Can you play Taylor Swift's most popular song?\", id='7e32f0f3-75f5-48e1-a4ae-d38ccc15973b'),\n  AIMessage(content=\"It's quiet hours so I can't play any music right now!\", id='run-af077bc4-f03c-4afe-8d92-78bdae394412-0')]}\n\nbranch_state.next\n\n()\n\nYou can see the snapshot was updated and now correctly reflects that there is no next step.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "09f68f98-0956-5097-9df2-a2e5da9e35a0"}, "page_content": "# Create a new Service key    api_key_res = session.post(        api_keys_path,        json={\"description\": \"my key\"},        headers=new_workspace_headers,    )    api_key_res.raise_for_status()    api_key_json = api_key_res.json()    api_key = api_key_json[\"key\"]if __name__ == \"__main__\":    main()Was this page helpful?You can leave detailed feedback on GitHub.PreviousUpdate invoice email, tax id and, business informationNextSet up access controlWorkspacesUser managementRBACMembership managementAPI KeysSecurity SettingsUser-Only EndpointsSample CodeCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Natural Language API Toolkits (NLAToolkits) permit LangChain Agents to efficiently plan and combine calls across endpoints.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/openapi_nla/", "title": "Natural Language API Toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/openapi_nla/", "uuid": "af021b44-dd3f-5454-a72c-fc0d6b9d61b0"}, "page_content": "```python\nmrkl.run(\n    \"I have an end of year party for my Italian class and have to buy some Italian clothes for it\"\n)\n```\n\n```output\n\n[1m> Entering new AgentExecutor chain...[0m\n[32;1m[1;3m I need to find out what kind of Italian clothes are available\nAction: Open_AI_Klarna_product_Api.productsUsingGET\nAction Input: Italian clothes[0m\nObservation: [31;1m[1;3mThe API response contains two products from the Al\u00e9 brand in Italian Blue. The first is the Al\u00e9 Colour Block Short Sleeve Jersey Men - Italian Blue, which costs $86.49, and the second is the Al\u00e9 Dolid Flash Jersey Men - Italian Blue, which costs $40.00.[0m\nThought:[32;1m[1;3m I now know what kind of Italian clothes are available and how much they cost.\nFinal Answer: You can buy two products from the Al\u00e9 brand in Italian Blue for your end of year party. The Al\u00e9 Colour Block Short Sleeve Jersey Men - Italian Blue costs $86.49, and the Al\u00e9 Dolid Flash Jersey Men - Italian Blue costs $40.00.[0m\n\n[1m> Finished chain.[0m\n```\n\n```output\n'You can buy two products from the Al\u00e9 brand in Italian Blue for your end of year party. The Al\u00e9 Colour Block Short Sleeve Jersey Men - Italian Blue costs $86.49, and the Al\u00e9 Dolid Flash Jersey Men - Italian Blue costs $40.00.'\n```\n\n### Use Auth and add more Endpoints\u200b\n\nSome endpoints may require user authentication via things like access tokens. Here we show how to pass in the authentication information via the `Requests` wrapper object.\n\nSince each NLATool exposes a concisee natural language interface to its wrapped API, the top level conversational agent has an easier job incorporating each endpoint to satisfy a user's request.\n\n**Adding the Spoonacular endpoints.**\n\n1. Go to the [Spoonacular API Console](https://spoonacular.com/food-api/console#Profile) and make a free account.\n\n2. Click on `Profile` and copy your API key below.\n\n```python\nspoonacular_api_key = \"\"  # Copy from the API Console\n```\n\n```python\nrequests = Requests(headers={\"x-api-key\": spoonacular_api_key})\nspoonacular_toolkit = NLAToolkit.from_llm_and_url(\n    llm,\n    \"https://spoonacular.com/application/frontend/downloads/spoonacular-openapi-3.json\",\n    requests=requests,\n    max_text_length=1800,  # If you want to truncate the response text\n)\n```", "type": "Document"}, {"metadata": {"description": "We can construct agents to consume arbitrary APIs, here APIs conformant to the OpenAPI/Swagger specification.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/openapi/", "title": "OpenAPI Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/openapi/", "uuid": "da934010-23b4-5081-9ba1-471f31454415"}, "page_content": "```python\nuser_query = \"give me a song I'd like, make it blues-ey\"\nspotify_agent.invoke(user_query)\n```\n\n```output\n\n[1m> Entering new AgentExecutor chain...[0m\n[32;1m[1;3mAction: api_planner\nAction Input: I need to find the right API calls to get a blues song recommendation for the user[0m\nObservation: [36;1m[1;3m1. GET /me to get the current user's information\n2. GET /recommendations/available-genre-seeds to retrieve a list of available genres\n3. GET /recommendations with the seed_genre parameter set to \"blues\" to get a blues song recommendation for the user[0m\nThought:[32;1m[1;3mI have the plan, now I need to execute the API calls.\nAction: api_controller\nAction Input: 1. GET /me to get the current user's information\n2. GET /recommendations/available-genre-seeds to retrieve a list of available genres\n3. GET /recommendations with the seed_genre parameter set to \"blues\" to get a blues song recommendation for the user[0m\n\n[1m> Entering new AgentExecutor chain...[0m\n[32;1m[1;3mAction: requests_get\nAction Input: {\"url\": \"https://api.spotify.com/v1/me\", \"output_instructions\": \"Extract the user's id and username\"}[0m\nObservation: [36;1m[1;3mID: 22rhrz4m4kvpxlsb5hezokzwi, Username: Jeremy Welborn[0m\nThought:[32;1m[1;3mAction: requests_get\nAction Input: {\"url\": \"https://api.spotify.com/v1/recommendations/available-genre-seeds\", \"output_instructions\": \"Extract the list of available genres\"}[0m\nObservation: [36;1m[1;3macoustic, afrobeat, alt-rock, alternative, ambient, anime, black-metal, bluegrass, blues, bossanova, brazil, breakbeat, british, cantopop, chicago-house, children, chill, classical, club, comedy, country, dance, dancehall, death-metal, deep-house, detroit-techno, disco, disney, drum-and-bass, dub, dubstep, edm, electro, electronic, emo, folk, forro, french, funk, garage, german, gospel, goth, grindcore, groove, grunge, guitar, happy, hard-rock, hardcore, hardstyle, heavy-metal, hip-hop, holidays, honky-tonk, house, idm, indian, indie, indie-pop, industrial, iranian, j-dance, j-idol, j-pop, j-rock, jazz, k-pop, kids, latin, latino, malay, mandopop, metal, metal-misc, metalcore, minimal-techno, movies, mpb, new-age, new-release, opera, pagode, party, philippines-[0m\nThought:\n``````output\nRetrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2167437a0072228238f3c0c5b3882764 in your message.).\n``````output\n[32;1m[1;3mAction: requests_get\nAction Input: {\"url\": \"https://api.spotify.com/v1/recommendations?seed_genres=blues\", \"output_instructions\": \"Extract the list of recommended tracks with their ids and names\"}[0m\nObservation: [36;1m[1;3m[\n  {\n    id: '03lXHmokj9qsXspNsPoirR',\n    name: 'Get Away Jordan'\n  }\n][0m\nThought:[32;1m[1;3mI am finished executing the plan.\nFinal Answer: The recommended blues song for user Jeremy Welborn (ID: 22rhrz4m4kvpxlsb5hezokzwi) is \"Get Away Jordan\" with the track ID: 03lXHmokj9qsXspNsPoirR.[0m\n\n[1m> Finished chain.[0m\n\nObservation: [33;1m[1;3mThe recommended blues song for user Jeremy Welborn (ID: 22rhrz4m4kvpxlsb5hezokzwi) is \"Get Away Jordan\" with the track ID: 03lXHmokj9qsXspNsPoirR.[0m\nThought:[32;1m[1;3mI am finished executing the plan and have the information the user asked for.\nFinal Answer: The recommended blues song for you is \"Get Away Jordan\" with the track ID: 03lXHmokj9qsXspNsPoirR.[0m\n\n[1m> Finished chain.[0m\n```\n\n```output\n'The recommended blues song for you is \"Get Away Jordan\" with the track ID: 03lXHmokj9qsXspNsPoirR.'\n```\n\n#### Try another API.\u200b\n\n```python\nheaders = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\nopenai_requests_wrapper = RequestsWrapper(headers=headers)\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/community_api_reference.html", "title": "langchain_community 0.2.19 \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "9f92d047-73be-5e79-bfbb-d42ae017f64f"}, "page_content": "utilities.openapi.OpenAPISpec()\nOpenAPI Model that removes mis-formatted parts of the spec.\n\nutilities.openweathermap.OpenWeatherMapAPIWrapper\nWrapper for OpenWeatherMap API using PyOWM.\n\nutilities.oracleai.OracleSummary(conn,\u00a0params)\nGet Summary :param conn: Oracle Connection, :param params: Summary parameters, :param proxy: Proxy\n\nutilities.outline.OutlineAPIWrapper\nWrapper around OutlineAPI.\n\nutilities.passio_nutrition_ai.ManagedPassioLifeAuth(...)\nManage the token for the NutritionAI API.\n\nutilities.passio_nutrition_ai.NoDiskStorage()\nMixin to prevent storing on disk.\n\nutilities.passio_nutrition_ai.NutritionAIAPI\nWrapper for the Passio Nutrition AI API.\n\nutilities.pebblo.App\nPebblo AI application.\n\nutilities.pebblo.Doc\nPebblo document.\n\nutilities.pebblo.Framework\nPebblo Framework instance.\n\nutilities.pebblo.IndexedDocument\nPebblo Indexed Document.\n\nutilities.pebblo.PebbloLoaderAPIWrapper\nWrapper for Pebblo Loader API.\n\nutilities.pebblo.Routes(value)\nRoutes available for the Pebblo API as enumerator.\n\nutilities.pebblo.Runtime\nPebblo Runtime.\n\nutilities.polygon.PolygonAPIWrapper\nWrapper for Polygon API.\n\nutilities.portkey.Portkey()\nPortkey configuration.\n\nutilities.powerbi.PowerBIDataset\nCreate PowerBI engine from dataset ID and credential or token.\n\nutilities.pubmed.PubMedAPIWrapper\nWrapper around PubMed API.\n\nutilities.reddit_search.RedditSearchAPIWrapper\nWrapper for Reddit API\n\nutilities.redis.TokenEscaper([escape_chars_re])\nEscape punctuation within an input string.\n\nutilities.rememberizer.RememberizerAPIWrapper\nWrapper for Rememberizer APIs.\n\nutilities.requests.GenericRequestsWrapper\nLightweight wrapper around requests library.\n\nutilities.requests.JsonRequestsWrapper\nLightweight wrapper around requests library, with async support.\n\nutilities.requests.Requests\nWrapper around requests to handle auth and async.\n\nutilities.requests.RequestsWrapper\nalias of TextRequestsWrapper\n\nutilities.requests.TextRequestsWrapper\nLightweight wrapper around requests library, with async support.\n\nutilities.scenexplain.SceneXplainAPIWrapper\nWrapper for SceneXplain API.\n\nutilities.searchapi.SearchApiAPIWrapper\nWrapper around SearchApi API.\n\nutilities.searx_search.SearxResults(data)\nDict like wrapper around search api results.\n\nutilities.searx_search.SearxSearchWrapper\nWrapper for Searx API.\n\nutilities.semanticscholar.SemanticScholarAPIWrapper\nWrapper around semanticscholar.org API.\n\nutilities.serpapi.HiddenPrints()\nContext manager to hide prints.\n\nutilities.serpapi.SerpAPIWrapper\nWrapper around SerpAPI.\n\nutilities.spark_sql.SparkSQL([...])\nSparkSQL is a utility class for interacting with Spark SQL.\n\nutilities.sql_database.SQLDatabase(engine[,\u00a0...])\nSQLAlchemy wrapper around a database.\n\nutilities.stackexchange.StackExchangeAPIWrapper\nWrapper for Stack Exchange API.\n\nutilities.steam.SteamWebAPIWrapper\nWrapper for Steam API.\n\nutilities.tavily_search.TavilySearchAPIWrapper\nWrapper for Tavily Search API.\n\nutilities.tensorflow_datasets.TensorflowDatasets\nAccess to the TensorFlow Datasets.\n\nutilities.twilio.TwilioAPIWrapper\nMessaging Client using Twilio.\n\nutilities.wikidata.WikidataAPIWrapper\nWrapper around the Wikidata API.\n\nutilities.wikipedia.WikipediaAPIWrapper\nWrapper around WikipediaAPI.\n\nutilities.wolfram_alpha.WolframAlphaAPIWrapper\nWrapper for Wolfram Alpha.\n\nutilities.you.YouAPIOutput\nOutput from you.com API.\n\nutilities.you.YouDocument\nOutput of parsing one snippet.\n\nutilities.you.YouHit\nA single hit from you.com, which may contain multiple snippets\n\nutilities.you.YouHitMetadata\nMetadata on a single hit from you.com\n\nutilities.you.YouSearchAPIWrapper\nWrapper for you.com Search and News API.\n\nutilities.zapier.ZapierNLAWrapper\nWrapper for Zapier NLA.\n\nFunctions\u00b6\n\nutilities.anthropic.get_num_tokens_anthropic(text)\nGet the number of tokens in a string of text.\n\nutilities.anthropic.get_token_ids_anthropic(text)\nGet the token ids for a string of text.\n\nutilities.cassandra.aexecute_cql(session,\u00a0...)\nExecute a CQL query asynchronously.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html", "title": "langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "bd748c13-29d2-5b67-8c59-5b17dccd0c0d"}, "page_content": "Example: Dispatch Custom Event\nfrom langchain_core.callbacks.manager import (\n    adispatch_custom_event,\n)\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\nimport asyncio\n\nasync def slow_thing(some_input: str, config: RunnableConfig) -> str:\n    \"\"\"Do something that takes a long time.\"\"\"\n    await asyncio.sleep(1) # Placeholder for some slow operation\n    await adispatch_custom_event(\n        \"progress_event\",\n        {\"message\": \"Finished step 1 of 3\"},\n        config=config # Must be included for python < 3.10\n    )\n    await asyncio.sleep(1) # Placeholder for some slow operation\n    await adispatch_custom_event(\n        \"progress_event\",\n        {\"message\": \"Finished step 2 of 3\"},\n        config=config # Must be included for python < 3.10\n    )\n    await asyncio.sleep(1) # Placeholder for some slow operation\n    return \"Done\"\n\nslow_thing = RunnableLambda(slow_thing)\n\nasync for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n    print(event)\n\nParameters\n\ninput (Any) \u2013 The input to the Runnable.\nconfig (Optional[RunnableConfig]) \u2013 The config to use for the Runnable.\nversion (Literal['v1', 'v2']) \u2013 The version of the schema to use either v2 or v1.\nUsers should use v2.\nv1 is for backwards compatibility and will be deprecated\nin 0.4.0.\nNo default will be assigned until the API is stabilized.\ncustom events will only be surfaced in v2.\ninclude_names (Optional[Sequence[str]]) \u2013 Only include events from runnables with matching names.\ninclude_types (Optional[Sequence[str]]) \u2013 Only include events from runnables with matching types.\ninclude_tags (Optional[Sequence[str]]) \u2013 Only include events from runnables with matching tags.\nexclude_names (Optional[Sequence[str]]) \u2013 Exclude events from runnables with matching names.\nexclude_types (Optional[Sequence[str]]) \u2013 Exclude events from runnables with matching types.\nexclude_tags (Optional[Sequence[str]]) \u2013 Exclude events from runnables with matching tags.\nkwargs (Any) \u2013 Additional keyword arguments to pass to the Runnable.\nThese will be passed to astream_log as this implementation\nof astream_events is built on top of astream_log.\n\nYields\nAn async stream of StreamEvents.\n\nRaises\nNotImplementedError \u2013 If the version is not v1 or v2.\n\nReturn type\nAsyncIterator[Union[StandardStreamEvent, CustomStreamEvent]]\n\nbatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) \u2192 List[Output]\u00b6\nDefault implementation runs invoke in parallel using a thread pool executor.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses should override this method if they can batch more efficiently;\ne.g., if the underlying Runnable uses an API which supports a batch mode.\n\nParameters\n\ninputs (List[Input]) \u2013 \nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) \u2013 \nreturn_exceptions (bool) \u2013 \nkwargs (Optional[Any]) \u2013 \n\nReturn type\nList[Output]\n\nbatch_as_completed(inputs: Sequence[Input], config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) \u2192 Iterator[Tuple[int, Union[Output, Exception]]]\u00b6\nRun invoke in parallel on a list of inputs,\nyielding results as they complete.\n\nParameters\n\ninputs (Sequence[Input]) \u2013 \nconfig (Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]) \u2013 \nreturn_exceptions (bool) \u2013 \nkwargs (Optional[Any]) \u2013 \n\nReturn type\nIterator[Tuple[int, Union[Output, Exception]]]\n\ncombine_docs(docs: List[Document], token_max: Optional[int] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) \u2192 Tuple[str, dict][source]\u00b6\nCombine documents in a map reduce manner.\nCombine by mapping first chain over all documents, then reducing the results.\nThis reducing can be done recursively if needed (if there are many documents).\n\nParameters", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser.html", "title": "langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "6528c62f-a30e-5793-bd9c-a60732ceba11"}, "page_content": "langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17\n\nThis is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\n\nLangChain\n\nCore\n\nCommunity\n\nExperimental\n\nText splitters\n\nai21\n\nairbyte\n\nanthropic\n\nastradb\n\naws\n\nazure-dynamic-sessions\n\nbox\n\nchroma\n\ncohere\n\ncouchbase\n\nelasticsearch\n\nexa\n\nfireworks\n\ngoogle-community\n\ngoogle-genai\n\ngoogle-vertexai\n\ngroq\n\nhuggingface\n\nibm\n\nmilvus\n\nmistralai\n\nmongodb\n\nnomic\n\nnvidia-ai-endpoints\n\nollama\n\nopenai\n\npinecone\n\npostgres\n\nprompty\n\nqdrant\n\nrobocorp\n\ntogether\n\nunstructured\n\nvoyageai\n\nweaviate\n\nPartner libs\n\nai21\nairbyte\nanthropic\nastradb\naws\nazure-dynamic-sessions\nbox\nchroma\ncohere\ncouchbase\nelasticsearch\nexa\nfireworks\ngoogle-community\ngoogle-genai\ngoogle-vertexai\ngroq\nhuggingface\nibm\nmilvus\nmistralai\nmongodb\nnomic\nnvidia-ai-endpoints\nollama\nopenai\npinecone\npostgres\nprompty\nqdrant\nrobocorp\ntogether\nunstructured\nvoyageai\nweaviate\n\nDocs\n\nToggle Menu\n\nlangchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser\nPairwiseStringResultOutputParser\nPairwiseStringResultOutputParser._type\nPairwiseStringResultOutputParser.abatch()\nPairwiseStringResultOutputParser.abatch_as_completed()\nPairwiseStringResultOutputParser.ainvoke()\nPairwiseStringResultOutputParser.aparse()\nPairwiseStringResultOutputParser.aparse_result()\nPairwiseStringResultOutputParser.as_tool()\nPairwiseStringResultOutputParser.astream()\nPairwiseStringResultOutputParser.astream_events()\nPairwiseStringResultOutputParser.batch()\nPairwiseStringResultOutputParser.batch_as_completed()\nPairwiseStringResultOutputParser.configurable_alternatives()\nPairwiseStringResultOutputParser.configurable_fields()\nPairwiseStringResultOutputParser.get_format_instructions()\nPairwiseStringResultOutputParser.invoke()\nPairwiseStringResultOutputParser.parse()\nPairwiseStringResultOutputParser.parse_result()\nPairwiseStringResultOutputParser.parse_with_prompt()\nPairwiseStringResultOutputParser.stream()\nPairwiseStringResultOutputParser.to_json()\n\nlangchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser\u00b6\n\nNote\nPairwiseStringResultOutputParser implements the standard Runnable Interface. \ud83c\udfc3\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\n\nclass langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser[source]\u00b6\nBases: BaseOutputParser[dict]\nA parser for the output of the PairwiseStringEvalChain.\n\n_type\u00b6\nThe type of the output parser.\n\nType\nstr\n\nasync abatch(inputs: List[Input], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Optional[Any]) \u2192 List[Output]\u00b6\nDefault implementation runs ainvoke in parallel using asyncio.gather.\nThe default implementation of batch works well for IO bound runnables.\nSubclasses should override this method if they can batch more efficiently;\ne.g., if the underlying Runnable uses an API which supports a batch mode.\n\nParameters\n\ninputs (List[Input]) \u2013 A list of inputs to the Runnable.\nconfig (Optional[Union[RunnableConfig, List[RunnableConfig]]]) \u2013 A config to use when invoking the Runnable.\nThe config supports standard keys like \u2018tags\u2019, \u2018metadata\u2019 for tracing\npurposes, \u2018max_concurrency\u2019 for controlling how much work to do\nin parallel, and other keys. Please refer to the RunnableConfig\nfor more details. Defaults to None.\nreturn_exceptions (bool) \u2013 Whether to return exceptions instead of raising them.\nDefaults to False.\nkwargs (Optional[Any]) \u2013 Additional keyword arguments to pass to the Runnable.\n\nReturns\nA list of outputs from the Runnable.\n\nReturn type\nList[Output]", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html", "title": "langchain.chains.combine_documents.reduce.ReduceDocumentsChain \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "f75fa5fa-1ead-554e-88de-cba0ade4e815"}, "page_content": "Yields\nA tuple of the index of the input and the output from the Runnable.\n\nReturn type\nAsyncIterator[Tuple[int, Union[Output, Exception]]]\n\nasync acall(inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) \u2192 Dict[str, Any]\u00b6\n\nDeprecated since version langchain==0.1.0: Use ainvoke instead.\n\nAsynchronously execute the chain.\n\nParameters\n\ninputs (Union[Dict[str, Any], Any]) \u2013 Dictionary of inputs, or single input if chain expects\nonly one param. Should contain all inputs specified in\nChain.input_keys except for inputs that will be set by the chain\u2019s\nmemory.\nreturn_only_outputs (bool) \u2013 Whether to return only outputs in the\nresponse. If True, only new keys generated by this chain will be\nreturned. If False, both input keys and new keys generated by this\nchain will be returned. Defaults to False.\ncallbacks (Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]) \u2013 Callbacks to use for this chain run. These will be called in\naddition to callbacks passed to the chain during construction, but only\nthese runtime callbacks will propagate to calls to other objects.\ntags (Optional[List[str]]) \u2013 List of string tags to pass to all callbacks. These will be passed in\naddition to tags passed to the chain during construction, but only\nthese runtime tags will propagate to calls to other objects.\nmetadata (Optional[Dict[str, Any]]) \u2013 Optional metadata associated with the chain. Defaults to None\ninclude_run_info (bool) \u2013 Whether to include run info in the response. Defaults\nto False.\nrun_name (Optional[str]) \u2013 \n\nReturns\n\nA dict of named outputs. Should contain all outputs specified inChain.output_keys.\n\nReturn type\nDict[str, Any]\n\nasync acombine_docs(docs: List[Document], token_max: Optional[int] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) \u2192 Tuple[str, dict][source]\u00b6\nAsync combine multiple documents recursively.\n\nParameters\n\ndocs (List[Document]) \u2013 List of documents to combine, assumed that each one is less than\ntoken_max.\ntoken_max (Optional[int]) \u2013 Recursively creates groups of documents less than this number\nof tokens.\ncallbacks (Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]) \u2013 Callbacks to be passed through\n**kwargs (Any) \u2013 additional parameters to be passed to LLM calls (like other\ninput variables besides the documents)\n\nReturns\nThe first element returned is the single string output. The second\nelement returned is a dictionary of other keys to return.\n\nReturn type\nTuple[str, dict]\n\nasync ainvoke(input: Dict[str, Any], config: Optional[RunnableConfig] = None, **kwargs: Any) \u2192 Dict[str, Any]\u00b6\nDefault implementation of ainvoke, calls invoke from a thread.\nThe default implementation allows usage of async code even if\nthe Runnable did not implement a native async version of invoke.\nSubclasses should override this method if they can run asynchronously.\n\nParameters\n\ninput (Dict[str, Any]) \u2013 \nconfig (Optional[RunnableConfig]) \u2013 \nkwargs (Any) \u2013 \n\nReturn type\nDict[str, Any]\n\napply(input_list: List[Dict[str, Any]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None) \u2192 List[Dict[str, str]]\u00b6\n\nDeprecated since version langchain==0.1.0: Use batch instead.\n\nCall the chain on all inputs in the list.\n\nParameters\n\ninput_list (List[Dict[str, Any]]) \u2013 \ncallbacks (Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]) \u2013 \n\nReturn type\nList[Dict[str, str]]\n\nasync aprep_inputs(inputs: Union[Dict[str, Any], Any]) \u2192 Dict[str, str]\u00b6\nPrepare chain inputs, including adding inputs from memory.", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful."}, {"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_glVSpiYKgEWkKSxgMPA1BHcD", "function": {"arguments": "{\"steps\":[\"Update the `final_action` method to concatenate the API response with the last message from the previous node.\",\"Rename the `final_action` node to `policy_check_action` for better clarity.\",\"Modify the `call_policy_api` method to return the URLs of the policies instead of a string message.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-41963906-a901-41b1-89a3-f865f129e005", "example": false, "tool_calls": [{"name": "Plan", "args": {"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}, "id": "call_glVSpiYKgEWkKSxgMPA1BHcD", "type": "tool_call"}], "invalid_tool_calls": []}}
{"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": "delete", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": [], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"question": "Update the `final_action` method to concatenate the API response with the last message from the previous node."}
{"question": "Update the `final_action` method to concatenate the API response with the last message from the previous node."}
{"input": {"question": "Update the `final_action` method to concatenate the API response with the last message from the previous node.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Update the `final_action` method to concatenate the API response with the last message from the previous node."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Update the `final_action` method to concatenate the API response with the last message from the previous node.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_F97r20IRhBO8K5vmjsu1t1mB", "function": {"arguments": "{\"queries\":[\"how to concatenate strings in Python\",\"Python API response handling\",\"best practices for updating methods in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-e1faadbe-c463-4517-a212-d3c72063254c", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}, "id": "call_F97r20IRhBO8K5vmjsu1t1mB", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}
{"input": {"question": "Update the `final_action` method to concatenate the API response with the last message from the previous node.", "queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"], "documents": []}}
{"input": {"query": "how to concatenate strings in Python"}}
{"query": "how to concatenate strings in Python"}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}]}
{"input": {"query": "Python API response handling"}}
{"query": "Python API response handling"}
{"documents": [{"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}]}
{"input": {"query": "best practices for updating methods in Python"}}
{"query": "best practices for updating methods in Python"}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}], "steps": ["Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"question": "Rename the `final_action` node to `policy_check_action` for better clarity."}
{"question": "Rename the `final_action` node to `policy_check_action` for better clarity."}
{"input": {"question": "Rename the `final_action` node to `policy_check_action` for better clarity.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Rename the `final_action` node to `policy_check_action` for better clarity."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Rename the `final_action` node to `policy_check_action` for better clarity.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_VBnRpwF00YrEWl8YDjkE62Jd", "function": {"arguments": "{\"queries\":[\"how to rename a node in a programming context\",\"best practices for naming nodes in code\",\"importance of clear naming conventions in programming\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-102cd220-d41a-4600-b2d7-f4d0bbc8caf3", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}, "id": "call_VBnRpwF00YrEWl8YDjkE62Jd", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}
{"input": {"question": "Rename the `final_action` node to `policy_check_action` for better clarity.", "queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"], "documents": []}}
{"input": {"query": "how to rename a node in a programming context"}}
{"query": "how to rename a node in a programming context"}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}]}
{"input": {"query": "best practices for naming nodes in code"}}
{"query": "best practices for naming nodes in code"}
{"documents": [{"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}]}
{"input": {"query": "importance of clear naming conventions in programming"}}
{"query": "importance of clear naming conventions in programming"}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}]}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}], "steps": ["Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"question": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."}
{"question": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."}
{"input": {"question": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_2XUi37EfO58MXkEUCZ2cnYSS", "function": {"arguments": "{\"queries\":[\"how to modify a method in Python to return a list of URLs\",\"Python return values from a method\",\"best practices for API response formatting in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f7fd7489-b7dc-44a7-869d-dc009f7f6e7a", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}, "id": "call_2XUi37EfO58MXkEUCZ2cnYSS", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}
{"input": {"question": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message.", "queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"], "documents": []}}
{"input": {"query": "how to modify a method in Python to return a list of URLs"}}
{"query": "how to modify a method in Python to return a list of URLs"}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}]}
{"input": {"query": "Python return values from a method"}}
{"query": "Python return values from a method"}
{"documents": [{"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}]}
{"input": {"query": "best practices for API response formatting in Python"}}
{"query": "best practices for API response formatting in Python"}
{"documents": [{"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "steps": []}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are an expert programmer and problem-solver, tasked with answering any question about LangChain.\n\nGenerate a comprehensive and informative answer for the given question based solely on the provided search results (URL and content). Do NOT ramble, and adjust your response length based on the question. If they ask a question that can be answered in one sentence, do that. If 5 paragraphs of detail is needed, do that. You must only use information from the provided search results. Use an unbiased and journalistic tone. Combine search results together into a coherent answer. Do not repeat text. Cite search results using [${number}] notation. Only cite the most relevant results that answer the question accurately. Place these citations at the end of the individual sentence or paragraph that reference them. Do not put them all at the end, but rather sprinkle them throughout. If different results refer to different entities within the same name, write separate answers for each entity.\n\nYou should use bullet points in your answer for readability. Put citations where they apply rather than putting them all at the end. DO NOT PUT THEM ALL THAT END, PUT THEM IN THE BULLET POINTS.\n\nIf there is nothing in the context relevant to the question at hand, do NOT make up an answer. Rather, tell them why you're unsure and ask for any additional information that may help you answer better.\n\nSometimes, what a user is asking may NOT be possible. Do NOT tell them that things are possible if you don't see evidence for it in the context below. If you don't see based in the information below that something is possible, do NOT say that it is - instead say that you're not sure.\n\nAnything between the following `context` html blocks is retrieved from a knowledge bank, not part of the conversation with the user.\n\n<context>\n    <documents>\n<document  description='This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/recursive_text_splitter/' title='How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/recursive_text_splitter/' uuid='2a474310-58dd-588a-8a83-f69b18560da6'>\n- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)\n</document>\n<document  language='en' description='MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/' title='MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/' uuid='c3a47ddd-79a9-54e8-b337-8e2921003905'>\n```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)\n</document>\n<document  language='en' description='Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/' title='SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/' uuid='58000905-a1e1-50d7-9c14-d8718b31d860'>\n```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)\n</document>\n<document  description='ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/chat/ibm_watsonx/' title='ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/chat/ibm_watsonx/' uuid='be87491d-12a3-5b60-8bb9-2c73dcbd4c0c'>\n```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```\n</document>\n<document  description='You are passing too many, too few, or mismatched ToolMessages to a model.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' title='INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' uuid='f407f6b8-de0b-524d-832b-957b0cfe561d'>\n1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```\n</document>\n<document  description='You are passing too many, too few, or mismatched ToolMessages to a model.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' title='INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' uuid='af64dba8-1a60-5892-aaf6-95ebcbf323d1'>\n1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```\n</document>\n<document  description='you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/retrievers/you-retriever/' title='You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/retrievers/you-retriever/' uuid='70ba3a01-9214-56f1-970b-adcee018ef60'>\n```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```\n</document>\n<document  description=\"By themselves, language models can't take actions - they just output text.\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/agents/' title='Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/agents/' uuid='7591f837-705f-5c9b-97b3-3856a7187e44'>\n```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```\n</document>\n<document  description='You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically' title='How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='65cd272f-88db-5377-80b6-a6e1210414ed'>\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n<document  description='You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically' title='How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='02d58a65-2ae7-548d-9665-5784c40f176e'>\nList examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/concepts/memory/' title='Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/concepts/memory/' uuid='a0d5b93f-8557-52fc-b343-cb64956ee4f3'>\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments\n</document>\n<document  language='en' description='<Prerequisites titlesAndLinks={[' changefreq='weekly' source='https://python.langchain.com/docs/how_to/tool_runtime/' title='How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/tool_runtime/' uuid='a6c56983-ca28-50d5-b299-4b26180bbaa6'>\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/reference/graphs/' title='Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/reference/graphs/' uuid='c6f90d44-d75d-5850-8e21-fe4a62bc6f11'>\nExamples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self\n</document>\n<document  language='' description='' changefreq=None source='https://langchain-ai.github.io/langgraph/reference/graphs/' title='Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/reference/graphs/' uuid='6588b9de-33e7-5c51-be6f-0b5238fb6fd9'>\nAdds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n</document>\n<document  language='en' description='langgraph' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph' title='How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='26ce0504-6c6e-55aa-9514-8d7e3327eac3'>\nRunning and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b\n</document>\n<document  language='en' description='Neo4j is a graph database management system developed by Neo4j, Inc.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/' title='Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/' uuid='5eb12c5d-6706-58e5-8e62-9c3549e6e7bd'>\n- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='7b547e20-3a35-5cb0-881c-341c25231f06'>\nof the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='8af62e09-443d-5cc9-9c24-a8bd13563ed0'>\nwill write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files\n</document>\n<document  language='en' description=\"LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.\" changefreq=None source='https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api' title='Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='cd89709f-0016-5447-9ab3-2e9efb7db0fa'>\nSample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.\n</document>\n<document  description='Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/exa_search/' title='Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/exa_search/' uuid='4997eea3-74a6-5adb-ae92-6c424b87b63e'>\nmanaging ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,\n</document>\n</documents>\n<context/>", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527"}}]]}
{"messages": [{"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "num_response_attempts": 1}
{"input": {"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."}, {"role": "human", "content": "Set of facts: \n\n <documents>\n<document  description='This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/recursive_text_splitter/' title='How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/recursive_text_splitter/' uuid='2a474310-58dd-588a-8a83-f69b18560da6'>\n- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)\n</document>\n<document  language='en' description='MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/' title='MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/' uuid='c3a47ddd-79a9-54e8-b337-8e2921003905'>\n```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)\n</document>\n<document  language='en' description='Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/' title='SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/' uuid='58000905-a1e1-50d7-9c14-d8718b31d860'>\n```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)\n</document>\n<document  description='ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/chat/ibm_watsonx/' title='ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/chat/ibm_watsonx/' uuid='be87491d-12a3-5b60-8bb9-2c73dcbd4c0c'>\n```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```\n</document>\n<document  description='You are passing too many, too few, or mismatched ToolMessages to a model.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' title='INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' uuid='f407f6b8-de0b-524d-832b-957b0cfe561d'>\n1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```\n</document>\n<document  description='You are passing too many, too few, or mismatched ToolMessages to a model.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' title='INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' uuid='af64dba8-1a60-5892-aaf6-95ebcbf323d1'>\n1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```\n</document>\n<document  description='you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/retrievers/you-retriever/' title='You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/retrievers/you-retriever/' uuid='70ba3a01-9214-56f1-970b-adcee018ef60'>\n```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```\n</document>\n<document  description=\"By themselves, language models can't take actions - they just output text.\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/agents/' title='Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/agents/' uuid='7591f837-705f-5c9b-97b3-3856a7187e44'>\n```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```\n</document>\n<document  description='You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically' title='How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='65cd272f-88db-5377-80b6-a6e1210414ed'>\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n<document  description='You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically' title='How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='02d58a65-2ae7-548d-9665-5784c40f176e'>\nList examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/concepts/memory/' title='Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/concepts/memory/' uuid='a0d5b93f-8557-52fc-b343-cb64956ee4f3'>\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments\n</document>\n<document  language='en' description='<Prerequisites titlesAndLinks={[' changefreq='weekly' source='https://python.langchain.com/docs/how_to/tool_runtime/' title='How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/tool_runtime/' uuid='a6c56983-ca28-50d5-b299-4b26180bbaa6'>\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/reference/graphs/' title='Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/reference/graphs/' uuid='c6f90d44-d75d-5850-8e21-fe4a62bc6f11'>\nExamples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self\n</document>\n<document  language='' description='' changefreq=None source='https://langchain-ai.github.io/langgraph/reference/graphs/' title='Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/reference/graphs/' uuid='6588b9de-33e7-5c51-be6f-0b5238fb6fd9'>\nAdds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n</document>\n<document  language='en' description='langgraph' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph' title='How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='26ce0504-6c6e-55aa-9514-8d7e3327eac3'>\nRunning and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b\n</document>\n<document  language='en' description='Neo4j is a graph database management system developed by Neo4j, Inc.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/' title='Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/' uuid='5eb12c5d-6706-58e5-8e62-9c3549e6e7bd'>\n- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='7b547e20-3a35-5cb0-881c-341c25231f06'>\nof the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='8af62e09-443d-5cc9-9c24-a8bd13563ed0'>\nwill write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files\n</document>\n<document  language='en' description=\"LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.\" changefreq=None source='https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api' title='Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='cd89709f-0016-5447-9ab3-2e9efb7db0fa'>\nSample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.\n</document>\n<document  description='Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/exa_search/' title='Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/exa_search/' uuid='4997eea3-74a6-5adb-ae92-6c424b87b63e'>\nmanaging ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,\n</document>\n</documents> \n\n LLM generation: Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Set of facts: \n\n <documents>\n<document  description='This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/recursive_text_splitter/' title='How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/recursive_text_splitter/' uuid='2a474310-58dd-588a-8a83-f69b18560da6'>\n- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)\n</document>\n<document  language='en' description='MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/' title='MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/' uuid='c3a47ddd-79a9-54e8-b337-8e2921003905'>\n```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)\n</document>\n<document  language='en' description='Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/' title='SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/' uuid='58000905-a1e1-50d7-9c14-d8718b31d860'>\n```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)\n</document>\n<document  description='ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/chat/ibm_watsonx/' title='ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/chat/ibm_watsonx/' uuid='be87491d-12a3-5b60-8bb9-2c73dcbd4c0c'>\n```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```\n</document>\n<document  description='You are passing too many, too few, or mismatched ToolMessages to a model.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' title='INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' uuid='f407f6b8-de0b-524d-832b-957b0cfe561d'>\n1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```\n</document>\n<document  description='You are passing too many, too few, or mismatched ToolMessages to a model.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' title='INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/' uuid='af64dba8-1a60-5892-aaf6-95ebcbf323d1'>\n1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```\n</document>\n<document  description='you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/retrievers/you-retriever/' title='You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/retrievers/you-retriever/' uuid='70ba3a01-9214-56f1-970b-adcee018ef60'>\n```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```\n</document>\n<document  description=\"By themselves, language models can't take actions - they just output text.\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/agents/' title='Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/agents/' uuid='7591f837-705f-5c9b-97b3-3856a7187e44'>\n```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```\n</document>\n<document  description='You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically' title='How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='65cd272f-88db-5377-80b6-a6e1210414ed'>\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n<document  description='You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically' title='How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='02d58a65-2ae7-548d-9665-5784c40f176e'>\nList examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/concepts/memory/' title='Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/concepts/memory/' uuid='a0d5b93f-8557-52fc-b343-cb64956ee4f3'>\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments\n</document>\n<document  language='en' description='<Prerequisites titlesAndLinks={[' changefreq='weekly' source='https://python.langchain.com/docs/how_to/tool_runtime/' title='How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/tool_runtime/' uuid='a6c56983-ca28-50d5-b299-4b26180bbaa6'>\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/reference/graphs/' title='Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/reference/graphs/' uuid='c6f90d44-d75d-5850-8e21-fe4a62bc6f11'>\nExamples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self\n</document>\n<document  language='' description='' changefreq=None source='https://langchain-ai.github.io/langgraph/reference/graphs/' title='Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/reference/graphs/' uuid='6588b9de-33e7-5c51-be6f-0b5238fb6fd9'>\nAdds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n</document>\n<document  language='en' description='langgraph' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph' title='How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='26ce0504-6c6e-55aa-9514-8d7e3327eac3'>\nRunning and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b\n</document>\n<document  language='en' description='Neo4j is a graph database management system developed by Neo4j, Inc.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/' title='Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/' uuid='5eb12c5d-6706-58e5-8e62-9c3549e6e7bd'>\n- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='7b547e20-3a35-5cb0-881c-341c25231f06'>\nof the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='8af62e09-443d-5cc9-9c24-a8bd13563ed0'>\nwill write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files\n</document>\n<document  language='en' description=\"LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.\" changefreq=None source='https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api' title='Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='cd89709f-0016-5447-9ab3-2e9efb7db0fa'>\nSample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.\n</document>\n<document  description='Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/exa_search/' title='Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/exa_search/' uuid='4997eea3-74a6-5adb-ae92-6c424b87b63e'>\nmanaging ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,\n</document>\n</documents> \n\n LLM generation: Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_j3zt8hgpY8XRjRo4eG6GWGT6", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-fd581450-17d8-456d-8095-e0c52becd073", "example": false, "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_j3zt8hgpY8XRjRo4eG6GWGT6", "type": "tool_call"}], "invalid_tool_calls": []}}
