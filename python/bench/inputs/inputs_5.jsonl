{"messages": [{"role": "user", "content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}"}]}
{"messages": [{"role": "user", "content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}"}]}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "ab9bc7fa-8835-5f03-96a7-88a5d87c686c"}, "page_content": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Neo4j\n\n[Neo4j](https://neo4j.com/docs/getting-started/) is a graph database management system developed by `Neo4j, Inc`.\n\nThe data elements `Neo4j` stores are nodes, edges connecting them, and attributes of nodes and edges. Described by its developers as an ACID-compliant transactional database with native graph storage and processing, `Neo4j` is available in a non-open-source \"community edition\" licensed with a modification of the GNU General Public License, with online backup and high availability extensions licensed under a closed-source commercial license. Neo also licenses `Neo4j` with these extensions under closed-source commercial terms.\n\nThis notebook shows how to use LLMs to provide a natural language interface to a graph database you can query with the `Cypher` query language.\n\n[Cypher](https://en.wikipedia.org/wiki/Cypher_(query_language)) is a declarative graph query language that allows for expressive and efficient data querying in a property graph.\n\n## Setting up\u200b\n\nYou will need to have a running `Neo4j` instance. One option is to create a [free Neo4j database instance in their Aura cloud service](https://neo4j.com/cloud/platform/aura-graph-database/). You can also run the database locally using the [Neo4j Desktop application](https://neo4j.com/download/), or running a docker container.\nYou can run a local docker container by running the executing the following script:\n\n```text\ndocker run \\\n    --name neo4j \\\n    -p 7474:7474 -p 7687:7687 \\\n    -d \\\n    -e NEO4J_AUTH=neo4j/password \\\n    -e NEO4J_PLUGINS=\\[\\\"apoc\\\"\\]  \\\n    neo4j:latest\n```\n\nIf you are using the docker container, you need to wait a couple of second for the database to start.\n\n```python\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"password\")\n```\n\nWe default to OpenAI models in this guide.\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n## Seeding the database\u200b\n\nAssuming your database is empty, you can populate it using Cypher query language. The following Cypher statement is idempotent, which means the database information will be the same if you run it one or multiple times.\n\n```python\ngraph.query(\n    \"\"\"\nMERGE (m:Movie {name:\"Top Gun\", runtime: 120})\nWITH m\nUNWIND [\"Tom Cruise\", \"Val Kilmer\", \"Anthony Edwards\", \"Meg Ryan\"] AS actor\nMERGE (a:Actor {name:actor})\nMERGE (a)-[:ACTED_IN]->(m)\n\"\"\"\n)\n```\n\n```output\n[]\n```\n\n## Refresh graph schema information\u200b\n\nIf the schema of database changes, you can refresh the schema information needed to generate Cypher statements.\n\n```python\ngraph.refresh_schema()\n```\n\n```python\nprint(graph.schema)\n```\n\n```output\nNode properties:\nMovie {runtime: INTEGER, name: STRING}\nActor {name: STRING}\nRelationship properties:\n\nThe relationships:\n(:Actor)-[:ACTED_IN]->(:Movie)\n```\n\n## Enhanced schema information\u200b\n\nChoosing the enhanced schema version enables the system to automatically scan for example values within the databases and calculate some distribution metrics. For example, if a node property has less than 10 distinct values, we return all possible values in the schema. Otherwise, return only a single example value per node and relationship property.\n\n```python\nenhanced_graph = Neo4jGraph(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"password\",\n    enhanced_schema=True,\n)\nprint(enhanced_graph.schema)\n```", "type": "Document"}, {"metadata": {"description": "What is Neo4j?", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/neo4j/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/neo4j/", "uuid": "05d192b9-f554-515f-a926-52de8266b813"}, "page_content": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Neo4j\n\nWhat is `Neo4j`?\n\n- Neo4j is an `open-source database management system` that specializes in graph database technology.\n\n- Neo4j allows you to represent and store data in nodes and edges, making it ideal for handling connected data and relationships.\n\n- Neo4j provides a `Cypher Query Language`, making it easy to interact with and query your graph data.\n\n- With Neo4j, you can achieve high-performance `graph traversals and queries`, suitable for production-level systems.\n\nGet started with Neo4j by visiting [their website](https://neo4j.com/).\n\n## Installation and Setup\u200b\n\n- Install the Python SDK with `pip install neo4j langchain-neo4j`\n\n## VectorStore\u200b\n\nThe Neo4j vector index is used as a vectorstore,\nwhether for semantic search or example selection.\n\n```python\nfrom langchain_neo4j import Neo4jVector\n```\n\nSee a [usage example](/docs/integrations/vectorstores/neo4jvector/)\n\n## GraphCypherQAChain\u200b\n\nThere exists a wrapper around Neo4j graph database that allows you to generate Cypher statements based on the user input\nand use them to retrieve relevant information from the database.\n\n```python\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n```\n\nSee a [usage example](/docs/integrations/graphs/neo4j_cypher/)\n\n## Constructing a knowledge graph from text\u200b\n\nText data often contain rich relationships and insights that can be useful for various analytics, recommendation engines, or knowledge management applications.\nDiffbot's NLP API allows for the extraction of entities, relationships, and semantic meaning from unstructured text data.\nBy coupling Diffbot's NLP API with Neo4j, a graph database, you can create powerful, dynamic graph structures based on the information extracted from text.\nThese graph structures are fully queryable and can be integrated into various applications.\n\n```python\nfrom langchain_neo4j import Neo4jGraph\nfrom langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n```\n\n**API Reference:**[DiffbotGraphTransformer](https://python.langchain.com/api_reference/experimental/graph_transformers/langchain_experimental.graph_transformers.diffbot.DiffbotGraphTransformer.html)\nSee a [usage example](/docs/integrations/graphs/diffbot/)\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/neo4j_chat_message_history/).\n\n```python\nfrom langchain_neo4j import Neo4jChatMessageHistory\n```\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [VectorStore](#vectorstore)\n\n- [GraphCypherQAChain](#graphcypherqachain)\n\n- [Constructing a knowledge graph from text](#constructing-a-knowledge-graph-from-text)\n\n- [Memory](#memory)", "type": "Document"}, {"metadata": {"language": "en", "description": "Diffbot is a suite of ML-based products that make it easy to structure web data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/diffbot/", "title": "Diffbot | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/diffbot/", "uuid": "050f3e16-aae7-5adb-8835-64237a5fae26"}, "page_content": "```text\ndocker run \\\n    --name neo4j \\\n    -p 7474:7474 -p 7687:7687 \\\n    -d \\\n    -e NEO4J_AUTH=neo4j/password \\\n    -e NEO4J_PLUGINS=\\[\\\"apoc\\\"\\]  \\\n    neo4j:latest\n```\n\nIf you are using the docker container, you need to wait a couple of second for the database to start.\n\n```python\nfrom langchain_neo4j import Neo4jGraph\n\nurl = \"bolt://localhost:7687\"\nusername = \"neo4j\"\npassword = \"password\"\n\ngraph = Neo4jGraph(url=url, username=username, password=password)\n```\n\nThe `GraphDocuments` can be loaded into a knowledge graph using the `add_graph_documents` method.\n\n```python\ngraph.add_graph_documents(graph_documents)\n```\n\n## Refresh graph schema information\u200b\n\nIf the schema of database changes, you can refresh the schema information needed to generate Cypher statements\n\n```python\ngraph.refresh_schema()\n```\n\n## Querying the graph\u200b\n\nWe can now use the graph cypher QA chain to ask question of the graph. It is advisable to use **gpt-4** to construct Cypher queries to get the best experience.\n\n```python\nfrom langchain_neo4j import GraphCypherQAChain\nfrom langchain_openai import ChatOpenAI\n\nchain = GraphCypherQAChain.from_llm(\n    cypher_llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"),\n    qa_llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"),\n    graph=graph,\n    verbose=True,\n    allow_dangerous_requests=True,\n)\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain.run(\"Which university did Warren Buffett attend?\")\n```\n\n```output\n\n[1m> Entering new GraphCypherQAChain chain...[0m\nGenerated Cypher:\n[32;1m[1;3mMATCH (p:Person {name: \"Warren Buffett\"})-[:EDUCATED_AT]->(o:Organization)\nRETURN o.name[0m\nFull Context:\n[32;1m[1;3m[{'o.name': 'New York Institute of Finance'}, {'o.name': 'Alice Deal Junior High School'}, {'o.name': 'Woodrow Wilson High School'}, {'o.name': 'University of Nebraska'}][0m\n\n[1m> Finished chain.[0m\n```\n\n```output\n'Warren Buffett attended the University of Nebraska.'\n```\n\n```python\nchain.run(\"Who is or was working at Berkshire Hathaway?\")\n```\n\n```output\n\n[1m> Entering new GraphCypherQAChain chain...[0m\nGenerated Cypher:\n[32;1m[1;3mMATCH (p:Person)-[r:EMPLOYEE_OR_MEMBER_OF]->(o:Organization) WHERE o.name = 'Berkshire Hathaway' RETURN p.name[0m\nFull Context:\n[32;1m[1;3m[{'p.name': 'Charlie Munger'}, {'p.name': 'Oliver Chace'}, {'p.name': 'Howard Buffett'}, {'p.name': 'Howard'}, {'p.name': 'Susan Buffett'}, {'p.name': 'Warren Buffett'}][0m\n\n[1m> Finished chain.[0m\n```\n\n```output\n'Charlie Munger, Oliver Chace, Howard Buffett, Susan Buffett, and Warren Buffett are or were working at Berkshire Hathaway.'\n```\n\n#### Was this page helpful?\n\n- [Use case](#use-case)\n\n- [Overview](#overview)\n\n- [Setting up](#setting-up)- [Diffbot NLP API](#diffbot-nlp-api)\n\n- [Loading the data into a knowledge graph](#loading-the-data-into-a-knowledge-graph)\n\n- [Refresh graph schema information](#refresh-graph-schema-information)\n\n- [Querying the graph](#querying-the-graph)", "type": "Document"}, {"metadata": {"description": "Apache AGE is a PostgreSQL extension that provides graph database functionality. AGE is an acronym for A Graph Extension, and is inspired by Bitnine\u2019s fork of PostgreSQL 10, AgensGraph, which is a multi-model database. The goal of the project is to create single storage that can handle both relational and graph model data so that users can use standard ANSI SQL along with openCypher, the Graph query language. The data elements Apache AGE stores are nodes, edges connecting them, and attributes of nodes and edges.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/apache_age/", "title": "Apache AGE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/apache_age/", "uuid": "0948929a-a646-558a-b335-73f49d08ea37"}, "page_content": "Apache AGE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Apache AGE\n\n[Apache AGE](https://age.apache.org/) is a PostgreSQL extension that provides graph database functionality. AGE is an acronym for A Graph Extension, and is inspired by Bitnine\u2019s fork of PostgreSQL 10, AgensGraph, which is a multi-model database. The goal of the project is to create single storage that can handle both relational and graph model data so that users can use standard ANSI SQL along with openCypher, the Graph query language. The data elements `Apache AGE` stores are nodes, edges connecting them, and attributes of nodes and edges.\n\nThis notebook shows how to use LLMs to provide a natural language interface to a graph database you can query with the `Cypher` query language.\n\n[Cypher](https://en.wikipedia.org/wiki/Cypher_(query_language)) is a declarative graph query language that allows for expressive and efficient data querying in a property graph.\n\n## Setting up\u200b\n\nYou will need to have a running `Postgre` instance with the AGE extension installed. One option for testing is to run a docker container using the official AGE docker image.\nYou can run a local docker container by running the executing the following script:\n\n```text\ndocker run \\\n    --name age  \\\n    -p 5432:5432 \\\n    -e POSTGRES_USER=postgresUser \\\n    -e POSTGRES_PASSWORD=postgresPW \\\n    -e POSTGRES_DB=postgresDB \\\n    -d \\\n    apache/age\n```\n\nAdditional instructions on running in docker can be found [here](https://hub.docker.com/r/apache/age).\n\n```python\nfrom langchain_community.graphs.age_graph import AGEGraph\nfrom langchain_neo4j import GraphCypherQAChain\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[AGEGraph](https://python.langchain.com/api_reference/community/graphs/langchain_community.graphs.age_graph.AGEGraph.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nconf = {\n    \"database\": \"postgresDB\",\n    \"user\": \"postgresUser\",\n    \"password\": \"postgresPW\",\n    \"host\": \"localhost\",\n    \"port\": 5432,\n}\n\ngraph = AGEGraph(graph_name=\"age_test\", conf=conf)\n```\n\n## Seeding the database\u200b\n\nAssuming your database is empty, you can populate it using Cypher query language. The following Cypher statement is idempotent, which means the database information will be the same if you run it one or multiple times.\n\n```python\ngraph.query(\n    \"\"\"\nMERGE (m:Movie {name:\"Top Gun\"})\nWITH m\nUNWIND [\"Tom Cruise\", \"Val Kilmer\", \"Anthony Edwards\", \"Meg Ryan\"] AS actor\nMERGE (a:Actor {name:actor})\nMERGE (a)-[:ACTED_IN]->(m)\n\"\"\"\n)\n```\n\n```output\n[]\n```\n\n## Refresh graph schema information\u200b\n\nIf the schema of database changes, you can refresh the schema information needed to generate Cypher statements.\n\n```python\ngraph.refresh_schema()\n```\n\n```python\nprint(graph.schema)\n```\n\n```output\n\n        Node properties are the following:\n        [{'properties': [{'property': 'name', 'type': 'STRING'}], 'labels': 'Actor'}, {'properties': [{'property': 'property_a', 'type': 'STRING'}], 'labels': 'LabelA'}, {'properties': [], 'labels': 'LabelB'}, {'properties': [], 'labels': 'LabelC'}, {'properties': [{'property': 'name', 'type': 'STRING'}], 'labels': 'Movie'}]\n        Relationship properties are the following:\n        [{'properties': [], 'type': 'ACTED_IN'}, {'properties': [{'property': 'rel_prop', 'type': 'STRING'}], 'type': 'REL_TYPE'}]\n        The relationships are the following:\n        ['(:`Actor`)-[:`ACTED_IN`]->(:`Movie`)', '(:`LabelA`)-[:`REL_TYPE`]->(:`LabelB`)', '(:`LabelA`)-[:`REL_TYPE`]->(:`LabelC`)']\n```\n\n## Querying the graph\u200b\n\nWe can now use the graph cypher QA chain to ask question of the graph\n\n```python\nchain = GraphCypherQAChain.from_llm(\n    ChatOpenAI(temperature=0), graph=graph, verbose=True, allow_dangerous_requests=True\n)\n```\n\n```python\nchain.invoke(\"Who played in Top Gun?\")\n```\n\n```output", "type": "Document"}, {"metadata": {"description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook demonstrates the use of the langchaincommunity.documentloaders.ArcGISLoader class.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/arcgis/", "title": "ArcGIS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/arcgis/", "uuid": "a9dd78eb-3537-55b1-9e6d-99f9d35a489e"}, "page_content": "\"maxRecordCount\": 1000,\n   \"supportsStatistics\": true,\n   \"supportsAdvancedQueries\": true,\n   \"supportedQueryFormats\": \"JSON, geoJSON\",\n   \"isDataVersioned\": false,\n   \"ownershipBasedAccessControlForFeatures\": {\n     \"allowOthersToQuery\": true\n   },\n   \"useStandardizedQueries\": true,\n   \"advancedQueryCapabilities\": {\n     \"useStandardizedQueries\": true,\n     \"supportsStatistics\": true,\n     \"supportsHavingClause\": true,\n     \"supportsCountDistinct\": true,\n     \"supportsOrderBy\": true,\n     \"supportsDistinct\": true,\n     \"supportsPagination\": true,\n     \"supportsTrueCurve\": true,\n     \"supportsReturningQueryExtent\": true,\n     \"supportsQueryWithDistance\": true,\n     \"supportsSqlExpression\": true\n   },\n   \"supportsDatumTransformation\": true,\n   \"dateFieldsTimeReference\": null,\n   \"supportsCoordinatesQuantization\": true\n }}\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/all/", "title": "More | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/all/", "uuid": "3d36cf94-d71c-5d5b-8c50-9bf6921978ee"}, "page_content": "and datasets.](/docs/integrations/providers/modelscope/)[\ud83d\udcc4\ufe0f Modern TreasuryModern Treasury simplifies complex payment operations. It is a unified platform to power products and processes that move money.](/docs/integrations/providers/modern_treasury/)[\ud83d\udcc4\ufe0f MomentoMomento Cache is the world's first truly serverless caching service, offering instant elasticity, scale-to-zero](/docs/integrations/providers/momento/)[\ud83d\udcc4\ufe0f MongoDBMongoDB is a NoSQL, document-oriented](/docs/integrations/providers/mongodb/)[\ud83d\udcc4\ufe0f MongoDB AtlasMongoDB Atlas is a fully-managed cloud](/docs/integrations/providers/mongodb_atlas/)[\ud83d\udcc4\ufe0f MotherduckMotherduck is a managed DuckDB-in-the-cloud service.](/docs/integrations/providers/motherduck/)[\ud83d\udcc4\ufe0f Mot\u00f6rheadMot\u00f6rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.](/docs/integrations/providers/motorhead/)[\ud83d\udcc4\ufe0f MyScaleThis page covers how to use MyScale vector database within LangChain.](/docs/integrations/providers/myscale/)[\ud83d\udcc4\ufe0f NAVERAll functionality related to Naver, including HyperCLOVA X models, particularly those accessible through Naver Cloud CLOVA Studio.](/docs/integrations/providers/naver/)[\ud83d\udcc4\ufe0f Neo4jWhat is Neo4j?](/docs/integrations/providers/neo4j/)[\ud83d\udcc4\ufe0f NLPCloudNLP Cloud is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data.](/docs/integrations/providers/nlpcloud/)[\ud83d\udcc4\ufe0f NomicNomic builds tools that enable everyone to interact with AI scale datasets and run AI models on consumer computers.](/docs/integrations/providers/nomic/)[\ud83d\udcc4\ufe0f Notion DBNotion is a collaboration platform with modified Markdown support that integrates kanban](/docs/integrations/providers/notion/)[\ud83d\udcc4\ufe0f NucliaNuclia automatically indexes your unstructured data from any internal](/docs/integrations/providers/nuclia/)[\ud83d\udcc4\ufe0f NVIDIAThe langchain-nvidia-ai-endpoints package contains LangChain integrations building applications with models on](/docs/integrations/providers/nvidia/)[\ud83d\udcc4\ufe0f ObsidianObsidian is a powerful and extensible knowledge base](/docs/integrations/providers/obsidian/)[\ud83d\udcc4\ufe0f Oracle Cloud Infrastructure (OCI)The LangChain integrations related to Oracle Cloud Infrastructure.](/docs/integrations/providers/oci/)[\ud83d\udcc4\ufe0f OctoAIOctoAI offers easy access to efficient compute](/docs/integrations/providers/octoai/)[\ud83d\udcc4\ufe0f OllamaOllama allows you to run open-source large language models,](/docs/integrations/providers/ollama/)[\ud83d\udcc4\ufe0f Ontotext GraphDBOntotext GraphDB is a graph database and knowledge discovery tool compliant with RDF and SPARQL.](/docs/integrations/providers/ontotext_graphdb/)[\ud83d\udcc4\ufe0f OpenAIAll functionality related to OpenAI](/docs/integrations/providers/openai/)[\ud83d\udcc4\ufe0f OpenLLMThis page demonstrates how to use OpenLLM](/docs/integrations/providers/openllm/)[\ud83d\udcc4\ufe0f OpenSearchThis page covers how to use the OpenSearch ecosystem within LangChain.](/docs/integrations/providers/opensearch/)[\ud83d\udcc4\ufe0f OpenWeatherMapOpenWeatherMap provides all essential weather data for a specific location:](/docs/integrations/providers/openweathermap/)[\ud83d\udcc4\ufe0f OracleAI Vector SearchOracle AI Vector Search is designed for Artificial Intelligence (AI) workloads that allows you to query data based on semantics, rather than keywords.](/docs/integrations/providers/oracleai/)[\ud83d\udcc4\ufe0f OutlineOutline is an open-source collaborative knowledge base platform designed for team information sharing.](/docs/integrations/providers/outline/)[\ud83d\udcc4\ufe0f OutlinesOutlines is a Python library for constrained language generation. It provides a unified interface to various language models and allows for structured generation using techniques like regex matching, type constraints, JSON schemas, and context-free grammars.](/docs/integrations/providers/outlines/)[\ud83d\udcc4\ufe0f Pandaspandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,](/docs/integrations/providers/pandas/)[\ud83d\uddc3\ufe0f Pebblo1", "type": "Document"}, {"metadata": {"description": "LangSmith smoothly integrates with LangGraph (Python and JS)", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_with_langgraph", "title": "Trace with LangGraph (Python and JS/TS) | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb056452-16de-57e5-a282-9a77cbb83017"}, "page_content": "Without LangChain\u200b\nIf you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately\n(with the @traceable decorator in Python or the traceable function in JS, or something like e.g. wrap_openai for SDKs).\nIf you do so, LangSmith will automatically nest traces from those wrapped methods.\nHere's an example. You can also see this page for more information.\n0. Installation\u200b\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).\npipyarnnpmpnpmpip install openai langsmith langgraphyarn add openai langsmith @langchain/langgraphnpm install openai langsmith @langchain/langgraphpnpm add openai langsmith @langchain/langgraph\n1. Configure your environment\u200b\nPythonTypeScriptexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>export LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>infoIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\nIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=false\nSee this LangChain.js guide for more information.", "type": "Document"}, {"metadata": {"language": "en", "description": "Docusaurus is a static-site generator which provides out-of-the-box documentation features.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "title": "Docusaurus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "uuid": "e1a60685-daa6-5c67-ae41-b5d020b140fa"}, "page_content": "Below are some of the common use cases LangChain supports.\\\\n\\\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\\\nExtraction: Extract structured information from text.\\\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nReference Docs#\\\\nAll of LangChain\u2019s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\\\n\\\\nReference Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Ecosystem#\\\\nGuides for how other companies/products can be used with LangChain\\\\n\\\\nLangChain Ecosystem\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nAdditional Resources#\\\\nAdditional collection of resources we think may be useful as you develop your application!\\\\n\\\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\\\nDiscord: Join us on our Discord to discuss all things LangChain!\\\\nProduction Support: As you move your LangChains into production, we\u2019d love to offer more comprehensive support. Please fill out this form and we\u2019ll set up a dedicated support Slack channel.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nnext\\\\nQuickstart Guide\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Contents\\\\n  \\\\n\\\\n\\\\nGetting Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nBy Harrison Chase\\\\n\\\\n\\\\n\\\\n\\\\n    \\\\n      \u00a9 Copyright 2023, Harrison Chase.\\\\n      \\\\n\\\\n\\\\n\\\\n\\\\n  Last updated on Mar 27, 2023.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\', lookup_str=\\'\\', metadata={\\'source\\': \\'https://python.langchain.com/en/latest/\\', \\'loc\\': \\'https://python.langchain.com/en/latest/\\', \\'lastmod\\': \\'2023-03-27T22:50:49.790324+00:00\\', \\'changefreq\\': \\'daily\\', \\'priority\\': \\'0.9\\'}, lookup_index=0)Add custom scraping rules\\u200bThe SitemapLoader uses beautifulsoup4 for the scraping process, and it scrapes every element on the page by default. The SitemapLoader constructor accepts a custom scraping function. This feature can be helpful to tailor the scraping process to your specific needs; for example, you might want to avoid scraping headers or navigation elements. The following example shows how to develop and use a custom function to avoid navigation and header elements.Import the beautifulsoup4 library and define the", "type": "Document"}, {"metadata": {"description": "Configuring your installation", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/self_hosting/configuration", "title": "Configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "d21f969c-81c4-57ea-ae3c-6c75236e7ce7"}, "page_content": "Configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingArchitectural overviewScriptsInstallationConfigurationEmail/password a.k.a. basic authEnable blob storageConnect to an external ClickHouse databaseConnect to an external Postgres databaseConnect to an external Redis databaseSSO with OAuth2.0 and OIDCTTL and Data RetentionUser managementUsageUpgradesRelease notes (self-hosted)Frequently asked questionsLangSmith-managed ClickHouseTroubleshootingReferenceSelf-hostingConfigurationConfiguration\nYour LangSmith instance supports configuring a variety of parameters to suit your needs. This section contains guides for configuring your LangSmith instance.\n\nSSO with OAuth2.0 and OIDC\nEmail/password a.k.a. basic auth\nUser management features\nConnecting to an external ClickHouse database\nConnecting to an external Postgres database\nConnecting to an external Redis instance\nTTL and Data Retention\nBlob Storage\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousInstalling on DockerNextEmail/password a.k.a. basic authCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain is a framework that consists of a number of packages.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/architecture/", "title": "Architecture | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/architecture/", "uuid": "c609c312-0b79-5970-b1da-b28ffbe3501d"}, "page_content": "Architecture | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Architecture\n\nLangChain is a framework that consists of a number of packages.\n\n![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](/svg/langchain_stack_112024.svg)![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](/svg/langchain_stack_112024_dark.svg)\n## langchain-core\u200b\n\nThis package contains base abstractions for different components and ways to compose them together.\nThe interfaces for core components like chat models, vector stores, tools and more are defined here.\nNo third-party integrations are defined here.\nThe dependencies are very lightweight.\n\n## langchain\u200b\n\nThe main `langchain` package contains chains and retrieval strategies that make up an application's cognitive architecture.\nThese are NOT third-party integrations.\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\n\n## Integration packages\u200b\n\nPopular integrations have their own packages (e.g. `langchain-openai`, `langchain-anthropic`, etc) so that they can be properly versioned and appropriately lightweight.\n\nFor more information see:\n\n- A list [integrations packages](/docs/integrations/providers/)\n\n- The [API Reference](https://python.langchain.com/api_reference/) where you can find detailed information about each of the integration package.\n\n## langchain-community\u200b\n\nThis package contains third-party integrations that are maintained by the LangChain community.\nKey integration packages are separated out (see above).\nThis contains integrations for various components (chat models, vector stores, tools, etc).\nAll dependencies in this package are optional to keep the package as lightweight as possible.\n\n## langgraph\u200b\n\n`langgraph` is an extension of `langchain` aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.\n\nFurther reading- See our LangGraph overview [here](https://langchain-ai.github.io/langgraph/concepts/high_level/#core-principles).\n\n- See our LangGraph Academy Course [here](https://academy.langchain.com/courses/intro-to-langgraph).\n\n## langserve\u200b\n\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\n\nimportantLangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.\n\nIf you need a deployment option for LangGraph, you should instead be looking at LangGraph Cloud (beta) which will be better suited for deploying LangGraph applications.\n\nFor more information, see the [LangServe documentation](/docs/langserve/).\n\n## LangSmith\u200b\n\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\n\nFor more information, see the [LangSmith documentation](https://docs.smith.langchain.com)\n\n#### Was this page helpful?\n\n- [langchain-core](#langchain-core)\n\n- [langchain](#langchain)\n\n- [Integration packages](#integration-packages)\n\n- [langchain-community](#langchain-community)\n\n- [langgraph](#langgraph)\n\n- [langserve](#langserve)\n\n- [LangSmith](#langsmith)", "type": "Document"}, {"metadata": {"description": "Neo4j is an open-source graph database management system, renowned for its efficient management of highly connected data. Unlike traditional databases that store data in tables, Neo4j uses a graph structure with nodes, edges, and properties to represent and store data. This design allows for high-performance queries on complex data relationships.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/neo4j_chat_message_history/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/neo4j_chat_message_history/", "uuid": "62397dda-d346-5d81-a761-dbabf3f90aa3"}, "page_content": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Neo4j\n\n[Neo4j](https://en.wikipedia.org/wiki/Neo4j) is an open-source graph database management system, renowned for its efficient management of highly connected data. Unlike traditional databases that store data in tables, Neo4j uses a graph structure with nodes, edges, and properties to represent and store data. This design allows for high-performance queries on complex data relationships.\n\nThis notebook goes over how to use `Neo4j` to store chat message history.\n\n```python\nfrom langchain_neo4j import Neo4jChatMessageHistory\n\nhistory = Neo4jChatMessageHistory(\n    url=\"bolt://localhost:7687\",\n    username=\"neo4j\",\n    password=\"password\",\n    session_id=\"session_id_1\",\n)\n\nhistory.add_user_message(\"hi!\")\n\nhistory.add_ai_message(\"whats up?\")\n```\n\n```python\nhistory.messages\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Neo4j is a graph database that stores nodes and relationships, that also supports native vector search.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/neo4j_self_query/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/neo4j_self_query/", "uuid": "de2dfcea-3d07-57a5-b805-51caf9230ae8"}, "page_content": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Neo4j\n\n[Neo4j](https://neo4j.com/docs/) is a graph database that stores nodes and relationships, that also supports native vector search.\n\nIn the notebook, we'll demo the `SelfQueryRetriever` wrapped around a `Neo4j` vector store.\n\n## Creating a Neo4j vector store\u200b\n\nFirst we'll want to create a Neo4j vector store and seed it with some data. We've created a small demo set of documents that contain summaries of movies.\n\nWe want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key.\n\n```python\n%pip install --upgrade neo4j\n```\n\n```output\nRequirement already satisfied: neo4j in /Users/moyi/git/langchain/env/lib/python3.11/site-packages (5.24.0)\nRequirement already satisfied: pytz in /Users/moyi/git/langchain/env/lib/python3.11/site-packages (from neo4j) (2024.1)\nNote: you may need to restart the kernel to use updated packages.\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\n```python\n# To run this notebook, you can set up a free neo4j account on neo4j.com and input the following information.\n# (If you are having trouble connecting to the database, try using neo4j+ssc: instead of neo4j+s)\n\nif \"NEO4J_URI\" not in os.environ:\n    os.environ[\"NEO4J_URI\"] = getpass.getpass(\"Neo4j URL:\")\nif \"NEO4J_USERNAME\" not in os.environ:\n    os.environ[\"NEO4J_USERNAME\"] = getpass.getpass(\"Neo4j User Name:\")\nif \"NEO4J_PASSWORD\" not in os.environ:\n    os.environ[\"NEO4J_PASSWORD\"] = getpass.getpass(\"Neo4j Password:\")\n```\n\n```output\nNeo4j URL: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nNeo4j User Name: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nNeo4j Password: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\n```python\nfrom langchain_core.documents import Document\nfrom langchain_neo4j import Neo4jVector\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```python\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"science fiction\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Neo4jVector.from_documents(docs, embeddings)\n```", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful."}, {"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_GfXoS8ACVmmOnx9OceiQwmf4", "function": {"arguments": "{\"steps\":[\"Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.\",\"Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.\",\"After making the necessary imports and installations, run your code again to see if the error persists.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-17a6ba50-d1ec-4bfb-be56-10a48fa5bd8f", "example": false, "tool_calls": [{"name": "Plan", "args": {"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}, "id": "call_GfXoS8ACVmmOnx9OceiQwmf4", "type": "tool_call"}], "invalid_tool_calls": []}}
{"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."], "documents": "delete", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."], "documents": [], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"question": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script."}
{"question": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script."}
{"input": {"question": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_8z70ag95p8RqJhRoJ3dFO4F9", "function": {"arguments": "{\"queries\":[\"How to check if a library is imported in Python?\",\"Python import statement for OpenAI library\",\"Best practices for organizing Python scripts with imports\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-fa8f6dc5-f7a7-4a24-abcb-c9252b717cd7", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}, "id": "call_8z70ag95p8RqJhRoJ3dFO4F9", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}
{"input": {"question": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"], "documents": []}}
{"input": {"query": "How to check if a library is imported in Python?"}}
{"query": "How to check if a library is imported in Python?"}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}]}
{"input": {"query": "Python import statement for OpenAI library"}}
{"query": "Python import statement for OpenAI library"}
{"documents": [{"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}]}
{"input": {"query": "Best practices for organizing Python scripts with imports"}}
{"query": "Best practices for organizing Python scripts with imports"}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}], "steps": ["Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"question": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'."}
{"question": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'."}
{"input": {"question": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_WTxtVvhnonOy3cqPhrnCEQ60", "function": {"arguments": "{\"queries\":[\"How to check if OpenAI library is installed in Python?\",\"What command to use for installing OpenAI library with pip?\",\"Troubleshooting OpenAI library installation issues in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-9df9f557-1ac8-4164-b61f-3ebd6f3192d4", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}, "id": "call_WTxtVvhnonOy3cqPhrnCEQ60", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}
{"input": {"question": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"], "documents": []}}
{"input": {"query": "How to check if OpenAI library is installed in Python?"}}
{"query": "How to check if OpenAI library is installed in Python?"}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}]}
{"input": {"query": "What command to use for installing OpenAI library with pip?"}}
{"query": "What command to use for installing OpenAI library with pip?"}
{"documents": [{"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}]}
{"input": {"query": "Troubleshooting OpenAI library installation issues in Python"}}
{"query": "Troubleshooting OpenAI library installation issues in Python"}
{"documents": [{"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "All functionality related to OpenAI", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}], "steps": ["After making the necessary imports and installations, run your code again to see if the error persists."]}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["After making the necessary imports and installations, run your code again to see if the error persists."], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["After making the necessary imports and installations, run your code again to see if the error persists."], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"question": "After making the necessary imports and installations, run your code again to see if the error persists."}
{"question": "After making the necessary imports and installations, run your code again to see if the error persists."}
{"input": {"question": "After making the necessary imports and installations, run your code again to see if the error persists.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "After making the necessary imports and installations, run your code again to see if the error persists."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "After making the necessary imports and installations, run your code again to see if the error persists.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_lV1vnlbbx3p34aRARvaCgTLr", "function": {"arguments": "{\"queries\":[\"how to troubleshoot code errors after imports\",\"common Python errors after installing packages\",\"steps to resolve import errors in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f1cdd33f-10f5-4038-aa56-4d866e5223c0", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}, "id": "call_lV1vnlbbx3p34aRARvaCgTLr", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}
{"input": {"question": "After making the necessary imports and installations, run your code again to see if the error persists.", "queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"], "documents": []}}
{"input": {"query": "how to troubleshoot code errors after imports"}}
{"query": "how to troubleshoot code errors after imports"}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}]}
{"input": {"query": "common Python errors after installing packages"}}
{"query": "common Python errors after installing packages"}
{"documents": [{"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}]}
{"input": {"query": "steps to resolve import errors in Python"}}
{"query": "steps to resolve import errors in Python"}
{"documents": [{"metadata": {"language": "en", "description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "steps": []}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "answer": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are an expert programmer and problem-solver, tasked with answering any question about LangChain.\n\nGenerate a comprehensive and informative answer for the given question based solely on the provided search results (URL and content). Do NOT ramble, and adjust your response length based on the question. If they ask a question that can be answered in one sentence, do that. If 5 paragraphs of detail is needed, do that. You must only use information from the provided search results. Use an unbiased and journalistic tone. Combine search results together into a coherent answer. Do not repeat text. Cite search results using [${number}] notation. Only cite the most relevant results that answer the question accurately. Place these citations at the end of the individual sentence or paragraph that reference them. Do not put them all at the end, but rather sprinkle them throughout. If different results refer to different entities within the same name, write separate answers for each entity.\n\nYou should use bullet points in your answer for readability. Put citations where they apply rather than putting them all at the end. DO NOT PUT THEM ALL THAT END, PUT THEM IN THE BULLET POINTS.\n\nIf there is nothing in the context relevant to the question at hand, do NOT make up an answer. Rather, tell them why you're unsure and ask for any additional information that may help you answer better.\n\nSometimes, what a user is asking may NOT be possible. Do NOT tell them that things are possible if you don't see evidence for it in the context below. If you don't see based in the information below that something is possible, do NOT say that it is - instead say that you're not sure.\n\nAnything between the following `context` html blocks is retrieved from a knowledge bank, not part of the conversation with the user.\n\n<context>\n    <documents>\n<document  description='This guide walks through how to run the repository locally and check in your first code.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/contributing/how_to/code/setup/' title='Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/contributing/how_to/code/setup/' uuid='6ed90ecd-1415-547c-be14-24fcc4c21400'>\n```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)\n</document>\n<document  description=\"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/document_loader_directory/' title='How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/document_loader_directory/' uuid='a268cec3-3331-5785-9a42-e4fe0b984d2d'>\n```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b\n</document>\n<document  language='en' description='This guide assumes familiarity with the following concepts:' changefreq='weekly' source='https://python.langchain.com/docs/how_to/tools_builtin/' title='How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/tools_builtin/' uuid='97360d72-03fa-5fa7-8011-11e168ee6a72'>\n## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)\n</document>\n<document  language='en' description='There are several ways to log traces to LangSmith.' changefreq=None source='https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code' title='Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='7985443c-fb90-5bb8-a406-f416a274d65e'>\nExample usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n<document  language='en' description=\"Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\" changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/tavily_search/' title='Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/tavily_search/' uuid='26448348-5def-505f-9ab7-db75567551f5'>\n```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n</document>\n<document  description='The Github toolkit contains tools that enable an LLM agent to interact with a github repository.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/github/' title='Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/github/' uuid='ff3eac04-7315-530a-bb30-dace51723d24'>\nView available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n</document>\n<document  description='This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/qa_chat_history_how_to/' title='How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/qa_chat_history_how_to/' uuid='e26c4f35-e925-58a6-871b-32a1358c25f4'>\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n</document>\n<document  description=\"LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/sql_csv/' title='How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/sql_csv/' uuid='6758f6ac-ce1e-5e03-8cd0-55b0ae83c739'>\n```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n</document>\n<document  description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='8af62e09-443d-5cc9-9c24-a8bd13563ed0'>\nwill write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='7b547e20-3a35-5cb0-881c-341c25231f06'>\nof the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture\n</document>\n<document  language='en' description='Last updated: 09.16.24' changefreq='weekly' source='https://python.langchain.com/docs/versions/v0_3/' title='LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/versions/v0_3/' uuid='23e8bb05-3597-5c72-974c-eaa037023f7c'>\n### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply\n</document>\n<document  description='LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/versions/v0_2/' title='Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/versions/v0_2/' uuid='c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc'>\n# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)\n</document>\n<document  language='en' description='All functionality related to OpenAI' changefreq='weekly' source='https://python.langchain.com/docs/integrations/providers/openai/' title='OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/providers/openai/' uuid='b7d0a36c-1004-55bd-b931-52db68a7c3b2'>\nOpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).\n</document>\n<document  description='This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/document_loader_web/' title='How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/document_loader_web/' uuid='04bc0c1a-a685-5035-8cdc-a7f4387a8c41'>\n```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)\n</document>\n<document  description='This notebook walks through connecting a LangChain to the Google Drive API.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/google_drive/' title='Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/google_drive/' uuid='88f47abb-dad6-503d-9fe1-962edd2f255b'>\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)\n</document>\n<document  description='You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/llms/azure_openai/' title='Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/llms/azure_openai/' uuid='67bbea5a-555e-577f-8c50-bfe43cc3ffd9'>\nAzure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential\n</document>\n<document  description='This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/jina_search/' title='Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/jina_search/' uuid='ee54fa7b-b9f9-5a03-b91f-ad00ae989409'>\n```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n</document>\n<document  description='An output parser was unable to handle model output as expected.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/' title='OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/' uuid='4c5dd25c-312a-58f1-95ff-a433ccc0d012'>\n## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/' title='Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/' uuid='8524db99-5b8a-5036-9cfd-e8c034ec4ba7'>\n# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]\n</document>\n<document  language='en' description='This page contains guides around resolving common errors you may find while building with LangChain.' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/' title='Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/' uuid='8055e105-68e2-5e0f-950d-2f771934f6ee'>\nError reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?\n</document>\n</documents>\n<context/>", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63"}}]]}
{"messages": [{"content": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f908b8eb-ef1a-4047-8d0c-e1b06520851c", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "num_response_attempts": 1}
{"input": {"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}, {"content": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f908b8eb-ef1a-4047-8d0c-e1b06520851c", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "answer": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."}, {"role": "human", "content": "Set of facts: \n\n <documents>\n<document  description='This guide walks through how to run the repository locally and check in your first code.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/contributing/how_to/code/setup/' title='Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/contributing/how_to/code/setup/' uuid='6ed90ecd-1415-547c-be14-24fcc4c21400'>\n```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)\n</document>\n<document  description=\"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/document_loader_directory/' title='How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/document_loader_directory/' uuid='a268cec3-3331-5785-9a42-e4fe0b984d2d'>\n```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b\n</document>\n<document  language='en' description='This guide assumes familiarity with the following concepts:' changefreq='weekly' source='https://python.langchain.com/docs/how_to/tools_builtin/' title='How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/tools_builtin/' uuid='97360d72-03fa-5fa7-8011-11e168ee6a72'>\n## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)\n</document>\n<document  language='en' description='There are several ways to log traces to LangSmith.' changefreq=None source='https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code' title='Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='7985443c-fb90-5bb8-a406-f416a274d65e'>\nExample usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n<document  language='en' description=\"Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\" changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/tavily_search/' title='Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/tavily_search/' uuid='26448348-5def-505f-9ab7-db75567551f5'>\n```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n</document>\n<document  description='The Github toolkit contains tools that enable an LLM agent to interact with a github repository.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/github/' title='Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/github/' uuid='ff3eac04-7315-530a-bb30-dace51723d24'>\nView available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n</document>\n<document  description='This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/qa_chat_history_how_to/' title='How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/qa_chat_history_how_to/' uuid='e26c4f35-e925-58a6-871b-32a1358c25f4'>\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n</document>\n<document  description=\"LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/sql_csv/' title='How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/sql_csv/' uuid='6758f6ac-ce1e-5e03-8cd0-55b0ae83c739'>\n```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n</document>\n<document  description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='8af62e09-443d-5cc9-9c24-a8bd13563ed0'>\nwill write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='7b547e20-3a35-5cb0-881c-341c25231f06'>\nof the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture\n</document>\n<document  language='en' description='Last updated: 09.16.24' changefreq='weekly' source='https://python.langchain.com/docs/versions/v0_3/' title='LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/versions/v0_3/' uuid='23e8bb05-3597-5c72-974c-eaa037023f7c'>\n### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply\n</document>\n<document  description='LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/versions/v0_2/' title='Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/versions/v0_2/' uuid='c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc'>\n# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)\n</document>\n<document  language='en' description='All functionality related to OpenAI' changefreq='weekly' source='https://python.langchain.com/docs/integrations/providers/openai/' title='OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/providers/openai/' uuid='b7d0a36c-1004-55bd-b931-52db68a7c3b2'>\nOpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).\n</document>\n<document  description='This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/document_loader_web/' title='How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/document_loader_web/' uuid='04bc0c1a-a685-5035-8cdc-a7f4387a8c41'>\n```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)\n</document>\n<document  description='This notebook walks through connecting a LangChain to the Google Drive API.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/google_drive/' title='Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/google_drive/' uuid='88f47abb-dad6-503d-9fe1-962edd2f255b'>\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)\n</document>\n<document  description='You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/llms/azure_openai/' title='Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/llms/azure_openai/' uuid='67bbea5a-555e-577f-8c50-bfe43cc3ffd9'>\nAzure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential\n</document>\n<document  description='This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/jina_search/' title='Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/jina_search/' uuid='ee54fa7b-b9f9-5a03-b91f-ad00ae989409'>\n```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n</document>\n<document  description='An output parser was unable to handle model output as expected.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/' title='OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/' uuid='4c5dd25c-312a-58f1-95ff-a433ccc0d012'>\n## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/' title='Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/' uuid='8524db99-5b8a-5036-9cfd-e8c034ec4ba7'>\n# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]\n</document>\n<document  language='en' description='This page contains guides around resolving common errors you may find while building with LangChain.' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/' title='Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/' uuid='8055e105-68e2-5e0f-950d-2f771934f6ee'>\nError reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?\n</document>\n</documents> \n\n LLM generation: The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!"}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Set of facts: \n\n <documents>\n<document  description='This guide walks through how to run the repository locally and check in your first code.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/contributing/how_to/code/setup/' title='Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/contributing/how_to/code/setup/' uuid='6ed90ecd-1415-547c-be14-24fcc4c21400'>\n```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)\n</document>\n<document  description=\"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/document_loader_directory/' title='How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/document_loader_directory/' uuid='a268cec3-3331-5785-9a42-e4fe0b984d2d'>\n```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b\n</document>\n<document  language='en' description='This guide assumes familiarity with the following concepts:' changefreq='weekly' source='https://python.langchain.com/docs/how_to/tools_builtin/' title='How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/tools_builtin/' uuid='97360d72-03fa-5fa7-8011-11e168ee6a72'>\n## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)\n</document>\n<document  language='en' description='There are several ways to log traces to LangSmith.' changefreq=None source='https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code' title='Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='7985443c-fb90-5bb8-a406-f416a274d65e'>\nExample usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.\n</document>\n<document  language='en' description=\"Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.\" changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/tavily_search/' title='Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/tavily_search/' uuid='26448348-5def-505f-9ab7-db75567551f5'>\n```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n</document>\n<document  description='The Github toolkit contains tools that enable an LLM agent to interact with a github repository.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/github/' title='Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/github/' uuid='ff3eac04-7315-530a-bb30-dace51723d24'>\nView available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n</document>\n<document  description='This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/qa_chat_history_how_to/' title='How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/qa_chat_history_how_to/' uuid='e26c4f35-e925-58a6-871b-32a1358c25f4'>\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n</document>\n<document  description=\"LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:\" language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/sql_csv/' title='How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/sql_csv/' uuid='6758f6ac-ce1e-5e03-8cd0-55b0ae83c739'>\n```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n</document>\n<document  description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='8af62e09-443d-5cc9-9c24-a8bd13563ed0'>\nwill write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files\n</document>\n<document  language='en' description='markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' title='Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/document_transformers/markdownify/' uuid='7b547e20-3a35-5cb0-881c-341c25231f06'>\nof the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture\n</document>\n<document  language='en' description='Last updated: 09.16.24' changefreq='weekly' source='https://python.langchain.com/docs/versions/v0_3/' title='LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/versions/v0_3/' uuid='23e8bb05-3597-5c72-974c-eaa037023f7c'>\n### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply\n</document>\n<document  description='LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/versions/v0_2/' title='Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/versions/v0_2/' uuid='c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc'>\n# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)\n</document>\n<document  language='en' description='All functionality related to OpenAI' changefreq='weekly' source='https://python.langchain.com/docs/integrations/providers/openai/' title='OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/providers/openai/' uuid='b7d0a36c-1004-55bd-b931-52db68a7c3b2'>\nOpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).\n</document>\n<document  description='This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/document_loader_web/' title='How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/document_loader_web/' uuid='04bc0c1a-a685-5035-8cdc-a7f4387a8c41'>\n```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)\n</document>\n<document  description='This notebook walks through connecting a LangChain to the Google Drive API.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/google_drive/' title='Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/google_drive/' uuid='88f47abb-dad6-503d-9fe1-962edd2f255b'>\n```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)\n</document>\n<document  description='You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/llms/azure_openai/' title='Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/llms/azure_openai/' uuid='67bbea5a-555e-577f-8c50-bfe43cc3ffd9'>\nAzure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential\n</document>\n<document  description='This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/tools/jina_search/' title='Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/tools/jina_search/' uuid='ee54fa7b-b9f9-5a03-b91f-ad00ae989409'>\n```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n</document>\n<document  description='An output parser was unable to handle model output as expected.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/' title='OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/' uuid='4c5dd25c-312a-58f1-95ff-a433ccc0d012'>\n## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/' title='Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/' uuid='8524db99-5b8a-5036-9cfd-e8c034ec4ba7'>\n# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]\n</document>\n<document  language='en' description='This page contains guides around resolving common errors you may find while building with LangChain.' changefreq='weekly' source='https://python.langchain.com/docs/troubleshooting/errors/' title='Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/troubleshooting/errors/' uuid='8055e105-68e2-5e0f-950d-2f771934f6ee'>\nError reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?\n</document>\n</documents> \n\n LLM generation: The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_q1g6k272dvhQQrqGzReVa3eq", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-6e19b162-0adc-45a8-85f3-557b9c329660", "example": false, "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_q1g6k272dvhQQrqGzReVa3eq", "type": "tool_call"}], "invalid_tool_calls": []}}
