{"messages": [{"role": "user", "content": "Explain RAG techniques and how LangGraph can implement them."}]}
{"messages": [{"role": "user", "content": "Explain RAG techniques and how LangGraph can implement them."}]}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [], "answer": "", "query": "", "num_response_attempts": 0}}
{"input": [{"role": "system", "content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful."}, {"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Explain RAG techniques and how LangGraph can implement them.", "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_uMCtPzovceci4jYiUnIGPaH9", "function": {"arguments": "{\"steps\":[\"Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.\",\"Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.\",\"Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2e670901-afd8-40eb-804c-c200d79d0173", "example": false, "tool_calls": [{"name": "Plan", "args": {"steps": ["Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.", "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."]}, "id": "call_uMCtPzovceci4jYiUnIGPaH9", "type": "tool_call"}], "invalid_tool_calls": []}}
{"steps": ["Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.", "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."], "documents": "delete", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.", "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."], "documents": [], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"question": "Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved."}
{"question": "Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved."}
{"input": {"question": "Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_m3z3KO5eoXJjEjTCD1uiypqt", "function": {"arguments": "{\"queries\":[\"RAG techniques overview\",\"Retrieval-Augmented Generation methodologies\",\"core principles of RAG in NLP\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-96799c2f-cd79-48d2-86cb-3c99d5a04dcd", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["RAG techniques overview", "Retrieval-Augmented Generation methodologies", "core principles of RAG in NLP"]}, "id": "call_m3z3KO5eoXJjEjTCD1uiypqt", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["RAG techniques overview", "Retrieval-Augmented Generation methodologies", "core principles of RAG in NLP"]}
{"input": {"question": "Review the conceptual documentation on RAG (Retrieval-Augmented Generation) techniques to understand the core principles and methodologies involved.", "queries": ["RAG techniques overview", "Retrieval-Augmented Generation methodologies", "core principles of RAG in NLP"], "documents": []}}
{"input": {"query": "RAG techniques overview"}}
{"query": "RAG techniques overview"}
{"documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}]}
{"input": {"query": "Retrieval-Augmented Generation methodologies"}}
{"query": "Retrieval-Augmented Generation methodologies"}
{"documents": [{"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrieval", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}]}
{"input": {"query": "core principles of RAG in NLP"}}
{"query": "core principles of RAG in NLP"}
{"documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}, {"metadata": {"language": "en", "description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}], "steps": ["Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."]}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"question": "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities."}
{"question": "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities."}
{"input": {"question": "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_7wRY3rxOScyDFtGlVEhuNsV9", "function": {"arguments": "{\"queries\":[\"LangGraph integration documentation RAG techniques\",\"LangGraph features for retrieval-augmented generation\",\"LangGraph functionalities supporting RAG\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-446b50e6-9ea1-4cce-be18-bd5b40b5939d", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["LangGraph integration documentation RAG techniques", "LangGraph features for retrieval-augmented generation", "LangGraph functionalities supporting RAG"]}, "id": "call_7wRY3rxOScyDFtGlVEhuNsV9", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["LangGraph integration documentation RAG techniques", "LangGraph features for retrieval-augmented generation", "LangGraph functionalities supporting RAG"]}
{"input": {"question": "Examine the integration documentation for LangGraph to identify how it supports RAG techniques, including any specific features or functionalities.", "queries": ["LangGraph integration documentation RAG techniques", "LangGraph features for retrieval-augmented generation", "LangGraph functionalities supporting RAG"], "documents": []}}
{"input": {"query": "LangGraph integration documentation RAG techniques"}}
{"query": "LangGraph integration documentation RAG techniques"}
{"documents": [{"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}]}
{"input": {"query": "LangGraph features for retrieval-augmented generation"}}
{"query": "LangGraph features for retrieval-augmented generation"}
{"documents": [{"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "LangChain implements the latest research in the field of Natural Language Processing.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}]}
{"input": {"query": "LangGraph functionalities supporting RAG"}}
{"query": "LangGraph functionalities supporting RAG"}
{"documents": [{"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}]}
{"documents": [{"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"description": "LangChain implements the latest research in the field of Natural Language Processing.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}], "steps": ["Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."]}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"question": "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."}
{"question": "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."}
{"input": {"question": "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_jJ1Mky2fUd7mMtaxOFugDpuA", "function": {"arguments": "{\"queries\":[\"RAG implementation guide with LangGraph\",\"use cases of LangGraph in RAG applications\",\"practical examples of RAG and LangGraph integration\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-faaf2d41-3324-4677-8c09-a262a15d82f7", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["RAG implementation guide with LangGraph", "use cases of LangGraph in RAG applications", "practical examples of RAG and LangGraph integration"]}, "id": "call_jJ1Mky2fUd7mMtaxOFugDpuA", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["RAG implementation guide with LangGraph", "use cases of LangGraph in RAG applications", "practical examples of RAG and LangGraph integration"]}
{"input": {"question": "Look into how-to guides that provide practical examples or use cases of implementing RAG with LangGraph to illustrate the application of these techniques.", "queries": ["RAG implementation guide with LangGraph", "use cases of LangGraph in RAG applications", "practical examples of RAG and LangGraph integration"], "documents": []}}
{"input": {"query": "RAG implementation guide with LangGraph"}}
{"query": "RAG implementation guide with LangGraph"}
{"documents": [{"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}]}
{"input": {"query": "use cases of LangGraph in RAG applications"}}
{"query": "use cases of LangGraph in RAG applications"}
{"documents": [{"metadata": {"language": "en", "description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "title": "Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "uuid": "23701dd3-42b9-5e92-923c-f463fb360379"}, "page_content": "Why LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "title": "\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "uuid": "72626759-8f24-526d-ab64-cefc84c4f0ca"}, "page_content": "3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json", "type": "Document"}]}
{"input": {"query": "practical examples of RAG and LangGraph integration"}}
{"query": "practical examples of RAG and LangGraph integration"}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "title": "Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "uuid": "23701dd3-42b9-5e92-923c-f463fb360379"}, "page_content": "Why LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}]}
{"documents": [{"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "title": "Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "uuid": "23701dd3-42b9-5e92-923c-f463fb360379"}, "page_content": "Why LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "title": "\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "uuid": "72626759-8f24-526d-ab64-cefc84c4f0ca"}, "page_content": "3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}], "steps": []}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "title": "Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "uuid": "23701dd3-42b9-5e92-923c-f463fb360379"}, "page_content": "Why LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "title": "\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "uuid": "72626759-8f24-526d-ab64-cefc84c4f0ca"}, "page_content": "3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json", "type": "Document"}], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "title": "Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "uuid": "23701dd3-42b9-5e92-923c-f463fb360379"}, "page_content": "Why LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "title": "\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "uuid": "72626759-8f24-526d-ab64-cefc84c4f0ca"}, "page_content": "3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json", "type": "Document"}], "answer": "", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 0}}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are an expert programmer and problem-solver, tasked with answering any question about LangChain.\n\nGenerate a comprehensive and informative answer for the given question based solely on the provided search results (URL and content). Do NOT ramble, and adjust your response length based on the question. If they ask a question that can be answered in one sentence, do that. If 5 paragraphs of detail is needed, do that. You must only use information from the provided search results. Use an unbiased and journalistic tone. Combine search results together into a coherent answer. Do not repeat text. Cite search results using [${number}] notation. Only cite the most relevant results that answer the question accurately. Place these citations at the end of the individual sentence or paragraph that reference them. Do not put them all at the end, but rather sprinkle them throughout. If different results refer to different entities within the same name, write separate answers for each entity.\n\nYou should use bullet points in your answer for readability. Put citations where they apply rather than putting them all at the end. DO NOT PUT THEM ALL THAT END, PUT THEM IN THE BULLET POINTS.\n\nIf there is nothing in the context relevant to the question at hand, do NOT make up an answer. Rather, tell them why you're unsure and ask for any additional information that may help you answer better.\n\nSometimes, what a user is asking may NOT be possible. Do NOT tell them that things are possible if you don't see evidence for it in the context below. If you don't see based in the information below that something is possible, do NOT say that it is - instead say that you're not sure.\n\nAnything between the following `context` html blocks is retrieved from a knowledge bank, not part of the conversation with the user.\n\n<context>\n    <documents>\n<document  language='en' description='The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.' changefreq=None source='https://docs.smith.langchain.com/evaluation/concepts' title='Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='a24e3440-266a-53d3-895c-ef945d163e99'>\nRetrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation\n</document>\n<document  description='We will walk through the evaluation workflow for RAG (retrieval augmented generation).' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/tutorials/rag' title='RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='00d6eb31-f609-5497-946f-02b1b6f5ee07'>\nRAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.\n</document>\n<document  description='Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/retrievers/rememberizer/' title='Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/retrievers/rememberizer/' uuid='5ae25ce5-6e4c-522b-876d-5f24a7914f64'>\n```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)\n</document>\n<document  description='* Retrieval' language='en' changefreq='weekly' source='https://python.langchain.com/docs/concepts/rag/' title='Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/concepts/rag/' uuid='a539b77a-646a-5fe6-82b5-0f560e06ebf8'>\nRetrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)\n</document>\n<document  language='en' description='LangChain implements the latest research in the field of Natural Language Processing.' changefreq='weekly' source='https://python.langchain.com/docs/additional_resources/arxiv_references/' title='arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/additional_resources/arxiv_references/' uuid='6028efd4-4079-5b6d-868c-aadc9eea72c1'>\n| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |\n</document>\n<document  language='en' description='LangChain implements the latest research in the field of Natural Language Processing.' changefreq='weekly' source='https://python.langchain.com/docs/additional_resources/arxiv_references/' title='arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/additional_resources/arxiv_references/' uuid='340f01d5-de75-5ddd-a744-e6021efc7879'>\n- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/storm/storm/' title='Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/storm/storm/' uuid='cf1d633f-f889-528f-91a0-c6774265bd65'>\n## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).\n</document>\n<document  description='langgraph' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph' title='How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='b2aa2557-d3fc-55c9-8bc3-49f31ad6a160'>\nlanggraph evaluation docs\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/' title='Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/' uuid='bdb840c9-4cd3-5c1c-9603-c51cb8a5d358'>\nTutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments\n</document>\n<document  description='One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/rag/' title='Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/rag/' uuid='cd8936d6-ee48-5764-a3db-5025bf4ecb61'>\nDo I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```\n</document>\n<document  language='en' description='In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/qa_chat_history/' title='Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/qa_chat_history/' uuid='4f420bf5-f8f1-54bc-950d-66eac8371f81'>\nOur graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/' title='Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/' uuid='e82df499-c601-58a4-a012-6b245dd00721'>\nAdaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/concepts/high_level/' title='Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/concepts/high_level/' uuid='23701dd3-42b9-5e92-923c-f463fb360379'>\nWhy LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/introduction/' title='\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/introduction/' uuid='72626759-8f24-526d-ab64-cefc84c4f0ca'>\n3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json\n</document>\n</documents>\n<context/>", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Explain RAG techniques and how LangGraph can implement them.", "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce"}}]]}
{"messages": [{"content": "Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of language models by integrating external knowledge retrieval into the generation process. Here\u2019s a breakdown of RAG techniques and how LangGraph can implement them:\n\n### RAG Techniques\n\n- **Overview**: RAG combines the strengths of language models (LLMs) with external knowledge bases. When a query is made, RAG systems first retrieve relevant information from a knowledge base and then use this information to inform the response generated by the LLM. This approach helps in providing up-to-date and contextually relevant answers, reducing the chances of generating incorrect or outdated information.\n\n- **Key Components**:\n  - **Retrieval System**: This component is responsible for fetching relevant documents or data based on the user's query. It ensures that the LLM has access to the most pertinent information.\n  - **Integration of Knowledge**: The retrieved information is incorporated into the prompt sent to the LLM, allowing the model to generate responses that are grounded in factual data.\n\n- **Evaluation Methods**:\n  - **Reference Answer Evaluation**: Comparing the generated responses against known correct answers to assess accuracy.\n  - **Reference-Free Evaluation**: Using self-consistency checks to evaluate the quality of responses without needing a reference answer.\n\n### Implementing RAG with LangGraph\n\nLangGraph provides a structured framework for implementing RAG techniques effectively:\n\n- **Graph Structure**: LangGraph allows developers to create applications as directed acyclic graphs (DAGs), where each node represents a step in the RAG process (e.g., user input, retrieval, response generation). This structure facilitates clear control flow and state management across interactions.\n\n- **Node Types**:\n  - **Query Node**: This node handles user input and can either generate a query for the retriever or respond directly based on previous interactions.\n  - **Retriever Node**: Executes the retrieval step, fetching relevant documents based on the user\u2019s query.\n  - **Response Generation Node**: Utilizes the retrieved context to generate a final answer, ensuring that the response is informed by the most relevant data.\n\n- **Adaptive RAG**: LangGraph supports advanced RAG strategies, such as Adaptive RAG, which combines query analysis with self-corrective mechanisms. This allows the system to dynamically choose between different retrieval methods (e.g., web search or local index) based on the nature of the query.\n\n- **Human-in-the-Loop**: LangGraph's architecture supports human oversight, enabling developers to pause, review, and modify the state of the application during execution. This feature is particularly useful for ensuring the quality and relevance of the generated responses.\n\n- **Streaming and Debugging**: LangGraph offers built-in support for streaming outputs and debugging tools, allowing developers to visualize the flow of data and interactions within the RAG application, making it easier to identify and resolve issues.\n\n### Conclusion\n\nRAG techniques significantly enhance the performance of language models by grounding their responses in real-time, relevant information. LangGraph provides a robust framework for implementing these techniques, enabling developers to create sophisticated, stateful applications that leverage the power of both retrieval systems and language generation. This integration not only improves the accuracy of responses but also enriches the user experience in applications such as chatbots and automated content generation.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-59ff1d9f-6ca8-489e-a1b0-dca447db634e", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of language models by integrating external knowledge retrieval into the generation process. Here\u2019s a breakdown of RAG techniques and how LangGraph can implement them:\n\n### RAG Techniques\n\n- **Overview**: RAG combines the strengths of language models (LLMs) with external knowledge bases. When a query is made, RAG systems first retrieve relevant information from a knowledge base and then use this information to inform the response generated by the LLM. This approach helps in providing up-to-date and contextually relevant answers, reducing the chances of generating incorrect or outdated information.\n\n- **Key Components**:\n  - **Retrieval System**: This component is responsible for fetching relevant documents or data based on the user's query. It ensures that the LLM has access to the most pertinent information.\n  - **Integration of Knowledge**: The retrieved information is incorporated into the prompt sent to the LLM, allowing the model to generate responses that are grounded in factual data.\n\n- **Evaluation Methods**:\n  - **Reference Answer Evaluation**: Comparing the generated responses against known correct answers to assess accuracy.\n  - **Reference-Free Evaluation**: Using self-consistency checks to evaluate the quality of responses without needing a reference answer.\n\n### Implementing RAG with LangGraph\n\nLangGraph provides a structured framework for implementing RAG techniques effectively:\n\n- **Graph Structure**: LangGraph allows developers to create applications as directed acyclic graphs (DAGs), where each node represents a step in the RAG process (e.g., user input, retrieval, response generation). This structure facilitates clear control flow and state management across interactions.\n\n- **Node Types**:\n  - **Query Node**: This node handles user input and can either generate a query for the retriever or respond directly based on previous interactions.\n  - **Retriever Node**: Executes the retrieval step, fetching relevant documents based on the user\u2019s query.\n  - **Response Generation Node**: Utilizes the retrieved context to generate a final answer, ensuring that the response is informed by the most relevant data.\n\n- **Adaptive RAG**: LangGraph supports advanced RAG strategies, such as Adaptive RAG, which combines query analysis with self-corrective mechanisms. This allows the system to dynamically choose between different retrieval methods (e.g., web search or local index) based on the nature of the query.\n\n- **Human-in-the-Loop**: LangGraph's architecture supports human oversight, enabling developers to pause, review, and modify the state of the application during execution. This feature is particularly useful for ensuring the quality and relevance of the generated responses.\n\n- **Streaming and Debugging**: LangGraph offers built-in support for streaming outputs and debugging tools, allowing developers to visualize the flow of data and interactions within the RAG application, making it easier to identify and resolve issues.\n\n### Conclusion\n\nRAG techniques significantly enhance the performance of language models by grounding their responses in real-time, relevant information. LangGraph provides a robust framework for implementing these techniques, enabling developers to create sophisticated, stateful applications that leverage the power of both retrieval systems and language generation. This integration not only improves the accuracy of responses but also enriches the user experience in applications such as chatbots and automated content generation.", "num_response_attempts": 1}
{"input": {"messages": [{"content": "Explain RAG techniques and how LangGraph can implement them.", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "db0bdcdd-4525-4377-940b-0c7c468d78ce", "example": false}, {"content": "Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of language models by integrating external knowledge retrieval into the generation process. Here\u2019s a breakdown of RAG techniques and how LangGraph can implement them:\n\n### RAG Techniques\n\n- **Overview**: RAG combines the strengths of language models (LLMs) with external knowledge bases. When a query is made, RAG systems first retrieve relevant information from a knowledge base and then use this information to inform the response generated by the LLM. This approach helps in providing up-to-date and contextually relevant answers, reducing the chances of generating incorrect or outdated information.\n\n- **Key Components**:\n  - **Retrieval System**: This component is responsible for fetching relevant documents or data based on the user's query. It ensures that the LLM has access to the most pertinent information.\n  - **Integration of Knowledge**: The retrieved information is incorporated into the prompt sent to the LLM, allowing the model to generate responses that are grounded in factual data.\n\n- **Evaluation Methods**:\n  - **Reference Answer Evaluation**: Comparing the generated responses against known correct answers to assess accuracy.\n  - **Reference-Free Evaluation**: Using self-consistency checks to evaluate the quality of responses without needing a reference answer.\n\n### Implementing RAG with LangGraph\n\nLangGraph provides a structured framework for implementing RAG techniques effectively:\n\n- **Graph Structure**: LangGraph allows developers to create applications as directed acyclic graphs (DAGs), where each node represents a step in the RAG process (e.g., user input, retrieval, response generation). This structure facilitates clear control flow and state management across interactions.\n\n- **Node Types**:\n  - **Query Node**: This node handles user input and can either generate a query for the retriever or respond directly based on previous interactions.\n  - **Retriever Node**: Executes the retrieval step, fetching relevant documents based on the user\u2019s query.\n  - **Response Generation Node**: Utilizes the retrieved context to generate a final answer, ensuring that the response is informed by the most relevant data.\n\n- **Adaptive RAG**: LangGraph supports advanced RAG strategies, such as Adaptive RAG, which combines query analysis with self-corrective mechanisms. This allows the system to dynamically choose between different retrieval methods (e.g., web search or local index) based on the nature of the query.\n\n- **Human-in-the-Loop**: LangGraph's architecture supports human oversight, enabling developers to pause, review, and modify the state of the application during execution. This feature is particularly useful for ensuring the quality and relevance of the generated responses.\n\n- **Streaming and Debugging**: LangGraph offers built-in support for streaming outputs and debugging tools, allowing developers to visualize the flow of data and interactions within the RAG application, making it easier to identify and resolve issues.\n\n### Conclusion\n\nRAG techniques significantly enhance the performance of language models by grounding their responses in real-time, relevant information. LangGraph provides a robust framework for implementing these techniques, enabling developers to create sophisticated, stateful applications that leverage the power of both retrieval systems and language generation. This integration not only improves the accuracy of responses but also enriches the user experience in applications such as chatbots and automated content generation.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-59ff1d9f-6ca8-489e-a1b0-dca447db634e", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"language": "en", "description": "The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/concepts", "title": "Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "a24e3440-266a-53d3-895c-ef945d163e99"}, "page_content": "Retrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation", "type": "Document"}, {"metadata": {"description": "We will walk through the evaluation workflow for RAG (retrieval augmented generation).", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "00d6eb31-f609-5497-946f-02b1b6f5ee07"}, "page_content": "RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.", "type": "Document"}, {"metadata": {"description": "Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "title": "Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/rememberizer/", "uuid": "5ae25ce5-6e4c-522b-876d-5f24a7914f64"}, "page_content": "```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "* Retrieval", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/rag/", "title": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/rag/", "uuid": "a539b77a-646a-5fe6-82b5-0f560e06ebf8"}, "page_content": "Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "6028efd4-4079-5b6d-868c-aadc9eea72c1"}, "page_content": "| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain implements the latest research in the field of Natural Language Processing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "title": "arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/additional_resources/arxiv_references/", "uuid": "340f01d5-de75-5ddd-a744-e6021efc7879"}, "page_content": "- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "title": "Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/storm/storm/", "uuid": "cf1d633f-f889-528f-91a0-c6774265bd65"}, "page_content": "## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).", "type": "Document"}, {"metadata": {"description": "langgraph", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b2aa2557-d3fc-55c9-8bc3-49f31ad6a160"}, "page_content": "langgraph evaluation docs", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/", "title": "Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/", "uuid": "bdb840c9-4cd3-5c1c-9603-c51cb8a5d358"}, "page_content": "Tutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments", "type": "Document"}, {"metadata": {"description": "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/rag/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/rag/", "uuid": "cd8936d6-ee48-5764-a3db-5025bf4ecb61"}, "page_content": "Do I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "title": "Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/qa_chat_history/", "uuid": "4f420bf5-f8f1-54bc-950d-66eac8371f81"}, "page_content": "Our graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "title": "Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/", "uuid": "e82df499-c601-58a4-a012-6b245dd00721"}, "page_content": "Adaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "title": "Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/high_level/", "uuid": "23701dd3-42b9-5e92-923c-f463fb360379"}, "page_content": "Why LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "title": "\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "uuid": "72626759-8f24-526d-ab64-cefc84c4f0ca"}, "page_content": "3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json", "type": "Document"}], "answer": "Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of language models by integrating external knowledge retrieval into the generation process. Here\u2019s a breakdown of RAG techniques and how LangGraph can implement them:\n\n### RAG Techniques\n\n- **Overview**: RAG combines the strengths of language models (LLMs) with external knowledge bases. When a query is made, RAG systems first retrieve relevant information from a knowledge base and then use this information to inform the response generated by the LLM. This approach helps in providing up-to-date and contextually relevant answers, reducing the chances of generating incorrect or outdated information.\n\n- **Key Components**:\n  - **Retrieval System**: This component is responsible for fetching relevant documents or data based on the user's query. It ensures that the LLM has access to the most pertinent information.\n  - **Integration of Knowledge**: The retrieved information is incorporated into the prompt sent to the LLM, allowing the model to generate responses that are grounded in factual data.\n\n- **Evaluation Methods**:\n  - **Reference Answer Evaluation**: Comparing the generated responses against known correct answers to assess accuracy.\n  - **Reference-Free Evaluation**: Using self-consistency checks to evaluate the quality of responses without needing a reference answer.\n\n### Implementing RAG with LangGraph\n\nLangGraph provides a structured framework for implementing RAG techniques effectively:\n\n- **Graph Structure**: LangGraph allows developers to create applications as directed acyclic graphs (DAGs), where each node represents a step in the RAG process (e.g., user input, retrieval, response generation). This structure facilitates clear control flow and state management across interactions.\n\n- **Node Types**:\n  - **Query Node**: This node handles user input and can either generate a query for the retriever or respond directly based on previous interactions.\n  - **Retriever Node**: Executes the retrieval step, fetching relevant documents based on the user\u2019s query.\n  - **Response Generation Node**: Utilizes the retrieved context to generate a final answer, ensuring that the response is informed by the most relevant data.\n\n- **Adaptive RAG**: LangGraph supports advanced RAG strategies, such as Adaptive RAG, which combines query analysis with self-corrective mechanisms. This allows the system to dynamically choose between different retrieval methods (e.g., web search or local index) based on the nature of the query.\n\n- **Human-in-the-Loop**: LangGraph's architecture supports human oversight, enabling developers to pause, review, and modify the state of the application during execution. This feature is particularly useful for ensuring the quality and relevance of the generated responses.\n\n- **Streaming and Debugging**: LangGraph offers built-in support for streaming outputs and debugging tools, allowing developers to visualize the flow of data and interactions within the RAG application, making it easier to identify and resolve issues.\n\n### Conclusion\n\nRAG techniques significantly enhance the performance of language models by grounding their responses in real-time, relevant information. LangGraph provides a robust framework for implementing these techniques, enabling developers to create sophisticated, stateful applications that leverage the power of both retrieval systems and language generation. This integration not only improves the accuracy of responses but also enriches the user experience in applications such as chatbots and automated content generation.", "query": "Explain RAG techniques and how LangGraph can implement them.", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."}, {"role": "human", "content": "Set of facts: \n\n <documents>\n<document  language='en' description='The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.' changefreq=None source='https://docs.smith.langchain.com/evaluation/concepts' title='Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='a24e3440-266a-53d3-895c-ef945d163e99'>\nRetrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation\n</document>\n<document  description='We will walk through the evaluation workflow for RAG (retrieval augmented generation).' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/tutorials/rag' title='RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='00d6eb31-f609-5497-946f-02b1b6f5ee07'>\nRAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.\n</document>\n<document  description='Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/retrievers/rememberizer/' title='Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/retrievers/rememberizer/' uuid='5ae25ce5-6e4c-522b-876d-5f24a7914f64'>\n```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)\n</document>\n<document  description='* Retrieval' language='en' changefreq='weekly' source='https://python.langchain.com/docs/concepts/rag/' title='Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/concepts/rag/' uuid='a539b77a-646a-5fe6-82b5-0f560e06ebf8'>\nRetrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)\n</document>\n<document  language='en' description='LangChain implements the latest research in the field of Natural Language Processing.' changefreq='weekly' source='https://python.langchain.com/docs/additional_resources/arxiv_references/' title='arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/additional_resources/arxiv_references/' uuid='6028efd4-4079-5b6d-868c-aadc9eea72c1'>\n| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |\n</document>\n<document  language='en' description='LangChain implements the latest research in the field of Natural Language Processing.' changefreq='weekly' source='https://python.langchain.com/docs/additional_resources/arxiv_references/' title='arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/additional_resources/arxiv_references/' uuid='340f01d5-de75-5ddd-a744-e6021efc7879'>\n- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/storm/storm/' title='Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/storm/storm/' uuid='cf1d633f-f889-528f-91a0-c6774265bd65'>\n## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).\n</document>\n<document  description='langgraph' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph' title='How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='b2aa2557-d3fc-55c9-8bc3-49f31ad6a160'>\nlanggraph evaluation docs\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/' title='Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/' uuid='bdb840c9-4cd3-5c1c-9603-c51cb8a5d358'>\nTutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments\n</document>\n<document  description='One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/rag/' title='Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/rag/' uuid='cd8936d6-ee48-5764-a3db-5025bf4ecb61'>\nDo I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```\n</document>\n<document  language='en' description='In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/qa_chat_history/' title='Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/qa_chat_history/' uuid='4f420bf5-f8f1-54bc-950d-66eac8371f81'>\nOur graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/' title='Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/' uuid='e82df499-c601-58a4-a012-6b245dd00721'>\nAdaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/concepts/high_level/' title='Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/concepts/high_level/' uuid='23701dd3-42b9-5e92-923c-f463fb360379'>\nWhy LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/introduction/' title='\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/introduction/' uuid='72626759-8f24-526d-ab64-cefc84c4f0ca'>\n3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json\n</document>\n</documents> \n\n LLM generation: Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of language models by integrating external knowledge retrieval into the generation process. Here\u2019s a breakdown of RAG techniques and how LangGraph can implement them:\n\n### RAG Techniques\n\n- **Overview**: RAG combines the strengths of language models (LLMs) with external knowledge bases. When a query is made, RAG systems first retrieve relevant information from a knowledge base and then use this information to inform the response generated by the LLM. This approach helps in providing up-to-date and contextually relevant answers, reducing the chances of generating incorrect or outdated information.\n\n- **Key Components**:\n  - **Retrieval System**: This component is responsible for fetching relevant documents or data based on the user's query. It ensures that the LLM has access to the most pertinent information.\n  - **Integration of Knowledge**: The retrieved information is incorporated into the prompt sent to the LLM, allowing the model to generate responses that are grounded in factual data.\n\n- **Evaluation Methods**:\n  - **Reference Answer Evaluation**: Comparing the generated responses against known correct answers to assess accuracy.\n  - **Reference-Free Evaluation**: Using self-consistency checks to evaluate the quality of responses without needing a reference answer.\n\n### Implementing RAG with LangGraph\n\nLangGraph provides a structured framework for implementing RAG techniques effectively:\n\n- **Graph Structure**: LangGraph allows developers to create applications as directed acyclic graphs (DAGs), where each node represents a step in the RAG process (e.g., user input, retrieval, response generation). This structure facilitates clear control flow and state management across interactions.\n\n- **Node Types**:\n  - **Query Node**: This node handles user input and can either generate a query for the retriever or respond directly based on previous interactions.\n  - **Retriever Node**: Executes the retrieval step, fetching relevant documents based on the user\u2019s query.\n  - **Response Generation Node**: Utilizes the retrieved context to generate a final answer, ensuring that the response is informed by the most relevant data.\n\n- **Adaptive RAG**: LangGraph supports advanced RAG strategies, such as Adaptive RAG, which combines query analysis with self-corrective mechanisms. This allows the system to dynamically choose between different retrieval methods (e.g., web search or local index) based on the nature of the query.\n\n- **Human-in-the-Loop**: LangGraph's architecture supports human oversight, enabling developers to pause, review, and modify the state of the application during execution. This feature is particularly useful for ensuring the quality and relevance of the generated responses.\n\n- **Streaming and Debugging**: LangGraph offers built-in support for streaming outputs and debugging tools, allowing developers to visualize the flow of data and interactions within the RAG application, making it easier to identify and resolve issues.\n\n### Conclusion\n\nRAG techniques significantly enhance the performance of language models by grounding their responses in real-time, relevant information. LangGraph provides a robust framework for implementing these techniques, enabling developers to create sophisticated, stateful applications that leverage the power of both retrieval systems and language generation. This integration not only improves the accuracy of responses but also enriches the user experience in applications such as chatbots and automated content generation."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Set of facts: \n\n <documents>\n<document  language='en' description='The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. High quality evaluations can help you rapidly answer these types of questions with confidence.' changefreq=None source='https://docs.smith.langchain.com/evaluation/concepts' title='Evaluation concepts | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='a24e3440-266a-53d3-895c-ef945d163e99'>\nRetrieval Augmented Generation (RAG)\u200b\nRetrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.\ntipFor a comprehensive review of RAG concepts, see our RAG From Scratch series.\nDataset\u200b\nWhen evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).\nEvaluator\u200b\nLLM-as-judge is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.\n\nWhen evaluating RAG applications, you have two main options:\n\nReference answer: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.\nReference-free: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).\n\ntipDive deeper into RAG evaluation concepts with our LangSmith video series:\nRAG answer correctness evaluation\nRAG answer hallucination\nRAG document relevance\nRAG intermediate steps evaluation\n\nApplying RAG Evaluation\u200b\nWhen applying RAG evaluation, consider the following approaches:\n\nOffline evaluation: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.\n\nOnline evaluation: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.\n\nPairwise evaluation: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.\n\ntipExplore our LangSmith video series for more insights on RAG evaluation:\nRAG with online evaluation\nRAG pairwise evaluation\n</document>\n<document  description='We will walk through the evaluation workflow for RAG (retrieval augmented generation).' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/tutorials/rag' title='RAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' lastmod=None content_type='text/html; charset=utf-8' priority=None loc=None uuid='00d6eb31-f609-5497-946f-02b1b6f5ee07'>\nRAG Evaluations | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationQuick StartConceptual GuideHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate intermediate stepsHow to version datasetsUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experimentHow to export filtered traces from experiment to datasetHow to unit test applications (Python only)Run pairwise evaluationsHow to audit evaluator scorescreate_few_shot_evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorTutorialsEvaluate your LLM applicationRAG EvaluationsBacktestingEvaluate an agentRunning SWE-bench with LangSmithPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceEvaluationTutorialsRAG EvaluationsOn this pageRAG Evaluations\nWe will walk through the evaluation workflow for RAG (retrieval augmented generation).\nOverview\u200b\nWe will discuss each piece of the workflow below.\n</document>\n<document  description='Rememberizer is a knowledge enhancement service for AI applications created by  SkyDeck AI Inc.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/retrievers/rememberizer/' title='Rememberizer | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/retrievers/rememberizer/' uuid='5ae25ce5-6e4c-522b-876d-5f24a7914f64'>\n```output\n-> **Question**: What is RAG? \n\n**Answer**: RAG stands for Retrieval-Augmented Generation. It is an AI framework that retrieves facts from an external knowledge base to enhance the responses generated by Large Language Models (LLMs) by providing up-to-date and accurate information. This framework helps users understand the generative process of LLMs and ensures that the model has access to reliable information sources. \n\n-> **Question**: How does Large Language Models works? \n\n**Answer**: Large Language Models (LLMs) work by analyzing massive data sets of language to comprehend and generate human language text. They are built on machine learning, specifically deep learning, which involves training a program to recognize features of data without human intervention. LLMs use neural networks, specifically transformer models, to understand context in human language, making them better at interpreting language even in vague or new contexts. Developers can quickly start building their own LLMs by accessing multiple data sets and using services like Cloudflare's Vectorize and Cloudflare Workers AI platform.\n```\n\n## Related\u200b\n\n- Retriever [conceptual guide](/docs/concepts/retrievers/)\n\n- Retriever [how-to guides](/docs/how_to/#retrievers)\n\n#### Was this page helpful?\n\n- [Basic usage](#basic-usage)\n\n- [Related](#related)\n</document>\n<document  description='* Retrieval' language='en' changefreq='weekly' source='https://python.langchain.com/docs/concepts/rag/' title='Retrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/concepts/rag/' uuid='a539b77a-646a-5fe6-82b5-0f560e06ebf8'>\nRetrieval augmented generation (RAG) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval augmented generation (RAG)\n\nPrerequisites- [Retrieval](/docs/concepts/retrieval/)\n\n## Overview\u200b\n\nRetrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases.\nRAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information.\nWhen given a query, RAG systems first search a knowledge base for relevant information.\nThe system then incorporates this retrieved information into the model's prompt.\nThe model uses the provided context to generate a response to the query.\nBy bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.\n\n## Key concepts\u200b\n\n![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)\n\n(1) **Retrieval system**: Retrieve relevant information from a knowledge base.\n\n(2) **Adding external knowledge**: Pass retrieved information to a model.\n\n## Retrieval system\u200b\n\nModel's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.\nThis limits their ability to answer questions about current events, or to provide specific domain knowledge.\nTo address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training.\nBoth are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval.\nUsing a retrieval system offers several advantages:\n\n- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.\n\n- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.\n\n- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.\n\n- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.\n\nFurther readingSee our conceptual guide on [retrieval](/docs/concepts/retrieval/).\n\n## Adding external knowledge\u200b\n\nWith a retrieval system in place, we need to pass knowledge from this system to the model.\nA RAG pipeline typically achieves this following these steps:\n\n- Receive an input query.\n\n- Use the retrieval system to search for relevant information based on the query.\n\n- Incorporate the retrieved information into the prompt sent to the LLM.\n\n- Generate a response that leverages the retrieved context.\n\nAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\n# Define a system prompt that tells the model how to use the retrieved context\nsystem_prompt = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nContext: {context}:\"\"\"\n    \n# Define a question\nquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"\n\n# Retrieve relevant documents\ndocs = retriever.invoke(question)\n\n# Combine the documents into a single string\ndocs_text = \"\".join(d.page_content for d in docs)\n\n# Populate the system prompt with the retrieved context\nsystem_prompt_fmt = system_prompt.format(context=docs_text)\n</document>\n<document  language='en' description='LangChain implements the latest research in the field of Natural Language Processing.' changefreq='weekly' source='https://python.langchain.com/docs/additional_resources/arxiv_references/' title='arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/additional_resources/arxiv_references/' uuid='6028efd4-4079-5b6d-868c-aadc9eea72c1'>\n| 2005.11401v4Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks | Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al. | 2020\u201105\u201122 | Docs:docs/concepts |\n| 1909.05858v2CTRL: A Conditional Transformer Language Model for Controllable Generation | Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,  et al. | 2019\u201109\u201111 | API:langchain_huggingface...HuggingFaceEndpoint,langchain_community...HuggingFaceTextGenInference,langchain_community...HuggingFaceEndpoint |\n</document>\n<document  language='en' description='LangChain implements the latest research in the field of Natural Language Processing.' changefreq='weekly' source='https://python.langchain.com/docs/additional_resources/arxiv_references/' title='arXiv | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/additional_resources/arxiv_references/' uuid='340f01d5-de75-5ddd-a744-e6021efc7879'>\n- \n**arXiv id:** [2005.14165v4](http://arxiv.org/abs/2005.14165v4) **Published Date:** 2020-05-28\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Recent work has demonstrated substantial gains on many NLP tasks and\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\na specific task. While typically task-agnostic in architecture, this method\nstill requires task-specific fine-tuning datasets of thousands or tens of\nthousands of examples. By contrast, humans can generally perform a new language\ntask from only a few examples or from simple instructions - something which\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\neven reaching competitiveness with prior state-of-the-art fine-tuning\napproaches. Specifically, we train GPT-3, an autoregressive language model with\n175 billion parameters, 10x more than any previous non-sparse language model,\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\napplied without any gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the model. GPT-3\nachieves strong performance on many NLP datasets, including translation,\nquestion-answering, and cloze tasks, as well as several tasks that require\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\nas well as some datasets where GPT-3 faces methodological issues related to\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\nof news articles which human evaluators have difficulty distinguishing from\narticles written by humans. We discuss broader societal impacts of this finding\nand of GPT-3 in general.\n\n## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u200b\n\n- \n**Authors:** Patrick Lewis, Ethan Perez, Aleksandra Piktus,  et al.\n\n- \n**arXiv id:** [2005.11401v4](http://arxiv.org/abs/2005.11401v4) **Published Date:** 2020-05-22\n\n- \n**LangChain:**\n\n- **Documentation:** [docs/concepts](https://python.langchain.com/docs/concepts)\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on\ndownstream NLP tasks. However, their ability to access and precisely manipulate\nknowledge is still limited, and hence on knowledge-intensive tasks, their\nperformance lags behind task-specific architectures. Additionally, providing\nprovenance for their decisions and updating their world knowledge remain open\nresearch problems. Pre-trained models with a differentiable access mechanism to\nexplicit non-parametric memory can overcome this issue, but have so far been\nonly investigated for extractive downstream tasks. We explore a general-purpose\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\ncombine pre-trained parametric and non-parametric memory for language\ngeneration. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\nformulations, one which conditions on the same retrieved passages across the\nwhole generated sequence, the other can use different passages per token. We\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\nFor language generation tasks, we find that RAG models generate more specific,\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\nbaseline.\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/storm/storm/' title='Web Research (STORM) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/storm/storm/' uuid='cf1d633f-f889-528f-91a0-c6774265bd65'>\n## Integration of Million-Plus Token Context Window Models and RAG\n\nThe integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n\n## Impact on Natural Language Processing\n\nThe fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n\n## Applications\n\nThis technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n\n## Challenges and Limitations\n\nDespite their advancements, the integration of these technologies faces several challenges:\n- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n\n## Future Directions\n\nFuture research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n\n## Conclusion\n\nThe integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n\n## References\n\n1. Gemini 1.5 Documentation. (n.d.).\n2. The Evolution of Language Models. (2022).\n3. Introduction to Retrieval-Augmented Generation. (2021).\n4. Leveraging Large Context Windows for NLP. (2023).\n5. Integrating Context Window Models with RAG. (2023).\n6. Deep Learning in NLP. (2020).\n7. Coherence in Text Generation. (2019).\n8. Contextual Relevance in AI. (2021).\n9. Applications of NLP in Content Creation. (2022).\n10. AI in Customer Support. (2023).\n11. NLP for Research Assistance. (2021).\n12. Computational Challenges in NLP. (2022).\n13. Data Privacy in AI Systems. (2020).\n14. Addressing Bias in AI. (2021).\n15. Future of NLP Technologies. (2023).\n</document>\n<document  description='langgraph' language='en' changefreq=None source='https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph' title='How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith' content_type='text/html; charset=utf-8' lastmod=None priority=None loc=None uuid='b2aa2557-d3fc-55c9-8bc3-49f31ad6a160'>\nlanggraph evaluation docs\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/' title='Tutorials | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/' uuid='bdb840c9-4cd3-5c1c-9603-c51cb8a5d358'>\nTutorials\n\nTutorials\u00b6\nNew to LangGraph or LLM app development? Read this material to get up and running building your first applications.\nGet Started \ud83d\ude80\u00b6\n\nLangGraph Quickstart: Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.\nLangGraph Server Quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.\nLangGraph Cloud QuickStart: Deploy a LangGraph app using LangGraph Cloud.\n\nUse cases \ud83d\udee0\ufe0f\u00b6\nExplore practical implementations tailored for specific scenarios:\nChatbots\u00b6\n\nCustomer Support: Build a multi-functional support bot for flights, hotels, and car rentals.\nPrompt Generation from User Requirements: Build an information gathering chatbot.\nCode Assistant: Build a code analysis and generation assistant.\n\nRAG\u00b6\n\nAgentic RAG: Use an agent to figure out how to retrieve the most relevant information before using the retrieved information to answer the user's question.\nAdaptive RAG: Adaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG. Implementation of: https://arxiv.org/abs/2403.14403\nFor a version that uses a local LLM: Adaptive RAG using local LLMs\n\nCorrective RAG: Uses an LLM to grade the quality of the retrieved information from the given source, and if the quality is low, it will try to retrieve the information from another source. Implementation of: https://arxiv.org/pdf/2401.15884.pdf \nFor a version that uses a local LLM: Corrective RAG using local LLMs\n\nSelf-RAG: Self-RAG is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents and generations. Implementation of https://arxiv.org/abs/2310.11511.\nFor a version that uses a local LLM: Self-RAG using local LLMs \n\nSQL Agent: Build a SQL agent that can answer questions about a SQL database.\n\nAgent Architectures\u00b6\nMulti-Agent Systems\u00b6\n\nNetwork: Enable two or more agents to collaborate on a task\nSupervisor: Use an LLM to orchestrate and delegate to individual agents\nHierarchical Teams: Orchestrate nested teams of agents to solve problems\n\nPlanning Agents\u00b6\n\nPlan-and-Execute: Implement a basic planning and execution agent\nReasoning without Observation: Reduce re-planning by saving observations as variables\nLLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\n\nReflection & Critique\u00b6\n\nBasic Reflection: Prompt the agent to reflect on and revise its outputs\nReflexion: Critique missing and superfluous details to guide next steps\nTree of Thoughts: Search over candidate solutions to a problem using a scored tree\nLanguage Agent Tree Search: Use reflection and rewards to drive a monte-carlo tree search over agents\nSelf-Discover Agent: Analyze an agent that learns about its own capabilities\n\nEvaluation\u00b6\n\nAgent-based: Evaluate chatbots via simulated user interactions\nIn LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\n\nExperimental\u00b6\n\nWeb Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\nTNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\nWeb Navigation: Build an agent that can navigate and interact with websites\nCompetitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\nComplex data extraction: Build an agent that can use function calling to do complex extraction tasks\n\nComments\n</document>\n<document  description='One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/rag/' title='Build a Retrieval Augmented Generation (RAG) App: Part 1 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/rag/' uuid='cd8936d6-ee48-5764-a3db-5025bf4ecb61'>\nDo I need to use LangGraph?LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:\n\n```python\nquestion = \"...\"\n\nretrieved_docs = vector_store.similarity_search(question)\ndocs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\nprompt = prompt.invoke({\"question\": question, \"context\": formatted_docs})\nanswer = llm.invoke(prompt)\n```\n\nThe benefits of LangGraph include:\n\n- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;\n\n- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);\n\n- Support for persistence, human-in-the-loop, and other features.\n\nMany use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.\n\n#### Usage\u200b\n\nLet's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.\n\nInvoke:\n\n```python\nresult = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n\nprint(f'Context: {result[\"context\"]}\\n\\n')\nprint(f'Answer: {result[\"answer\"]}')\n```\n</document>\n<document  language='en' description='In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.' changefreq='weekly' source='https://python.langchain.com/docs/tutorials/qa_chat_history/' title='Build a Retrieval Augmented Generation (RAG) App: Part 2 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/tutorials/qa_chat_history/' uuid='4f420bf5-f8f1-54bc-950d-66eac8371f81'>\nOur graph will consist of three nodes:\n\n1. A node that fields the user input, either generating a query for the retriever or responding directly;\n\n2. A node for the retriever tool that executes the retrieval step;\n\n3. A node that generates the final response using the retrieved context.\n\nWe build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.\n\n```python\nfrom langchain_core.messages import SystemMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Step 1: Generate an AIMessage that may include a tool-call to be sent.\ndef query_or_respond(state: MessagesState):\n    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n    llm_with_tools = llm.bind_tools([retrieve])\n    response = llm_with_tools.invoke(state[\"messages\"])\n    # MessagesState appends messages to state instead of overwriting\n    return {\"messages\": [response]}\n\n# Step 2: Execute the retrieval.\ntools = ToolNode([retrieve])\n\n# Step 3: Generate a response using the retrieved content.\ndef generate(state: MessagesState):\n    \"\"\"Generate answer.\"\"\"\n    # Get generated ToolMessages\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            break\n    tool_messages = recent_tool_messages[::-1]\n\n    # Format into prompt\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    system_message_content = (\n        \"You are an assistant for question-answering tasks. \"\n        \"Use the following pieces of retrieved context to answer \"\n        \"the question. If you don't know the answer, say that you \"\n        \"don't know. Use three sentences maximum and keep the \"\n        \"answer concise.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Run\n    response = llm.invoke(prompt)\n    return {\"messages\": [response]}\n```\n\n**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)\nFinally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to \"short-circuit\" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step\n\n```python\nfrom langgraph.graph import END\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(generate)\n\ngraph_builder.set_entry_point(\"query_or_respond\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\ngraph_builder.add_edge(\"tools\", \"generate\")\ngraph_builder.add_edge(\"generate\", END)\n\ngraph = graph_builder.compile()\n```\n\n**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)\n```python\nfrom IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/' title='Adaptive RAG | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/' uuid='e82df499-c601-58a4-a012-6b245dd00721'>\nAdaptive RAG\n\nAdaptive RAG\u00b6\nAdaptive RAG is a strategy for RAG that unites (1) query analysis with (2) active / self-corrective RAG.\nIn the paper, they report query analysis to route across:\n\nNo Retrieval\nSingle-shot RAG\nIterative RAG\n\nLet's build on this using LangGraph. \nIn our implementation, we will route between:\n\nWeb search: for questions related to recent events\nSelf-corrective RAG: for questions related to our index\n\nSetup\u00b6\nFirst, let's install our required packages and set our API keys\n%%capture --no-stderr\n! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub chromadb langchain langgraph  tavily-python\n\nimport getpass\nimport os\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"COHERE_API_KEY\")\n_set_env(\"TAVILY_API_KEY\")\n\nSet up LangSmith for LangGraph development\n\n        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here. \n    \n\nCreate Index\u00b6\n### Build Index\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings\n\n# Set embeddings\nembd = OpenAIEmbeddings()\n\n# Docs to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorstore\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embd,\n)\nretriever = vectorstore.as_retriever()\n\nAPI Reference:\nRecursiveCharacterTextSplitter | WebBaseLoader | Chroma | OpenAIEmbeddings\n\nLLMs\u00b6\n\nUsing Pydantic with LangChain\n\n        This notebook uses Pydantic v2 BaseModel, which requires langchain-core >= 0.3. Using langchain-core < 0.3 will result in errors due to mixing of Pydantic v1 and v2 BaseModels.\n    \n\n### Router\n\nfrom typing import Literal\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom pydantic import BaseModel, Field\n\n# Data model\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n    )\n\n# LLM with function call\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nquestion_router = route_prompt | structured_llm_router\nprint(\n    question_router.invoke(\n        {\"question\": \"Who will the Bears draft first in the NFL draft?\"}\n    )\n)\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n\ndatasource='web_search'\ndatasource='vectorstore'\n\nAPI Reference:\nChatPromptTemplate | ChatOpenAI\n\n### Retrieval Grader\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/concepts/high_level/' title='Why LangGraph? | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' lastmod='2024-12-03' content_type=None priority=None loc='https://langchain-ai.github.io/langgraph/concepts/high_level/' uuid='23701dd3-42b9-5e92-923c-f463fb360379'>\nWhy LangGraph?\n\nWhy LangGraph?\u00b6\nLLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example RAG performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\nHowever, we often want LLM systems that can pick their own control flow! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent gives an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n\nUsing an LLM to route between two potential paths\nUsing an LLM to decide which of many tools to call\nUsing an LLM to decide whether the generated answer is sufficient or more work is need\n\nThere are many different types of agent architectures to consider, which give an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem. \n\nSeveral concepts are utilized in many agent architectures:\n\nTool calling: this is often how LLMs make decisions\nAction taking: often times, the LLMs' outputs are used as the input to an action\nMemory: reliable systems need to have knowledge of things that occurred\nPlanning: planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n\nChallenges\u00b6\nIn practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n\nCore Principles\u00b6\nThe motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents. \n\nControllability\nLangGraph gives the developer a high degree of control by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic. \nPersistence\nLangGraph gives the developer many options for persisting graph state using short-term or long-term (e.g., via a database) memory. \nHuman-in-the-Loop\nThe persistence layer enables several different human-in-the-loop interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step. \nStreaming\nLangGraph comes with first class support for streaming, which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events (like a tool call being taken) as well as of tokens that an LLM may emit.\nDebugging\u00b6\nOnce you've built a graph, you often want to test and debug it. LangGraph Studio is a specialized IDE for visualization and debugging of LangGraph applications.\n\nDeployment\u00b6\nOnce you have confidence in your LangGraph application, many developers want an easy path to deployment. LangGraph Platform offers a range of options for deploying LangGraph graphs.\nComments\n</document>\n<document  description='' language='' changefreq=None source='https://langchain-ai.github.io/langgraph/tutorials/introduction/' title='\ud83d\ude80 LangGraph Quick Start | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph' content_type=None lastmod='2024-12-03' priority=None loc='https://langchain-ai.github.io/langgraph/tutorials/introduction/' uuid='72626759-8f24-526d-ab64-cefc84c4f0ca'>\n3. Relation to LangChain:\n   - LangGraph builds upon LangChain, which is one of the leading frameworks for creating LLM-powered applications.\n   - LangChain itself uses the LangChain Expression Language (LCEL) to define and execute step-by-step action sequences, also known as chains.\n   - LangChain allows the creation of DAGs (Directed Acyclic Graphs), which represent the flow of operations in an application.\n\n4. Key Features:\n   - Stateful Applications: Unlike simple query-response models, LangGraph allows the creation of applications that maintain state across interactions.\n   - Multi-Actor Systems: It supports coordinating multiple AI \"actors\" or components within a single application, enabling more complex interactions and workflows.\n\n5. Use Cases:\n   While not explicitly mentioned in the search results, LangGraph is typically used for creating more sophisticated AI applications such as:\n   - Multi-turn conversational agents\n   - Complex task-planning systems\n   - Applications requiring memory and context management across multiple steps or actors\n\nLearning LangGraph can be a valuable skill, especially if you're interested in developing advanced applications with LLMs that go beyond simple question-answering or text generation tasks. It allows for the creation of more dynamic, interactive, and stateful AI systems.\n\nIs there any specific aspect of LangGraph you'd like to know more about, or do you have any questions about how it compares to or works with LangChain?\n\nevents = graph.stream(\n    {\n        \"messages\": [\n            (\"user\", \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\")\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n\n================================[1m Human Message [0m=================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n==================================[1m Ai Message [0m==================================\n\n[{'text': \"That's an excellent idea! Building an autonomous agent with LangGraph is a great way to explore its capabilities and learn about advanced AI application development. LangGraph's features make it well-suited for creating autonomous agents. Let me provide some additional insights and encouragement for your project.\", 'type': 'text'}, {'id': 'toolu_017t6BS5rNCzFWcpxRizDKjE', 'input': {'query': 'building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_017t6BS5rNCzFWcpxRizDKjE)\n Call ID: toolu_017t6BS5rNCzFWcpxRizDKjE\n  Args:\n    query: building autonomous agents with LangGraph examples and tutorials\n=================================[1m Tool Message [0m=================================\nName: tavily_search_results_json\n</document>\n</documents> \n\n LLM generation: Retrieval-Augmented Generation (RAG) is a powerful technique that enhances the capabilities of language models by integrating external knowledge retrieval into the generation process. Here\u2019s a breakdown of RAG techniques and how LangGraph can implement them:\n\n### RAG Techniques\n\n- **Overview**: RAG combines the strengths of language models (LLMs) with external knowledge bases. When a query is made, RAG systems first retrieve relevant information from a knowledge base and then use this information to inform the response generated by the LLM. This approach helps in providing up-to-date and contextually relevant answers, reducing the chances of generating incorrect or outdated information.\n\n- **Key Components**:\n  - **Retrieval System**: This component is responsible for fetching relevant documents or data based on the user's query. It ensures that the LLM has access to the most pertinent information.\n  - **Integration of Knowledge**: The retrieved information is incorporated into the prompt sent to the LLM, allowing the model to generate responses that are grounded in factual data.\n\n- **Evaluation Methods**:\n  - **Reference Answer Evaluation**: Comparing the generated responses against known correct answers to assess accuracy.\n  - **Reference-Free Evaluation**: Using self-consistency checks to evaluate the quality of responses without needing a reference answer.\n\n### Implementing RAG with LangGraph\n\nLangGraph provides a structured framework for implementing RAG techniques effectively:\n\n- **Graph Structure**: LangGraph allows developers to create applications as directed acyclic graphs (DAGs), where each node represents a step in the RAG process (e.g., user input, retrieval, response generation). This structure facilitates clear control flow and state management across interactions.\n\n- **Node Types**:\n  - **Query Node**: This node handles user input and can either generate a query for the retriever or respond directly based on previous interactions.\n  - **Retriever Node**: Executes the retrieval step, fetching relevant documents based on the user\u2019s query.\n  - **Response Generation Node**: Utilizes the retrieved context to generate a final answer, ensuring that the response is informed by the most relevant data.\n\n- **Adaptive RAG**: LangGraph supports advanced RAG strategies, such as Adaptive RAG, which combines query analysis with self-corrective mechanisms. This allows the system to dynamically choose between different retrieval methods (e.g., web search or local index) based on the nature of the query.\n\n- **Human-in-the-Loop**: LangGraph's architecture supports human oversight, enabling developers to pause, review, and modify the state of the application during execution. This feature is particularly useful for ensuring the quality and relevance of the generated responses.\n\n- **Streaming and Debugging**: LangGraph offers built-in support for streaming outputs and debugging tools, allowing developers to visualize the flow of data and interactions within the RAG application, making it easier to identify and resolve issues.\n\n### Conclusion\n\nRAG techniques significantly enhance the performance of language models by grounding their responses in real-time, relevant information. LangGraph provides a robust framework for implementing these techniques, enabling developers to create sophisticated, stateful applications that leverage the power of both retrieval systems and language generation. This integration not only improves the accuracy of responses but also enriches the user experience in applications such as chatbots and automated content generation.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_C1vxKb8Djixsvemn3UDIiVBW", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-113124fb-c295-410c-a3f1-e5df27fca6ba", "example": false, "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_C1vxKb8Djixsvemn3UDIiVBW", "type": "tool_call"}], "invalid_tool_calls": []}}
