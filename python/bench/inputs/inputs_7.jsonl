{"messages": [{"role": "user", "content": "can you host this using stream lit"}]}
{"messages": [{"role": "user", "content": "can you host this using stream lit"}]}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Docusaurus is a static-site generator which provides out-of-the-box documentation features.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "title": "Docusaurus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/docusaurus/", "uuid": "b3ac35d4-920c-50c6-beca-54999b9e3065"}, "page_content": "Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nCreate a custom prompt template\\\\nCreate a custom example selector\\\\nProvide few shot examples to a prompt\\\\nPrompt Serialization\\\\nExample Selectors\\\\nOutput Parsers\\\\n\\\\n\\\\nReference\\\\nPromptTemplates\\\\nExample Selector\\\\n\\\\n\\\\n\\\\n\\\\nLLMs\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nGeneric Functionality\\\\nCustom LLM\\\\nFake LLM\\\\nLLM Caching\\\\nLLM Serialization\\\\nToken Usage Tracking\\\\n\\\\n\\\\nIntegrations\\\\nAI21\\\\nAleph Alpha\\\\nAnthropic\\\\nAzure OpenAI LLM Example\\\\nBanana\\\\nCerebriumAI LLM Example\\\\nCohere\\\\nDeepInfra LLM Example\\\\nForefrontAI LLM Example\\\\nGooseAI LLM Example\\\\nHugging Face Hub\\\\nManifest\\\\nModal\\\\nOpenAI\\\\nPetals LLM Example\\\\nPromptLayer OpenAI\\\\nSageMakerEndpoint\\\\nSelf-Hosted Models via Runhouse\\\\nStochasticAI\\\\nWriter\\\\n\\\\n\\\\nAsync API for LLM\\\\nStreaming with LLMs\\\\n\\\\n\\\\nReference\\\\n\\\\n\\\\nDocument Loaders\\\\nKey Concepts\\\\nHow To Guides\\\\nCoNLL-U\\\\nAirbyte JSON\\\\nAZLyrics\\\\nBlackboard\\\\nCollege Confidential\\\\nCopy Paste\\\\nCSV Loader\\\\nDirectory Loader\\\\nEmail\\\\nEverNote\\\\nFacebook Chat\\\\nFigma\\\\nGCS Directory\\\\nGCS File Storage\\\\nGitBook\\\\nGoogle Drive\\\\nGutenberg\\\\nHacker News\\\\nHTML\\\\niFixit\\\\nImages\\\\nIMSDb\\\\nMarkdown\\\\nNotebook\\\\nNotion\\\\nObsidian\\\\nPDF\\\\nPowerPoint\\\\nReadTheDocs Documentation\\\\nRoam\\\\ns3 Directory\\\\ns3 File\\\\nSubtitle Files\\\\nTelegram\\\\nUnstructured File Loader\\\\nURL\\\\nWeb Base\\\\nWord Documents\\\\nYouTube\\\\n\\\\n\\\\n\\\\n\\\\nUtils\\\\nKey Concepts\\\\nGeneric Utilities\\\\nBash\\\\nBing Search\\\\nGoogle Search\\\\nGoogle Serper API\\\\nIFTTT WebHooks\\\\nPython REPL\\\\nRequests\\\\nSearxNG Search API\\\\nSerpAPI\\\\nWolfram Alpha\\\\nZapier Natural Language Actions API\\\\n\\\\n\\\\nReference\\\\nPython REPL\\\\nSerpAPI\\\\nSearxNG Search\\\\nDocstore\\\\nText Splitter\\\\nEmbeddings\\\\nVectorStores\\\\n\\\\n\\\\n\\\\n\\\\nIndexes\\\\nGetting Started\\\\nKey Concepts\\\\nHow To Guides\\\\nEmbeddings\\\\nHypothetical Document Embeddings\\\\nText Splitter\\\\nVectorStores\\\\nAtlasDB\\\\nChroma\\\\nDeep Lake\\\\nElasticSearch\\\\nFAISS\\\\nMilvus\\\\nOpenSearch\\\\nPGVector\\\\nPinecone\\\\nQdrant\\\\nRedis\\\\nWeaviate\\\\nChatGPT Plugin Retriever\\\\nVectorStore Retriever\\\\nAnalyze Document\\\\nChat Index\\\\nGraph QA\\\\nQuestion Answering with Sources\\\\nQuestion Answering\\\\nSummarization\\\\nRetrieval Question/Answering\\\\nRetrieval Question Answering with Sources\\\\nVector DB Text Generation\\\\n\\\\n\\\\n\\\\n\\\\nChains\\\\nGetting Started\\\\nHow-To Guides\\\\nGeneric Chains\\\\nLoading from LangChainHub\\\\nLLM Chain\\\\nSequential Chains\\\\nSerialization\\\\nTransformation Chain\\\\n\\\\n\\\\nUtility Chains\\\\nAPI Chains\\\\nSelf-Critique Chain with Constitutional AI\\\\nBashChain\\\\nLLMCheckerChain\\\\nLLM Math\\\\nLLMRequestsChain\\\\nLLMSummarizationCheckerChain\\\\nModeration\\\\nPAL\\\\nSQLite example\\\\n\\\\n\\\\nAsync API for Chain\\\\n\\\\n\\\\nKey Concepts\\\\nReference\\\\n\\\\n\\\\nAgents\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nAgents and Vectorstores\\\\nAsync API for Agent\\\\nConversation Agent (for Chat Models)\\\\nChatGPT Plugins\\\\nCustom Agent\\\\nDefining Custom Tools\\\\nHuman as a tool\\\\nIntermediate Steps\\\\nLoading from LangChainHub\\\\nMax Iterations\\\\nMulti Input Tools\\\\nSearch Tools\\\\nSerialization\\\\nAdding SharedMemory to an Agent and its Tools\\\\nCSV Agent\\\\nJSON Agent\\\\nOpenAPI Agent\\\\nPandas Dataframe Agent\\\\nPython Agent\\\\nSQL Database Agent\\\\nVectorstore Agent\\\\nMRKL\\\\nMRKL Chat\\\\nReAct\\\\nSelf Ask With Search\\\\n\\\\n\\\\nReference\\\\n\\\\n\\\\nMemory\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nConversationBufferMemory\\\\nConversationBufferWindowMemory\\\\nEntity Memory\\\\nConversation Knowledge Graph Memory\\\\nConversationSummaryMemory\\\\nConversationSummaryBufferMemory\\\\nConversationTokenBufferMemory\\\\nAdding Memory To an LLMChain\\\\nAdding Memory to a Multi-Input Chain\\\\nAdding Memory to an Agent\\\\nChatGPT Clone\\\\nConversation Agent\\\\nConversational Memory Customization\\\\nCustom Memory\\\\nMultiple Memory\\\\n\\\\n\\\\n\\\\n\\\\nChat\\\\nGetting Started\\\\nKey Concepts\\\\nHow-To Guides\\\\nAgent\\\\nChat Vector DB\\\\nFew Shot Examples\\\\nMemory\\\\nPromptLayer ChatOpenAI\\\\nStreaming\\\\nRetrieval", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html", "title": "langchain.memory.buffer.ConversationBufferMemory \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "43dec14d-b47c-5129-b551-7745bd6498a4"}, "page_content": "langchain.memory.buffer.ConversationBufferMemory \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17\n\nThis is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\n\nLangChain\n\nCore\n\nCommunity\n\nExperimental\n\nText splitters\n\nai21\n\nairbyte\n\nanthropic\n\nastradb\n\naws\n\nazure-dynamic-sessions\n\nbox\n\nchroma\n\ncohere\n\ncouchbase\n\nelasticsearch\n\nexa\n\nfireworks\n\ngoogle-community\n\ngoogle-genai\n\ngoogle-vertexai\n\ngroq\n\nhuggingface\n\nibm\n\nmilvus\n\nmistralai\n\nmongodb\n\nnomic\n\nnvidia-ai-endpoints\n\nollama\n\nopenai\n\npinecone\n\npostgres\n\nprompty\n\nqdrant\n\nrobocorp\n\ntogether\n\nunstructured\n\nvoyageai\n\nweaviate\n\nPartner libs\n\nai21\nairbyte\nanthropic\nastradb\naws\nazure-dynamic-sessions\nbox\nchroma\ncohere\ncouchbase\nelasticsearch\nexa\nfireworks\ngoogle-community\ngoogle-genai\ngoogle-vertexai\ngroq\nhuggingface\nibm\nmilvus\nmistralai\nmongodb\nnomic\nnvidia-ai-endpoints\nollama\nopenai\npinecone\npostgres\nprompty\nqdrant\nrobocorp\ntogether\nunstructured\nvoyageai\nweaviate\n\nDocs\n\nToggle Menu\n\nlangchain.memory.buffer.ConversationBufferMemory\nConversationBufferMemory\nConversationBufferMemory.ai_prefix\nConversationBufferMemory.chat_memory\nConversationBufferMemory.human_prefix\nConversationBufferMemory.input_key\nConversationBufferMemory.output_key\nConversationBufferMemory.return_messages\nConversationBufferMemory.abuffer()\nConversationBufferMemory.abuffer_as_messages()\nConversationBufferMemory.abuffer_as_str()\nConversationBufferMemory.aclear()\nConversationBufferMemory.aload_memory_variables()\nConversationBufferMemory.asave_context()\nConversationBufferMemory.clear()\nConversationBufferMemory.load_memory_variables()\nConversationBufferMemory.save_context()\nConversationBufferMemory.buffer\nConversationBufferMemory.buffer_as_messages\nConversationBufferMemory.buffer_as_str\n\nlangchain.memory.buffer.ConversationBufferMemory\u00b6\n\nclass langchain.memory.buffer.ConversationBufferMemory[source]\u00b6\nBases: BaseChatMemory\nBuffer for storing conversation memory.\n\nparam ai_prefix: str = 'AI'\u00b6\n\nparam chat_memory: BaseChatMessageHistory [Optional]\u00b6\n\nparam human_prefix: str = 'Human'\u00b6\n\nparam input_key: Optional[str] = None\u00b6\n\nparam output_key: Optional[str] = None\u00b6\n\nparam return_messages: bool = False\u00b6\n\nasync abuffer() \u2192 Any[source]\u00b6\nString buffer of memory.\n\nReturn type\nAny\n\nasync abuffer_as_messages() \u2192 List[BaseMessage][source]\u00b6\nExposes the buffer as a list of messages in case return_messages is False.\n\nReturn type\nList[BaseMessage]\n\nasync abuffer_as_str() \u2192 str[source]\u00b6\nExposes the buffer as a string in case return_messages is True.\n\nReturn type\nstr\n\nasync aclear() \u2192 None\u00b6\nClear memory contents.\n\nReturn type\nNone\n\nasync aload_memory_variables(inputs: Dict[str, Any]) \u2192 Dict[str, Any][source]\u00b6\nReturn key-value pairs given the text input to the chain.\n\nParameters\ninputs (Dict[str, Any]) \u2013 \n\nReturn type\nDict[str, Any]\n\nasync asave_context(inputs: Dict[str, Any], outputs: Dict[str, str]) \u2192 None\u00b6\nSave context from this conversation to buffer.\n\nParameters\n\ninputs (Dict[str, Any]) \u2013 \noutputs (Dict[str, str]) \u2013 \n\nReturn type\nNone\n\nclear() \u2192 None\u00b6\nClear memory contents.\n\nReturn type\nNone\n\nload_memory_variables(inputs: Dict[str, Any]) \u2192 Dict[str, Any][source]\u00b6\nReturn history buffer.\n\nParameters\ninputs (Dict[str, Any]) \u2013 \n\nReturn type\nDict[str, Any]\n\nsave_context(inputs: Dict[str, Any], outputs: Dict[str, str]) \u2192 None\u00b6\nSave context from this conversation to buffer.\n\nParameters\n\ninputs (Dict[str, Any]) \u2013 \noutputs (Dict[str, str]) \u2013 \n\nReturn type\nNone\n\nproperty buffer: Any\u00b6\nString buffer of memory.\n\nproperty buffer_as_messages: List[BaseMessage]\u00b6\nExposes the buffer as a list of messages in case return_messages is False.\n\nproperty buffer_as_str: str\u00b6\nExposes the buffer as a string in case return_messages is True.\n\nExamples using ConversationBufferMemory\u00b6\n\nBedrock\nBittensor\nChat Over Documents with Vectara\nGradio\nLlama2Chat\nMemorize\nNVIDIA NIMs\nReddit Search \nSAP HANA Cloud Vector Engine\nSceneXplain\nXata", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html", "title": "langchain.memory.token_buffer.ConversationTokenBufferMemory \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "f2b46bd1-4e6f-5942-8483-6c10a4447ec9"}, "page_content": "langchain.memory.token_buffer.ConversationTokenBufferMemory \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17\n\nThis is a legacy site. Please use the latest v0.2 and v0.3 API references instead.\n\nLangChain\n\nCore\n\nCommunity\n\nExperimental\n\nText splitters\n\nai21\n\nairbyte\n\nanthropic\n\nastradb\n\naws\n\nazure-dynamic-sessions\n\nbox\n\nchroma\n\ncohere\n\ncouchbase\n\nelasticsearch\n\nexa\n\nfireworks\n\ngoogle-community\n\ngoogle-genai\n\ngoogle-vertexai\n\ngroq\n\nhuggingface\n\nibm\n\nmilvus\n\nmistralai\n\nmongodb\n\nnomic\n\nnvidia-ai-endpoints\n\nollama\n\nopenai\n\npinecone\n\npostgres\n\nprompty\n\nqdrant\n\nrobocorp\n\ntogether\n\nunstructured\n\nvoyageai\n\nweaviate\n\nPartner libs\n\nai21\nairbyte\nanthropic\nastradb\naws\nazure-dynamic-sessions\nbox\nchroma\ncohere\ncouchbase\nelasticsearch\nexa\nfireworks\ngoogle-community\ngoogle-genai\ngoogle-vertexai\ngroq\nhuggingface\nibm\nmilvus\nmistralai\nmongodb\nnomic\nnvidia-ai-endpoints\nollama\nopenai\npinecone\npostgres\nprompty\nqdrant\nrobocorp\ntogether\nunstructured\nvoyageai\nweaviate\n\nDocs\n\nToggle Menu\n\nlangchain.memory.token_buffer.ConversationTokenBufferMemory\nConversationTokenBufferMemory\nConversationTokenBufferMemory.ai_prefix\nConversationTokenBufferMemory.chat_memory\nConversationTokenBufferMemory.human_prefix\nConversationTokenBufferMemory.input_key\nConversationTokenBufferMemory.llm\nConversationTokenBufferMemory.max_token_limit\nConversationTokenBufferMemory.memory_key\nConversationTokenBufferMemory.output_key\nConversationTokenBufferMemory.return_messages\nConversationTokenBufferMemory.aclear()\nConversationTokenBufferMemory.aload_memory_variables()\nConversationTokenBufferMemory.asave_context()\nConversationTokenBufferMemory.clear()\nConversationTokenBufferMemory.load_memory_variables()\nConversationTokenBufferMemory.save_context()\nConversationTokenBufferMemory.buffer\nConversationTokenBufferMemory.buffer_as_messages\nConversationTokenBufferMemory.buffer_as_str\n\nlangchain.memory.token_buffer.ConversationTokenBufferMemory\u00b6\n\nclass langchain.memory.token_buffer.ConversationTokenBufferMemory[source]\u00b6\nBases: BaseChatMemory\nConversation chat memory with token limit.\n\nparam ai_prefix: str = 'AI'\u00b6\n\nparam chat_memory: BaseChatMessageHistory [Optional]\u00b6\n\nparam human_prefix: str = 'Human'\u00b6\n\nparam input_key: Optional[str] = None\u00b6\n\nparam llm: BaseLanguageModel [Required]\u00b6\n\nparam max_token_limit: int = 2000\u00b6\n\nparam memory_key: str = 'history'\u00b6\n\nparam output_key: Optional[str] = None\u00b6\n\nparam return_messages: bool = False\u00b6\n\nasync aclear() \u2192 None\u00b6\nClear memory contents.\n\nReturn type\nNone\n\nasync aload_memory_variables(inputs: Dict[str, Any]) \u2192 Dict[str, Any]\u00b6\nAsync return key-value pairs given the text input to the chain.\n\nParameters\ninputs (Dict[str, Any]) \u2013 The inputs to the chain.\n\nReturns\nA dictionary of key-value pairs.\n\nReturn type\nDict[str, Any]\n\nasync asave_context(inputs: Dict[str, Any], outputs: Dict[str, str]) \u2192 None\u00b6\nSave context from this conversation to buffer.\n\nParameters\n\ninputs (Dict[str, Any]) \u2013 \noutputs (Dict[str, str]) \u2013 \n\nReturn type\nNone\n\nclear() \u2192 None\u00b6\nClear memory contents.\n\nReturn type\nNone\n\nload_memory_variables(inputs: Dict[str, Any]) \u2192 Dict[str, Any][source]\u00b6\nReturn history buffer.\n\nParameters\ninputs (Dict[str, Any]) \u2013 \n\nReturn type\nDict[str, Any]\n\nsave_context(inputs: Dict[str, Any], outputs: Dict[str, str]) \u2192 None[source]\u00b6\nSave context from this conversation to buffer. Pruned.\n\nParameters\n\ninputs (Dict[str, Any]) \u2013 \noutputs (Dict[str, str]) \u2013 \n\nReturn type\nNone\n\nproperty buffer: Any\u00b6\nString buffer of memory.\n\nproperty buffer_as_messages: List[BaseMessage]\u00b6\nExposes the buffer as a list of messages in case return_messages is True.\n\nproperty buffer_as_str: str\u00b6\nExposes the buffer as a string in case return_messages is False.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "e462f8bc-ada5-5ed1-bffb-264eabc3b6db"}, "page_content": "We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.\n\n## Automatic history management\u200b\n\nThe previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/). You can [enable persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) in LangGraph applications by providing a `checkpointer` when compiling the graph.\n\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nworkflow = StateGraph(state_schema=MessagesState)\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    system_prompt = (\n        \"You are a helpful assistant. \"\n        \"Answer all questions to the best of your ability.\"\n    )\n    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": response}\n\n# Define the node and edge\nworkflow.add_node(\"model\", call_model)\nworkflow.add_edge(START, \"model\")\n\n# Add simple in-memory checkpointer\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n```\n\n**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)\nWe'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:\n\n```python\napp.invoke(\n    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\n```\n\n```output\n{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),\n  AIMessage(content=\"J'aime programmer.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}\n```\n\n```python\napp.invoke(\n    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n    config={\"configurable\": {\"thread_id\": \"1\"}},\n)\n```", "type": "Document"}, {"metadata": {"description": "This tutorial shows how to implement an agent with long-term memory capabilities using LangGraph. The agent can store, retrieve, and use memories to enhance its interactions with users.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/migrating_memory/long_term_memory_agent/", "title": "A Long-Term Memory Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/migrating_memory/long_term_memory_agent/", "uuid": "f517dd95-be2a-5b0a-baff-a625d00adc8a"}, "page_content": "A Long-Term Memory Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# A Long-Term Memory Agent\n\nThis tutorial shows how to implement an agent with long-term memory capabilities using LangGraph. The agent can store, retrieve, and use memories to enhance its interactions with users.\n\nInspired by papers like [MemGPT](https://memgpt.ai/) and distilled from our own works on long-term memory, the graph extracts memories from chat interactions and persists them to a database. \"Memory\" in this tutorial will be represented in two ways:\n\n- a piece of text information that is generated by the agent\n\n- structured information about entities extracted by the agent in the shape of `(subject, predicate, object)` knowledge triples.\n\nThis information can later be read or queried semantically to provide personalized context when your bot is responding to a particular user.\n\nThe KEY idea is that by saving memories, the agent persists information about users that is SHARED across multiple conversations (threads), which is different from memory of a single conversation that is already enabled by LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/).", "type": "Document"}, {"metadata": {"language": "en", "description": "As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into their LangChain application.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/migrating_memory/", "title": "How to migrate to LangGraph memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/migrating_memory/", "uuid": "8160aa02-30c5-55ff-aafb-a101dc4172d7"}, "page_content": "How to migrate to LangGraph memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to migrate to LangGraph memory\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into their LangChain application.\n\n- Users that rely on `RunnableWithMessageHistory` or `BaseChatMessageHistory` do **not** need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.\n\n- Users that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.\n\n## Why use LangGraph for memory?\u200b\n\nThe main advantages of persistence in LangGraph are:\n\n- Built-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.\n\n- Ability to save and resume complex conversations at any point. This helps with: \n- Error recovery\n\n- Allowing human intervention in AI workflows\n\n- Exploring different conversation paths (\"time travel\")\n\n- Full compatibility with both traditional [language models](/docs/concepts/text_llms/) and modern [chat models](/docs/concepts/chat_models/). Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.\n\n- Highly customizable, allowing you to fully control how memory works and use different storage backends.\n\n## Evolution of memory in LangChain\u200b\n\nThe concept of memory has evolved significantly in LangChain since its initial release.\n\n### LangChain 0.0.x memory\u200b\n\nBroadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\n\n| Use Case | Example |\n| ---- | ---- |\n| Managing conversation history | Keep only the lastnturns of the conversation between the user and the AI. |\n| Extraction of structured information | Extract structured information from the conversation history, such as a list of facts learned about the user. |\n| Composite memory implementations | Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation. |\n\nWhile the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\n\nMost of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.\n\n### RunnableWithMessageHistory and BaseChatMessageHistory\u200b\n\nnotePlease see [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history/), if you would like to use `BaseChatMessageHistory` (with or without `RunnableWithMessageHistory`) in LangGraph.\n\nAs of LangChain v0.1, we started recommending that users rely primarily on [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory). `BaseChatMessageHistory` serves\nas a simple persistence for storing and retrieving messages in a conversation.\n\nAt that time, the only option for orchestrating LangChain chains was via [LCEL](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel). To incorporate memory with `LCEL`, users had to use the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory) interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.", "type": "Document"}, {"metadata": {"description": "As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of LangGraph persistence to incorporate memory into their LangChain application.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/migrating_memory/", "title": "How to migrate to LangGraph memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/migrating_memory/", "uuid": "07868afa-3fa0-5b58-bd2f-af3001de9cdd"}, "page_content": "As of LangChain v0.3, we recommend that **new** code takes advantage of LangGraph for both orchestration and persistence:\n\n- Orchestration: In LangGraph, users define [graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/) that specify the flow of the application. This allows users to keep using `LCEL` within individual nodes when `LCEL` is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\n\n- Persistence: Users can rely on LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the `RunnableWithMessageHistory` interface.\n\nimportantIf you have been using `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\n## Migrations\u200b\n\nPrerequisitesThese guides assume some familiarity with the following concepts:\n\n- [LangGraph](https://langchain-ai.github.io/langgraph/)\n\n- [v0.0.x Memory](https://python.langchain.com/v0.1/docs/modules/memory/)\n\n- [How to add persistence (\"memory\") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n\n### 1. Managing conversation history\u200b\n\nThe goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.\n\nOften this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.\n\nMemory classes that fall into this category include:\n\n| Memory Type | How to Migrate | Description |\n| ---- | ---- | ---- |\n| ConversationBufferMemory | Link to Migration Guide | A basic memory implementation that simply stores the conversation history. |\n| ConversationStringBufferMemory | Link to Migration Guide | A special case ofConversationBufferMemorydesigned for LLMs and no longer relevant. |\n| ConversationBufferWindowMemory | Link to Migration Guide | Keeps the lastnturns of the conversation. Drops the oldest turn when the buffer is full. |\n| ConversationTokenBufferMemory | Link to Migration Guide | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n| ConversationSummaryMemory | Link to Migration Guide | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history. |\n| ConversationSummaryBufferMemory | Link to Migration Guide | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |\n| VectorStoreRetrieverMemory | See relatedlong-term memory agent tutorial | Stores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input. |\n\n### 2. Extraction of structured information from the conversation history\u200b\n\nPlease see [long-term memory agent tutorial](/docs/versions/migrating_memory/long_term_memory_agent/) implements an agent that can extract structured information from the conversation history.\n\nMemory classes that fall into this category include:", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "f6b1c7d3-6340-59b9-8f73-645e1465a6cb"}, "page_content": "Memory\n\nMemory\u00b6\nWhat is Memory?\u00b6\nMemory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future. Consider the frustration of working with a colleague who forgets everything you tell them, requiring constant repetition! As AI agents undertake more complex tasks involving numerous user interactions, equipping them with memory becomes equally crucial for efficiency and user satisfaction. With memory, agents can learn from feedback and adapt to users' preferences. This guide covers two types of memory based on recall scope:\nShort-term memory, or thread-scoped memory, can be recalled at any time from within a single conversational thread with a user. LangGraph manages short-term memory as a part of your agent's state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.\nLong-term memory is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories.\nBoth are important to understand and implement for your application.\n\nShort-term memory\u00b6\nShort-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nLangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.\nSince conversation history is the most common form of representing short-term memory, in the next section, we will cover techniques for managing conversation history when the list of messages becomes long. If you want to stick to the high-level concepts, continue on to the long-term memory section.\nManaging long conversation history\u00b6\nLong conversations pose a challenge to today's LLMs. The full history may not even fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM technically supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs.\nManaging short-term memory is an exercise of balancing precision & recall with your application's other performance requirements (latency & cost). As always, it's important to think critically about how you represent information for your LLM and to look at your data. We cover a few common techniques for managing message lists below and hope to provide sufficient context for you to pick the best tradeoffs for your application:\n\nEditing message lists: How to think about trimming and filtering a list of messages before passing to language model.\nSummarizing past conversations: A common technique to use when you don't just want to filter the list of messages.\n\nEditing message lists\u00b6\nChat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/chat_models/", "title": "Chat models | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/chat_models/", "uuid": "e2d53047-308b-5e15-bf79-5086a49e1d3b"}, "page_content": "If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens/) which are the unit of processing that the model uses.\n\n## Advanced topics\u200b\n\n### Rate-limiting\u200b\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\nYou have a few options to deal with rate limits:\n\n1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\n### Caching\u200b\n\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\n\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\n\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\n\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models/) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.\n\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\n\nPlease see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.\n\n## Related resources\u200b\n\n- How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).\n\n- List of supported chat models: [chat model integrations](/docs/integrations/chat/).\n\n### Conceptual guides\u200b\n\n- [Messages](/docs/concepts/messages/)\n\n- [Tool calling](/docs/concepts/tool_calling/)", "type": "Document"}, {"metadata": {"description": "This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/chatbot/", "title": "Build a Chatbot | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/chatbot/", "uuid": "8b976575-4436-56f5-8b68-7631069a255b"}, "page_content": "And now we can see that we get a good response!\n\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\nSo how do we best implement this?\n\n## Message persistence\u200b\n\n[LangGraph](https://langchain-ai.github.io/langgraph/) implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n\nWrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n\nLangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\n\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START, MessagesState, StateGraph\n\n# Define a new graph\nworkflow = StateGraph(state_schema=MessagesState)\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\n# Define the (single) node in the graph\nworkflow.add_edge(START, \"model\")\nworkflow.add_node(\"model\", call_model)\n\n# Add memory\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n```\n\n**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)\nWe now need to create a `config` that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a `thread_id`. This should look like:\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\n```\n\nThis enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n\nWe can then invoke the application:\n\n```python\nquery = \"Hi! I'm Bob.\"\n\ninput_messages = [HumanMessage(query)]\noutput = app.invoke({\"messages\": input_messages}, config)\noutput[\"messages\"][-1].pretty_print()  # output contains all messages in state\n```\n\n```output\n==================================[1m Ai Message [0m==================================\n\nHi Bob! How can I assist you today?\n```\n\n```python\nquery = \"What's my name?\"\n\ninput_messages = [HumanMessage(query)]\noutput = app.invoke({\"messages\": input_messages}, config)\noutput[\"messages\"][-1].pretty_print()\n```\n\n```output\n==================================[1m Ai Message [0m==================================\n\nYour name is Bob! How can I help you today, Bob?\n```\n\nGreat! Our chatbot now remembers things about us. If we change the config to reference a different `thread_id`, we can see that it starts the conversation fresh.\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"abc234\"}}\n\ninput_messages = [HumanMessage(query)]\noutput = app.invoke({\"messages\": input_messages}, config)\noutput[\"messages\"][-1].pretty_print()\n```\n\n```output\n==================================[1m Ai Message [0m==================================\n\nI'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?\n```\n\nHowever, we can always go back to the original conversation (since we are persisting it in a database)\n\n```python\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}\n\ninput_messages = [HumanMessage(query)]\noutput = app.invoke({\"messages\": input_messages}, config)\noutput[\"messages\"][-1].pretty_print()\n```\n\n```output\n==================================[1m Ai Message [0m==================================\n\nYour name is Bob. What would you like to discuss today?\n```\n\nThis is how we can support a chatbot having conversations with many users!", "type": "Document"}, {"metadata": {"description": "This tutorial previously used the RunnableWithMessageHistory abstraction. You can access that version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/chatbot/", "title": "Build a Chatbot | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/chatbot/", "uuid": "4a4b6a70-17a4-587e-9343-1a62fe56e02d"}, "page_content": "```python\nfrom langchain_core.messages import HumanMessage\n\nmodel.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n```\n\n**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)\n```output\nAIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nThe model on its own does not have any concept of state. For example, if you ask a followup question:\n\n```python\nmodel.invoke([HumanMessage(content=\"What's my name?\")])\n```\n\n```output\nAIMessage(content=\"I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nLet's take a look at the example [LangSmith trace](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)\n\nWe can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\nThis makes for a terrible chatbot experience!\n\nTo get around this, we need to pass the entire [conversation history](/docs/concepts/chat_history/) into the model. Let's see what happens when we do that:\n\n```python\nfrom langchain_core.messages import AIMessage\n\nmodel.invoke(\n    [\n        HumanMessage(content=\"Hi! I'm Bob\"),\n        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n        HumanMessage(content=\"What's my name?\"),\n    ]\n)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html)\n```output\nAIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n```\n\nAnd now we can see that we get a good response!\n\nThis is the basic idea underpinning a chatbot's ability to interact conversationally.\nSo how do we best implement this?\n\n## Message persistence\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "- Messages", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/chat_history/", "title": "Chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/chat_history/", "uuid": "70ab0f21-b4b7-507f-afbb-5b640f6d8fef"}, "page_content": "Chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Chat history\n\nPrerequisites- [Messages](/docs/concepts/messages/)\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\nChat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of [messages](/docs/concepts/messages/), each of which is associated with a specific [role](/docs/concepts/messages/#role), such as \"user\", \"assistant\", \"system\", or \"tool\".\n\n## Conversation patterns\u200b\n\n![Conversation patterns](/assets/images/conversation_patterns-0e4c2311b54fae7412f74b1408615432.png)\n\nMost conversations start with a **system message** that sets the context for the conversation. This is followed by a **user message** containing the user's input, and then an **assistant message** containing the model's response.\n\nThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling/) be invoked to perform a specific task.\n\nA full conversation often involves a combination of two patterns of alternating messages:\n\n1. The **user** and the **assistant** representing a back-and-forth conversation.\n\n2. The **assistant** and **tool messages** representing an [\"agentic\" workflow](/docs/concepts/agents/) where the assistant is invoking tools to perform specific tasks.\n\n## Managing chat history\u200b\n\nSince chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models/#context-window).\n\nWhile processing chat history, it's essential to preserve a correct conversation structure.\n\nKey guidelines for managing chat history:\n\n- The conversation should follow one of these structures: \n- The first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\n\n- The last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\n\n- When using [tool calling](/docs/concepts/tool_calling/), a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation.\n\ntipUnderstanding correct conversation structure is essential for being able to properly implement\n[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in chat models.\n\n## Related resources\u200b\n\n- [How to trim messages](/docs/how_to/trim_messages/)\n\n- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).\n\n#### Was this page helpful?\n\n- [Conversation patterns](#conversation-patterns)\n\n- [Managing chat history](#managing-chat-history)\n\n- [Related resources](#related-resources)", "type": "Document"}, {"metadata": {"description": "Retrieval is a common technique chatbots use to augment their responses with data outside a chat model's training data. This section will cover how to implement retrieval in the context of chatbots, but it's worth noting that retrieval is a very subtle and deep topic - we encourage you to explore other parts of the documentation that go into greater depth!", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "title": "How to add retrieval to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_retrieval/", "uuid": "7a95db2d-b52e-548b-b4c8-2d132da762df"}, "page_content": "How to add retrieval to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add retrieval to chatbots\n\n[Retrieval](/docs/concepts/retrieval/) is a common technique chatbots use to augment their responses with data outside a chat model's training data. This section will cover how to implement retrieval in the context of chatbots, but it's worth noting that retrieval is a very subtle and deep topic - we encourage you to explore [other parts of the documentation](/docs/how_to/#qa-with-rag) that go into greater depth!\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install -qU langchain langchain-openai langchain-chroma beautifulsoup4\n\n# Set env var OPENAI_API_KEY or load from a .env file:\nimport dotenv\n\ndotenv.load_dotenv()\n```\n\n```output\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m\n[0mNote: you may need to restart the kernel to use updated packages.\n```\n\n```output\nTrue\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nchat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Creating a retriever\u200b\n\nWe'll use [the LangSmith documentation](https://docs.smith.langchain.com/overview) as source material and store the content in a [vector store](/docs/concepts/vectorstores/) for later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source - you can see more [in-depth documentation on creating retrieval systems here](/docs/how_to/#qa-with-rag).\n\nLet's use a document loader to pull text from the docs:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\ndata = loader.load()\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nNext, we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:\n\n```python\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nall_splits = text_splitter.split_documents(data)\n```\n\n**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\nThen we embed and store those chunks in a vector database:\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\nAnd finally, let's create a retriever from our initialized vectorstore:\n\n```python\n# k is the number of chunks to retrieve\nretriever = vectorstore.as_retriever(k=4)\n\ndocs = retriever.invoke(\"Can LangSmith help test my LLM applications?\")\n\ndocs\n```", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html", "title": "langchain.memory.buffer.ConversationBufferMemory \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5173f477-6c43-51e5-9e88-27d035c4b60c"}, "page_content": "Examples using ConversationBufferMemory\u00b6\n\nBedrock\nBittensor\nChat Over Documents with Vectara\nGradio\nLlama2Chat\nMemorize\nNVIDIA NIMs\nReddit Search \nSAP HANA Cloud Vector Engine\nSceneXplain\nXata\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "029a0b9b-e466-5a14-b60c-934263c7e94d"}, "page_content": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add chat history\n\nnoteThis guide previously used the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) abstraction. You can access this version of the documentation in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\nIn many [Q&A applications](/docs/concepts/rag/) we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n\nIn this guide we focus on **adding logic for incorporating historical messages.**\n\nThis is largely a condensed version of the [Conversational RAG tutorial](/docs/tutorials/qa_chat_history/).\n\nWe will cover two approaches:\n\n1. [Chains](/docs/how_to/qa_chat_history_how_to/#chains), in which we always execute a retrieval step;\n\n2. [Agents](/docs/how_to/qa_chat_history_how_to/#agents), in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\n\nFor the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag/).\n\nBoth approaches leverage [LangGraph](https://langchain-ai.github.io/langgraph/) as an orchestration framework. LangGraph implements a built-in [persistence layer](https://langchain-ai.github.io/langgraph/concepts/persistence/), making it ideal for chat applications that support multiple conversational turns.\n\n## Setup\u200b\n\n### Dependencies\u200b\n\nWe'll use OpenAI embeddings and an InMemory vector store in this walkthrough, but everything shown here works with any [Embeddings](/docs/concepts/embedding_models/), and [VectorStore](/docs/concepts/vectorstores/) or [Retriever](/docs/concepts/retrievers/).\n\nWe'll use the following packages:\n\n```python\n%%capture --no-stderr\n%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4\n```\n\n### LangSmith\u200b\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).\n\nNote that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:\n\n```python\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nif not os.environ.get(\"LANGCHAIN_API_KEY\"):\n    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n```\n\n### Components\u200b\n\nWe will need to select three components from LangChain's suite of integrations.\n\nA [chat model](/docs/integrations/chat/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful."}, {"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a LangChain expert and a world-class researcher, here to assist with any and all questions or issues with LangChain, LangGraph, LangSmith, or any related functionality. Users may come to you with questions or issues.\n\nBased on the conversation below, generate a plan for how you will research the answer to their question.\n\nThe plan should generally not be more than 3 steps long, it can be as short as one. The length of the plan depends on the question.\n\nYou have access to the following documentation sources:\n- Conceptual docs\n- Integration docs\n- How-to guides\n\nYou do not need to specify where you want to research for all steps of the plan, but it's sometimes helpful.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "can you host this using stream lit", "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_1FekkZrJ8WF5PjJDmkNjyQnR", "function": {"arguments": "{\"steps\":[\"1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.\",\"2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.\",\"3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-82264f13-7a64-4719-84b5-5d0a90568ef3", "example": false, "tool_calls": [{"name": "Plan", "args": {"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}, "id": "call_1FekkZrJ8WF5PjJDmkNjyQnR", "type": "tool_call"}], "invalid_tool_calls": []}}
{"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": "delete", "query": "can you host this using stream lit", "num_response_attempts": 0}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": [], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"question": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`."}
{"question": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`."}
{"input": {"question": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_tMj0q4ODYI57MjcYaiZLegRp", "function": {"arguments": "{\"queries\":[\"How to install Streamlit and its dependencies using pip?\",\"What are the required dependencies for Streamlit and Langchain?\",\"Troubleshooting Streamlit installation issues with pip\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-d000d505-35ba-4be0-92c9-e829820d912d", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}, "id": "call_tMj0q4ODYI57MjcYaiZLegRp", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}
{"input": {"question": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"], "documents": []}}
{"input": {"query": "How to install Streamlit and its dependencies using pip?"}}
{"query": "How to install Streamlit and its dependencies using pip?"}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"input": {"query": "What are the required dependencies for Streamlit and Langchain?"}}
{"query": "What are the required dependencies for Streamlit and Langchain?"}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}]}
{"input": {"query": "Troubleshooting Streamlit installation issues with pip"}}
{"query": "Troubleshooting Streamlit installation issues with pip"}
{"documents": [{"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}], "steps": ["2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"question": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time."}
{"question": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time."}
{"input": {"question": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_HuHFbyDMVA4PdK8csP0r9geB", "function": {"arguments": "{\"queries\":[\"Streamlit app chatbot with memory example\",\"how to create a real-time chatbot using Streamlit\",\"Streamlit user interface for chatbot input and output\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-85f1f739-ea85-4846-b65f-2b543ad60ed3", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}, "id": "call_HuHFbyDMVA4PdK8csP0r9geB", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}
{"input": {"question": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"], "documents": []}}
{"input": {"query": "Streamlit app chatbot with memory example"}}
{"query": "Streamlit app chatbot with memory example"}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}]}
{"input": {"query": "how to create a real-time chatbot using Streamlit"}}
{"query": "how to create a real-time chatbot using Streamlit"}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"input": {"query": "Streamlit user interface for chatbot input and output"}}
{"query": "Streamlit user interface for chatbot input and output"}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}], "steps": ["3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": ["3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"question": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."}
{"question": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."}
{"input": {"question": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser.", "queries": [], "documents": []}}
{"input": [{"role": "system", "content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones."}, {"role": "human", "content": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "Generate 3 search queries to search for to answer the user's question.\n\nThese search queries should be diverse in nature - do not generate repetitive ones.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_KrSt4lbkbWos2k2DOLNrOqcp", "function": {"arguments": "{\"queries\":[\"How to run a Streamlit app from the command line\",\"Accessing Streamlit app in a web browser\",\"Common issues when running Streamlit apps\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-39771bab-768d-4e20-a6e1-fc7d4bb4c0b9", "example": false, "tool_calls": [{"name": "Response", "args": {"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}, "id": "call_KrSt4lbkbWos2k2DOLNrOqcp", "type": "tool_call"}], "invalid_tool_calls": []}}
{"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}
{"input": {"question": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser.", "queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"], "documents": []}}
{"input": {"query": "How to run a Streamlit app from the command line"}}
{"query": "How to run a Streamlit app from the command line"}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"input": {"query": "Accessing Streamlit app in a web browser"}}
{"query": "Accessing Streamlit app in a web browser"}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"input": {"query": "Common issues when running Streamlit apps"}}
{"query": "Common issues when running Streamlit apps"}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}], "steps": []}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}], "answer": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "query": "can you host this using stream lit", "num_response_attempts": 0}}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are an expert programmer and problem-solver, tasked with answering any question about LangChain.\n\nGenerate a comprehensive and informative answer for the given question based solely on the provided search results (URL and content). Do NOT ramble, and adjust your response length based on the question. If they ask a question that can be answered in one sentence, do that. If 5 paragraphs of detail is needed, do that. You must only use information from the provided search results. Use an unbiased and journalistic tone. Combine search results together into a coherent answer. Do not repeat text. Cite search results using [${number}] notation. Only cite the most relevant results that answer the question accurately. Place these citations at the end of the individual sentence or paragraph that reference them. Do not put them all at the end, but rather sprinkle them throughout. If different results refer to different entities within the same name, write separate answers for each entity.\n\nYou should use bullet points in your answer for readability. Put citations where they apply rather than putting them all at the end. DO NOT PUT THEM ALL THAT END, PUT THEM IN THE BULLET POINTS.\n\nIf there is nothing in the context relevant to the question at hand, do NOT make up an answer. Rather, tell them why you're unsure and ask for any additional information that may help you answer better.\n\nSometimes, what a user is asking may NOT be possible. Do NOT tell them that things are possible if you don't see evidence for it in the context below. If you don't see based in the information below that something is possible, do NOT say that it is - instead say that you're not sure.\n\nAnything between the following `context` html blocks is retrieved from a knowledge bank, not part of the conversation with the user.\n\n<context>\n    <documents>\n<document  description='Streamlit is a faster way to build and share data apps.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/providers/streamlit/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/providers/streamlit/' uuid='ac804cdf-be7d-5fa9-84dc-0accd6a28710'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)\n</document>\n<document  language='en' description='Streamlit is a faster way to build and share data apps.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/callbacks/streamlit/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/callbacks/streamlit/' uuid='bcc0826d-251d-5752-8793-fe3a3c84e446'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n</document>\n<document  description='Streamlit is an open-source Python library that makes it easy to create and share beautiful,' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' uuid='2ac81d31-09f3-58df-baa2-b1a43cccc13b'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n</document>\n<document  description='Streamlit is an open-source Python library that makes it easy to create and share beautiful,' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' uuid='0a24bd10-dad1-594f-b63b-9978ee1d6605'>\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?\n</document>\n<document  language='en' description='The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of' changefreq='weekly' source='https://python.langchain.com/docs/how_to/installation/' title='How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/installation/' uuid='bd6c4c8f-8776-5114-a21f-27e577ade970'>\nHow to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b\n</document>\n<document  language='en' description='This guide assumes familiarity with the following concepts:' changefreq='weekly' source='https://python.langchain.com/docs/how_to/streaming/' title='How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/streaming/' uuid='58a4554b-2db8-56be-bc91-364df95968d3'>\nHow to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n</document>\n<document  description='A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/chatbots_memory/' title='How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/chatbots_memory/' uuid='3c556051-f083-5312-a927-d17d05ad67e3'>\nHow to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```\n</document>\n</documents>\n<context/>", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "tool_calls": [], "invalid_tool_calls": []}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "can you host this using stream lit", "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc"}}]]}
{"messages": [{"content": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2bb58070-a81f-4883-ae8d-b50fc60faed7", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "num_response_attempts": 1}
{"input": {"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}, {"content": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2bb58070-a81f-4883-ae8d-b50fc60faed7", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "router": {"type": "general", "logic": ""}, "steps": [], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}], "answer": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "query": "can you host this using stream lit", "num_response_attempts": 1}}
{"input": [{"role": "system", "content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts."}, {"role": "human", "content": "Set of facts: \n\n <documents>\n<document  description='Streamlit is a faster way to build and share data apps.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/providers/streamlit/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/providers/streamlit/' uuid='ac804cdf-be7d-5fa9-84dc-0accd6a28710'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)\n</document>\n<document  language='en' description='Streamlit is a faster way to build and share data apps.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/callbacks/streamlit/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/callbacks/streamlit/' uuid='bcc0826d-251d-5752-8793-fe3a3c84e446'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n</document>\n<document  description='Streamlit is an open-source Python library that makes it easy to create and share beautiful,' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' uuid='2ac81d31-09f3-58df-baa2-b1a43cccc13b'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n</document>\n<document  description='Streamlit is an open-source Python library that makes it easy to create and share beautiful,' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' uuid='0a24bd10-dad1-594f-b63b-9978ee1d6605'>\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?\n</document>\n<document  language='en' description='The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of' changefreq='weekly' source='https://python.langchain.com/docs/how_to/installation/' title='How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/installation/' uuid='bd6c4c8f-8776-5114-a21f-27e577ade970'>\nHow to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b\n</document>\n<document  language='en' description='This guide assumes familiarity with the following concepts:' changefreq='weekly' source='https://python.langchain.com/docs/how_to/streaming/' title='How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/streaming/' uuid='58a4554b-2db8-56be-bc91-364df95968d3'>\nHow to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n</document>\n<document  description='A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/chatbots_memory/' title='How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/chatbots_memory/' uuid='3c556051-f083-5312-a927-d17d05ad67e3'>\nHow to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```\n</document>\n</documents> \n\n LLM generation: To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface."}]}
{"messages": [[{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "SystemMessage"], "kwargs": {"content": "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n \n    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.", "type": "system"}}, {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "Set of facts: \n\n <documents>\n<document  description='Streamlit is a faster way to build and share data apps.' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/providers/streamlit/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/providers/streamlit/' uuid='ac804cdf-be7d-5fa9-84dc-0accd6a28710'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)\n</document>\n<document  language='en' description='Streamlit is a faster way to build and share data apps.' changefreq='weekly' source='https://python.langchain.com/docs/integrations/callbacks/streamlit/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/callbacks/streamlit/' uuid='bcc0826d-251d-5752-8793-fe3a3c84e446'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n</document>\n<document  description='Streamlit is an open-source Python library that makes it easy to create and share beautiful,' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' uuid='2ac81d31-09f3-58df-baa2-b1a43cccc13b'>\nStreamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n</document>\n<document  description='Streamlit is an open-source Python library that makes it easy to create and share beautiful,' language='en' changefreq='weekly' source='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' title='Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/' uuid='0a24bd10-dad1-594f-b63b-9978ee1d6605'>\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?\n</document>\n<document  language='en' description='The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of' changefreq='weekly' source='https://python.langchain.com/docs/how_to/installation/' title='How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' content_type=None lastmod=None priority='0.5' loc='https://python.langchain.com/docs/how_to/installation/' uuid='bd6c4c8f-8776-5114-a21f-27e577ade970'>\nHow to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b\n</document>\n<document  language='en' description='This guide assumes familiarity with the following concepts:' changefreq='weekly' source='https://python.langchain.com/docs/how_to/streaming/' title='How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/streaming/' uuid='58a4554b-2db8-56be-bc91-364df95968d3'>\nHow to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n</document>\n<document  description='A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:' language='en' changefreq='weekly' source='https://python.langchain.com/docs/how_to/chatbots_memory/' title='How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain' lastmod=None content_type=None priority='0.5' loc='https://python.langchain.com/docs/how_to/chatbots_memory/' uuid='3c556051-f083-5312-a927-d17d05ad67e3'>\nHow to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```\n</document>\n</documents> \n\n LLM generation: To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "type": "human"}}]]}
{"input": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_lHrzCSVojq39CfYbXkMxRj5e", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-a79b9cc6-1449-4060-b557-82247a2289e8", "example": false, "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_lHrzCSVojq39CfYbXkMxRj5e", "type": "tool_call"}], "invalid_tool_calls": []}}
