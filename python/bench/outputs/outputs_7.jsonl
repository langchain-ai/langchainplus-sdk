{"messages": [{"content": "import os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n# from helper_functions import *\n# from evaluation.evalute_rag import *\n\n# Load environment variables from a .env file\nload_dotenv()\n\n# Set the OpenAI API key environment variable\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(\n    collection_name=\"pdf_documents\", embedding_function=embeddings\n)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\nretriever.add_documents(docs)\n\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \"Include key details if they are important.\"\n    )\n)\n\ngeneration_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n\ndef self_rag(query, retriever, vectorstore, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts)\n    combined_sub_contexts = \" \".join(sub_contexts)\n\n    combined_context = combined_context + \" \" + combined_sub_contexts\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Modify how you invoke the generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Direct model call without structured output\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Modify your generation chain\ngeneration_chain = generation_prompt | llm  # Remove structured output temporarily\n\nmake this into a chatbot with memory", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e7d0470f-b981-4a37-bbc4-cb327bd70ab4", "example": false}, {"content": "To transform your existing code into a chatbot with memory capabilities using LangChain, you can incorporate memory management features that allow the chatbot to remember past interactions. Below is a modified version of your code that includes memory functionality using `ConversationBufferMemory` for short-term memory and `LangGraph` for persistence.\n\n### Step-by-Step Implementation\n\n1. **Install Required Packages**: Ensure you have the necessary packages installed. You may need to install `langchain`, `langchain-groq`, and other dependencies if you haven't already.\n\n2. **Set Up Memory**: Use `ConversationBufferMemory` to store the conversation history.\n\n3. **Integrate Memory into the Chatbot**: Modify the chatbot to utilize the memory when processing queries.\n\nHere\u2019s how you can implement these changes:\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # Add the parent directory to the path\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    print(f\"\\nProcessing query: {query}\")\n\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    print(f\"Retrieved and combined {len(contexts)} documents.\")\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Example usage\nquery = \"What is the main topic of the document?\"\nresponse = self_rag(query, retriever, vectorstore, memory)\nprint(response)\n```\n\n### Key Changes Made:\n\n- **Memory Initialization**: A `ConversationBufferMemory` instance is created to store the conversation history.\n- **Memory Usage**: The `self_rag` function now saves the current query and context into memory, allowing the chatbot to remember past interactions.\n- **Response Generation**: The response generation process remains the same, but it now benefits from the stored context in memory.\n\n### Additional Considerations:\n\n- **Long-Term Memory**: If you want to implement long-term memory, consider using `LangGraph` for persistence, which allows you to save and retrieve memories across different sessions.\n- **User Interaction**: You may want to implement a loop to continuously accept user queries and respond, maintaining the conversation state.\n\nThis setup provides a basic framework for a chatbot with memory capabilities using LangChain. You can further enhance it by adding features like user authentication, more complex memory management, and improved response handling.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-61a65bb2-41b2-49d9-9745-a56b1f5eded9", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "can you host this using stream lit", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "bf785a3c-e7fc-475f-9fb7-f8b3b7ba81dc", "example": false}, {"content": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2bb58070-a81f-4883-ae8d-b50fc60faed7", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "steps": [], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}], "answer": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "query": "can you host this using stream lit", "num_response_attempts": 1}
{"messages": [{"role": "user", "content": "can you host this using stream lit"}]}
{"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": "delete", "query": "can you host this using stream lit", "num_response_attempts": 0}
{"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_1FekkZrJ8WF5PjJDmkNjyQnR", "function": {"arguments": "{\"steps\":[\"1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.\",\"2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.\",\"3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-82264f13-7a64-4719-84b5-5d0a90568ef3", "tool_calls": [{"name": "Plan", "args": {"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}, "id": "call_1FekkZrJ8WF5PjJDmkNjyQnR", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"steps": ["1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."], "documents": "delete", "query": "can you host this using stream lit", "num_response_attempts": 0}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}], "steps": ["2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"question": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`.", "queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}]}
{"question": "1. Install Streamlit and required dependencies if not already installed, using pip: `pip install streamlit langchain langchain-groq langchain-huggingface langchain-chroma langchain-community`."}
{"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}
{"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_tMj0q4ODYI57MjcYaiZLegRp", "function": {"arguments": "{\"queries\":[\"How to install Streamlit and its dependencies using pip?\",\"What are the required dependencies for Streamlit and Langchain?\",\"Troubleshooting Streamlit installation issues with pip\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-d000d505-35ba-4be0-92c9-e829820d912d", "tool_calls": [{"name": "Response", "args": {"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}, "id": "call_tMj0q4ODYI57MjcYaiZLegRp", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}
{"queries": ["How to install Streamlit and its dependencies using pip?", "What are the required dependencies for Streamlit and Langchain?", "Troubleshooting Streamlit installation issues with pip"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='How to install Streamlit and its dependencies using pip?'))", "Send(node='retrieve_documents', arg=QueryState(query='What are the required dependencies for Streamlit and Langchain?'))", "Send(node='retrieve_documents', arg=QueryState(query='Troubleshooting Streamlit installation issues with pip'))"]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"language": "en", "description": "The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/installation/", "title": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/installation/", "uuid": "bd6c4c8f-8776-5114-a21f-27e577ade970"}, "page_content": "How to install LangChain packages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to install LangChain packages\n\nThe LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\nfunctionality to install.\n\n## Official release\u200b\n\nTo install the main `langchain` package, run:\n\n \nPip\n```bash\npip install langchain\n```\n\nConda\n```bash\nconda install langchain -c conda-forge\n```\n\nWhile this package acts as a sane starting point to using LangChain,\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\n\n## Ecosystem packages\u200b\n\nWith the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\nA directed arrow indicates that the source package depends on the target package:\n\n![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)\n\nWhen installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`).\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\n\n### LangChain core\u200b\n\nThe `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:\n\n```bash\npip install langchain-core\n```\n\n### Integration packages\u200b\n\nCertain integrations like OpenAI and Anthropic have their own packages.\nAny integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/).\nYou can see a list of all integration packages in the [API reference](https://api.python.langchain.com) under the \"Partner libs\" dropdown.\nTo install one of these run:\n\n```bash\npip install langchain-openai\n```\n\nAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:\n\n```bash\npip install langchain-community\n```\n\n### LangChain experimental\u200b\n\nThe `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses.\nInstall with:\n\n```bash\npip install langchain-experimental\n```\n\n### LangGraph\u200b\n\n`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\nInstall with:\n\n```bash\npip install langgraph\n```\n\n### LangServe\u200b\n\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\nLangServe is automatically installed by LangChain CLI.\nIf not using LangChain CLI, install with:\n\n```bash\npip install \"langserve[all]\"\n```\n\nfor both client and server dependencies. Or `pip install \"langserve[client]\"` for client code, and `pip install \"langserve[server]\"` for server code.\n\n### LangChain CLI\u200b\n\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\nInstall with:\n\n```bash\npip install langchain-cli\n```\n\n### LangSmith SDK\u200b\n\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\n`langchain-core`, and can be installed and used independently if desired.\nIf you are not using LangChain, you can install it with:\n\n```bash\npip install langsmith\n```\n\n### From source\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/streaming/", "title": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/streaming/", "uuid": "58a4554b-2db8-56be-bc91-364df95968d3"}, "page_content": "How to stream runnables | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to stream runnables\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [LangChain Expression Language](/docs/concepts/lcel/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nImportant LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).\n\nThis interface provides two general approaches to stream content:\n\n1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.\n\n2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.\n\nLet's take a look at both approaches, and try to understand how to use them.\n\ninfoFor a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).\n\n## Using Stream\u200b\n\nAll `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.\n\nThese methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n\nStreaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.\n\nThe complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.\n\nThe best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!\n\n### LLMs and Chat Models\u200b\n\nLarge language models and their chat variants are the primary bottleneck in LLM based apps.\n\nLarge language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.\n\nThe key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.\n\nWe will show examples of streaming using a chat model. Choose one from the options below:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nmodel = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nmodel = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock", "type": "Document"}], "steps": ["2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}], "steps": ["3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"question": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time.", "queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"], "documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"question": "2. Create a Streamlit app script that initializes the chatbot with memory, sets up the user interface for input and output, and handles user queries in real-time."}
{"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}
{"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_HuHFbyDMVA4PdK8csP0r9geB", "function": {"arguments": "{\"queries\":[\"Streamlit app chatbot with memory example\",\"how to create a real-time chatbot using Streamlit\",\"Streamlit user interface for chatbot input and output\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-85f1f739-ea85-4846-b65f-2b543ad60ed3", "tool_calls": [{"name": "Response", "args": {"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}, "id": "call_HuHFbyDMVA4PdK8csP0r9geB", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}
{"queries": ["Streamlit app chatbot with memory example", "how to create a real-time chatbot using Streamlit", "Streamlit user interface for chatbot input and output"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='Streamlit app chatbot with memory example'))", "Send(node='retrieve_documents', arg=QueryState(query='how to create a real-time chatbot using Streamlit'))", "Send(node='retrieve_documents', arg=QueryState(query='Streamlit user interface for chatbot input and output'))"]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/chatbots_memory/", "title": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/chatbots_memory/", "uuid": "3c556051-f083-5312-a927-d17d05ad67e3"}, "page_content": "How to add memory to chatbots | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to add memory to chatbots\n\nA key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:\n\n- Simply stuffing previous messages into a chat model prompt.\n\n- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.\n\n- More complex modifications like synthesizing summaries for long running conversations.\n\nWe'll go into more detail on a few techniques below!\n\nnoteThis how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).\n\nAs of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.\n\nIf your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.\n\nPlease see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.\n\n## Setup\u200b\n\nYou'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:\n\n```python\n%pip install --upgrade --quiet langchain langchain-openai langgraph\n\nimport getpass\nimport os\n\nif not os.environ.get(\"OPENAI_API_KEY\"):\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```output\nOpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n```\n\nLet's also set up a chat model that we'll use for the below examples.\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n## Message passing\u200b\n\nThe simplest form of memory is simply passing chat history messages into a chain. Here's an example:\n\n```python\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)\n\nchain = prompt | model\n\nai_msg = chain.invoke(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"Translate from English to French: I love programming.\"\n            ),\n            AIMessage(content=\"J'adore la programmation.\"),\n            HumanMessage(content=\"What did you just say?\"),\n        ],\n    }\n)\nprint(ai_msg.content)\n```\n\n**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n```output\nI said, \"I love programming\" in French: \"J'adore la programmation.\"\n```", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}], "steps": ["3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}], "steps": []}
{"question": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser.", "queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"], "documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"question": "3. Run the Streamlit app using the command `streamlit run your_script_name.py` and access it in a web browser."}
{"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}
{"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_KrSt4lbkbWos2k2DOLNrOqcp", "function": {"arguments": "{\"queries\":[\"How to run a Streamlit app from the command line\",\"Accessing Streamlit app in a web browser\",\"Common issues when running Streamlit apps\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-39771bab-768d-4e20-a6e1-fc7d4bb4c0b9", "tool_calls": [{"name": "Response", "args": {"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}, "id": "call_KrSt4lbkbWos2k2DOLNrOqcp", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}
{"queries": ["How to run a Streamlit app from the command line", "Accessing Streamlit app in a web browser", "Common issues when running Streamlit apps"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='How to run a Streamlit app from the command line'))", "Send(node='retrieve_documents', arg=QueryState(query='Accessing Streamlit app in a web browser'))", "Send(node='retrieve_documents', arg=QueryState(query='Common issues when running Streamlit apps'))"]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Streamlit is a faster way to build and share data apps.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/streamlit/", "uuid": "ac804cdf-be7d-5fa9-84dc-0accd6a28710"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://streamlit.io/) is a faster way to build and share data apps.\n`Streamlit` turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n## Installation and Setup\u200b\n\nWe need to install the  `streamlit` Python package:\n\n```bash\npip install streamlit\n```\n\n## Memory\u200b\n\nSee a [usage example](/docs/integrations/memory/streamlit_chat_message_history/).\n\n```python\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n## Callbacks\u200b\n\nSee a [usage example](/docs/integrations/callbacks/streamlit/).\n\n```python\nfrom langchain_community.callbacks import StreamlitCallbackHandler\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)\n\n- [Memory](#memory)\n\n- [Callbacks](#callbacks)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is a faster way to build and share data apps.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/callbacks/streamlit/", "uuid": "bcc0826d-251d-5752-8793-fe3a3c84e446"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n**Streamlit is a faster way to build and share data apps.**\nStreamlit turns data scripts into shareable web apps in minutes. All in pure Python. No front\u2011end experience required.\nSee more examples at [streamlit.io/generative-ai](https://streamlit.io/generative-ai).\n\n[](https://codespaces.new/langchain-ai/streamlit-agent?quickstart=1)\n\nIn this guide we will demonstrate how to use `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an\ninteractive Streamlit app. Try it out with the running app below using the MRKL agent:\n\n## Installation and Setup\u200b\n\n```bash\npip install langchain streamlit\n```\n\nYou can run `streamlit hello` to load a sample app and validate your install succeeded. See full instructions in Streamlit's\n[Getting started documentation](https://docs.streamlit.io/library/get-started).\n\n## Display thoughts and actions\u200b\n\nTo create a `StreamlitCallbackHandler`, you just need to provide a parent container to render the output.\n\n```python\nfrom langchain_community.callbacks.streamlit import (\n    StreamlitCallbackHandler,\n)\nimport streamlit as st\n\nst_callback = StreamlitCallbackHandler(st.container())\n```\n\n**API Reference:**[StreamlitCallbackHandler](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.streamlit.StreamlitCallbackHandler.html)\nAdditional keyword arguments to customize the display behavior are described in the\n[API reference](https://python.langchain.com/api_reference/langchain/callbacks/langchain.callbacks.streamlit.streamlit_callback_handler.StreamlitCallbackHandler.html).\n\n### Scenario 1: Using an Agent with Tools\u200b\n\nThe primary supported use case today is visualizing the actions of an Agent with Tools (or Agent Executor). You can create an\nagent in your Streamlit app and simply pass the `StreamlitCallbackHandler` to `agent.run()` in order to visualize the\nthoughts and actions live in your app.\n\n```python\nimport streamlit as st\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent, load_tools\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0, streaming=True)\ntools = load_tools([\"ddg-search\"])\nprompt = hub.pull(\"hwchase17/react\")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nif prompt := st.chat_input():\n    st.chat_message(\"user\").write(prompt)\n    with st.chat_message(\"assistant\"):\n        st_callback = StreamlitCallbackHandler(st.container())\n        response = agent_executor.invoke(\n            {\"input\": prompt}, {\"callbacks\": [st_callback]}\n        )\n        st.write(response[\"output\"])\n```\n\n**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create_react_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html) | [load_tools](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.load_tools.load_tools.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n**Note:** You will need to set `OPENAI_API_KEY` for the above app code to run successfully.\nThe easiest way to do this is via [Streamlit secrets.toml](https://docs.streamlit.io/library/advanced-features/secrets-management),\nor any other local ENV management tool.\n\n### Additional scenarios\u200b\n\nCurrently `StreamlitCallbackHandler` is geared towards use with a LangChain Agent Executor. Support for additional agent types,\nuse directly with Chains, etc will be added in the future.\n\nYou may also be interested in using\n[StreamlitChatMessageHistory](/docs/integrations/memory/streamlit_chat_message_history/) for LangChain.\n\n#### Was this page helpful?\n\n- [Installation and Setup](#installation-and-setup)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "2ac81d31-09f3-58df-baa2-b1a43cccc13b"}, "page_content": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Streamlit\n\n[Streamlit](https://docs.streamlit.io/) is an open-source Python library that makes it easy to create and share beautiful,\ncustom web apps for machine learning and data science.\n\nThis notebook goes over how to store and use chat message history in a `Streamlit` app. `StreamlitChatMessageHistory` will store messages in\n[Streamlit session state](https://docs.streamlit.io/library/api-reference/session-state)\nat the specified `key=`. The default key is `\"langchain_messages\"`.\n\n- Note, `StreamlitChatMessageHistory` only works when run in a Streamlit app.\n\n- You may also be interested in [StreamlitCallbackHandler](/docs/integrations/callbacks/streamlit/) for LangChain.\n\n- For more on Streamlit check out their\n[getting started documentation](https://docs.streamlit.io/library/get-started).\n\nThe integration lives in the `langchain-community` package, so we need to install that. We also need to install `streamlit`.\n\n```text\npip install -U langchain-community streamlit\n```\n\nYou can see the [full app example running here](https://langchain-st-memory.streamlit.app/), and more examples in\n[github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n\n```python\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\n\nhistory = StreamlitChatMessageHistory(key=\"chat_messages\")\n\nhistory.add_user_message(\"hi!\")\nhistory.add_ai_message(\"whats up?\")\n```\n\n**API Reference:**[StreamlitChatMessageHistory](https://python.langchain.com/api_reference/community/chat_message_histories/langchain_community.chat_message_histories.streamlit.StreamlitChatMessageHistory.html)\n```python\nhistory.messages\n```\n\nWe can easily combine this message history class with [LCEL Runnables](/docs/how_to/message_history/).\n\nThe history will be persisted across re-runs of the Streamlit app within a given user session. A given `StreamlitChatMessageHistory` will NOT be persisted or shared across user sessions.\n\n```python\n# Optionally, specify your own session_state key for storing messages\nmsgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n```\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\n```\n\n**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n```\n\nConversational Streamlit apps will often re-draw each previous chat message on every re-run. This is easy to do by iterating through `StreamlitChatMessageHistory.messages`:\n\n```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)", "type": "Document"}, {"metadata": {"language": "en", "description": "Streamlit is an open-source Python library that makes it easy to create and share beautiful,", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "title": "Streamlit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history/", "uuid": "0a24bd10-dad1-594f-b63b-9978ee1d6605"}, "page_content": "```python\nimport streamlit as st\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n```\n\n**View the final app.**\n\n#### Was this page helpful?", "type": "Document"}], "steps": []}
{"output": "respond"}
{"messages": [{"content": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2bb58070-a81f-4883-ae8d-b50fc60faed7", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "num_response_attempts": 1}
{"generations": [{"text": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2bb58070-a81f-4883-ae8d-b50fc60faed7", "tool_calls": [], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"messages": [{"content": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2bb58070-a81f-4883-ae8d-b50fc60faed7", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To host your chatbot using Streamlit, you can create a simple Streamlit application that allows users to interact with the chatbot through a web interface. Below is a step-by-step guide to set up your chatbot with Streamlit, including the necessary code.\n\n### Step-by-Step Implementation\n\n1. **Install Streamlit**: If you haven't already, install Streamlit using pip:\n\n   ```bash\n   pip install streamlit\n   ```\n\n2. **Create a Streamlit App**: Create a new Python file (e.g., `app.py`) and include the following code to set up the Streamlit interface for your chatbot.\n\n### Streamlit Application Code\n\n```python\nimport os\nimport sys\nfrom dotenv import load_dotenv\nimport streamlit as st\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_groq import ChatGroq\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load environment variables\nload_dotenv()\n\n# Set the API key for Groq\nos.environ[\"GROQ_API_KEY\"] = \"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\"\n\n# Load PDF documents\npdf_path = \"/content/input.pdf\"  # Update this path as needed\nloader = PyPDFLoader(pdf_path)\ndocs = loader.load()\n\n# Set up embeddings and vector store\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nvectorstore = Chroma(collection_name=\"pdf_documents\", embedding_function=embeddings)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# Set up in-memory store and retriever\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter)\nretriever.add_documents(docs)\n\n# Initialize the language model\nllm = ChatGroq(model=\"gemma2-9b-it\")\n\n# Define the response model\nclass GenerationResponse(BaseModel):\n    response: str = Field(description=\"Generated response based on the query and context\")\n\n# Modify your prompt template\ngeneration_prompt = PromptTemplate(\n    input_variables=[\"query\", \"context\"],\n    template=(\n        \"You are an AI assistant analyzing ONLY knowledge files to answer queries.\\n\\n\"\n        \"Knowledge Context:\\n{context}\\n\\n\"\n        \"User Query: {query}\\n\\n\"\n        \"If the answer cannot be found within the given context, respond with:\\n\"\n        \"\\\"Sorry, I didn\u2019t understand your question. Do you want to connect with a live agent?\\\"\\n\"\n        \"Generate a precise, structured, informative response based strictly on the provided context:\"\n        \" Include key details if they are important.\"\n    )\n)\n\n# Initialize memory for the chatbot\nmemory = ConversationBufferMemory()\n\n# Create a generation chain with memory\ngeneration_chain = generation_prompt | llm\n\ndef self_rag(query, retriever, vectorstore, memory, top_k=10):\n    # Retrieve documents\n    docs = retriever.invoke(query)\n    sub_docs = vectorstore.similarity_search(query, k=top_k)\n\n    contexts = [doc.page_content for doc in docs]\n    sub_contexts = [doc.page_content for doc in sub_docs]\n    \n    combined_context = \" \".join(contexts) + \" \" + \" \".join(sub_contexts)\n\n    # Store the current query and context in memory\n    memory.save_context({\"query\": query}, {\"response\": combined_context})\n\n    # Prepare input data for generation\n    input_data = {\"query\": query, \"context\": combined_context}\n\n    # Invoke the generation chain\n    response = generation_chain.invoke(input_data)\n\n    return response\n\n# Streamlit UI\nst.title(\"Chatbot with Memory\")\nst.write(\"Ask your questions below:\")\n\n# User input\nuser_query = st.text_input(\"Your Query:\")\n\nif st.button(\"Submit\"):\n    if user_query:\n        response = self_rag(user_query, retriever, vectorstore, memory)\n        st.write(\"Response:\")\n        st.write(response)\n    else:\n        st.write(\"Please enter a query.\")\n```\n\n### Running the Streamlit App\n\n1. **Save the Code**: Save the above code in a file named `app.py`.\n\n2. **Run the Streamlit App**: Open your terminal, navigate to the directory where `app.py` is located, and run the following command:\n\n   ```bash\n   streamlit run app.py\n   ```\n\n3. **Access the App**: After running the command, Streamlit will provide a local URL (usually `http://localhost:8501`) where you can access your chatbot in a web browser.\n\n### Key Features of the Streamlit App\n\n- **User Input**: Users can enter their queries in a text input field.\n- **Response Display**: The chatbot's response is displayed below the input field after the user submits their query.\n- **Memory Management**: The chatbot remembers previous interactions using `ConversationBufferMemory`.\n\n### Additional Considerations\n\n- **PDF Path**: Ensure that the `pdf_path` variable points to a valid PDF file on your system.\n- **Environment Variables**: Make sure your `.env` file is correctly set up with any necessary API keys.\n- **Deployment**: If you want to deploy your Streamlit app, consider using platforms like Streamlit Sharing, Heroku, or AWS.\n\nThis setup provides a functional chatbot with memory capabilities hosted on Streamlit, allowing users to interact with it through a web interface.", "num_response_attempts": 1}
{"output": "end"}
{"output": {"binary_score": "yes"}}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_lHrzCSVojq39CfYbXkMxRj5e", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-a79b9cc6-1449-4060-b557-82247a2289e8", "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_lHrzCSVojq39CfYbXkMxRj5e", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"output": {"binary_score": "yes"}}
