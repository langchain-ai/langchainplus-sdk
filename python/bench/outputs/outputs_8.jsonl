{"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "steps": [], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "answer": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 1}
{"messages": [{"role": "user", "content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto"}]}
{"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": "delete", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}
{"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_glVSpiYKgEWkKSxgMPA1BHcD", "function": {"arguments": "{\"steps\":[\"Update the `final_action` method to concatenate the API response with the last message from the previous node.\",\"Rename the `final_action` node to `policy_check_action` for better clarity.\",\"Modify the `call_policy_api` method to return the URLs of the policies instead of a string message.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-41963906-a901-41b1-89a3-f865f129e005", "tool_calls": [{"name": "Plan", "args": {"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}, "id": "call_glVSpiYKgEWkKSxgMPA1BHcD", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"steps": ["Update the `final_action` method to concatenate the API response with the last message from the previous node.", "Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."], "documents": "delete", "query": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "num_response_attempts": 0}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}], "steps": ["Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"question": "Update the `final_action` method to concatenate the API response with the last message from the previous node.", "queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"], "documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}]}
{"question": "Update the `final_action` method to concatenate the API response with the last message from the previous node."}
{"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}
{"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_F97r20IRhBO8K5vmjsu1t1mB", "function": {"arguments": "{\"queries\":[\"how to concatenate strings in Python\",\"Python API response handling\",\"best practices for updating methods in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-e1faadbe-c463-4517-a212-d3c72063254c", "tool_calls": [{"name": "Response", "args": {"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}, "id": "call_F97r20IRhBO8K5vmjsu1t1mB", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}
{"queries": ["how to concatenate strings in Python", "Python API response handling", "best practices for updating methods in Python"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to concatenate strings in Python'))", "Send(node='retrieve_documents', arg=QueryState(query='Python API response handling'))", "Send(node='retrieve_documents', arg=QueryState(query='best practices for updating methods in Python'))"]}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "title": "How to recursively split text by characters | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/recursive_text_splitter/", "uuid": "2a474310-58dd-588a-8a83-f69b18560da6"}, "page_content": "- Add ASCII comma \"`,`\", Unicode fullwidth comma \"`\uff0c`\", and Unicode ideographic comma \"`\u3001`\"\n\n```python\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \".\",\n        \",\",\n        \"\\u200b\",  # Zero-width space\n        \"\\uff0c\",  # Fullwidth comma\n        \"\\u3001\",  # Ideographic comma\n        \"\\uff0e\",  # Fullwidth full stop\n        \"\\u3002\",  # Ideographic full stop\n        \"\",\n    ],\n    # Existing args\n)\n```\n\n#### Was this page helpful?\n\n- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)", "type": "Document"}, {"metadata": {"language": "en", "description": "MongoDB is a source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "title": "MongoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/mongodb_chat_message_history/", "uuid": "c3a47ddd-79a9-54e8-b337-8e2921003905"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hi Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob. Is there anything else I can help you with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"language": "en", "description": "Structured Query Language (SQL) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e., data incorporating relations among entities and variables.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "title": "SQL (SQLAlchemy) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/sql_chat_message_history/", "uuid": "58000905-a1e1-50d7-9c14-d8718b31d860"}, "page_content": "```python\nchain_with_history.invoke({\"question\": \"Hi! I'm bob\"}, config=config)\n```\n\n```output\nAIMessage(content='Hello Bob! How can I assist you today?')\n```\n\n```python\nchain_with_history.invoke({\"question\": \"Whats my name\"}, config=config)\n```\n\n```output\nAIMessage(content='Your name is Bob! Is there anything specific you would like assistance with, Bob?')\n```\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Usage](#usage)\n\n- [Chaining](#chaining)", "type": "Document"}, {"metadata": {"description": "ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "title": "ChatWatsonx | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/chat/ibm_watsonx/", "uuid": "be87491d-12a3-5b60-8bb9-2c73dcbd4c0c"}, "page_content": "```python\nchain = prompt | chat\nchain.invoke(\n    {\n        \"input_language\": \"English\",\n        \"output_language\": \"German\",\n        \"input\": \"I love Python\",\n    }\n)\n```\n\n```output\nAIMessage(content='Ich liebe Python.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 28, 'total_tokens': 35}, 'model_name': 'ibm/granite-34b-code-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-fef871190b6047a7a3e68c58b3810c33', usage_metadata={'input_tokens': 28, 'output_tokens': 7, 'total_tokens': 35})\n```\n\n## Streaming the Model output\u200b\n\nYou can stream the model output.\n\n```python\nsystem_message = SystemMessage(\n    content=\"You are a helpful assistant which telling short-info about provided topic.\"\n)\nhuman_message = HumanMessage(content=\"moon\")\n\nfor chunk in chat.stream([system_message, human_message]):\n    print(chunk.content, end=\"\")\n```\n\n```output\nThe Moon is the fifth largest moon in the solar system and the largest relative to its host planet. It is the fifth brightest object in Earth's night sky after the Sun, the stars, the Milky Way, and the Moon itself. It orbits around the Earth at an average distance of 238,855 miles (384,400 kilometers). The Moon's gravity is about one-sixthth of Earth's and thus allows for the formation of tides on Earth. The Moon is thought to have formed around 4.5 billion years ago from debris from a collision between Earth and a Mars-sized body named Theia. The Moon is effectively immutable, with its current characteristics remaining from formation. Aside from Earth, the Moon is the only other natural satellite of Earth. The most widely accepted theory is that it formed from the debris of a collision\n```\n\n## Batch the Model output\u200b\n\nYou can batch the model output.\n\n```python\nmessage_1 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"cat\"),\n]\nmessage_2 = [\n    SystemMessage(\n        content=\"You are a helpful assistant which telling short-info about provided topic.\"\n    ),\n    HumanMessage(content=\"dog\"),\n]\n\nchat.batch([message_1, message_2])\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "f407f6b8-de0b-524d-832b-957b0cfe561d"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[0].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "You are passing too many, too few, or mismatched ToolMessages to a model.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "title": "INVALID_TOOL_RESULTS | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/", "uuid": "af64dba8-1a60-5892-aaf6-95ebcbf323d1"}, "page_content": "1257     self,\n   1258     path: str,\n   (...)\n   1265     stream_cls: type[_StreamT] | None = None,\n   1266 ) -> ResponseT | _StreamT:\n   1267     opts = FinalRequestOptions.construct(\n   1268         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1269     )\n-> 1270     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:947, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    944 else:\n    945     retries_taken = 0\n--> 947 return self._request(\n    948     cast_to=cast_to,\n    949     options=options,\n    950     stream=stream,\n    951     stream_cls=stream_cls,\n    952     retries_taken=retries_taken,\n    953 )\n``````output\nFile ~/langchain/oss-py/docs/.venv/lib/python3.11/site-packages/openai/_base_client.py:1051, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   1048         err.response.read()\n   1050     log.debug(\"Re-raising status error\")\n-> 1051     raise self._make_status_error_from_response(err.response) from None\n   1053 return self._process_response(\n   1054     cast_to=cast_to,\n   1055     options=options,\n   (...)\n   1059     retries_taken=retries_taken,\n   1060 )\n``````output\nBadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[4].role', 'code': None}}\n```", "type": "Document"}, {"metadata": {"description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "By themselves, language models can't take actions - they just output text.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/tutorials/agents/", "title": "Build an Agent | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/tutorials/agents/", "uuid": "7591f837-705f-5c9b-97b3-3856a7187e44"}, "page_content": "```python\nresponse = agent_executor.invoke(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n)\nresponse[\"messages\"]\n```\n\n```output\n[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),\n AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),\n ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1717238703, \\'localtime\\': \\'2024-06-01 3:45\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717237800, \\'last_updated\\': \\'2024-06-01 03:30\\', \\'temp_c\\': 12.0, \\'temp_f\\': 53.6, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Mist\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/143.png\\', \\'code\\': 1030}, \\'wind_mph\\': 5.6, \\'wind_kph\\': 9.0, \\'wind_degree\\': 310, \\'wind_dir\\': \\'NW\\', \\'pressure_mb\\': 1013.0, \\'pressure_in\\': 29.92, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 100, \\'feelslike_c\\': 10.5, \\'feelslike_f\\': 50.8, \\'windchill_c\\': 9.3, \\'windchill_f\\': 48.7, \\'heatindex_c\\': 11.1, \\'heatindex_f\\': 51.9, \\'dewpoint_c\\': 8.8, \\'dewpoint_f\\': 47.8, \\'vis_km\\': 6.4, \\'vis_miles\\': 3.0, \\'uv\\': 1.0, \\'gust_mph\\': 12.5, \\'gust_kph\\': 20.1}}\"}, {\"url\": \"https://www.timeanddate.com/weather/usa/san-francisco/hourly\", \"content\": \"Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\\\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\"}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),\n AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently misty with a temperature of around 53\u00b0F (12\u00b0C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \\n\\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]\n```\n\nWe can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.\n\n## Streaming Messages\u200b\n\nWe've seen how the agent can be called with `.invoke` to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\n\n```python\nfor chunk in agent_executor.stream(\n    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]}\n):\n    print(chunk)\n    print(\"----\")\n```", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/concepts/memory/", "title": "Memory | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/concepts/memory/", "uuid": "a0d5b93f-8557-52fc-b343-cb64956ee4f3"}, "page_content": "# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n\nWriting memories\u00b6\nWhile humans often form long-term memories during sleep, AI agents need a different approach. When and how should agents create new memories? There are at least two primary methods for agents to write memories: \"on the hot path\" and \"in the background\".\n\nWriting memories in the hot path\u00b6\nCreating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.\nHowever, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.\nAs an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.\nWriting memories in the background\u00b6\nCreating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.\nHowever, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.\nSee our memory-service template as an reference implementation.\nComments", "type": "Document"}, {"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}], "steps": ["Rename the `final_action` node to `policy_check_action` for better clarity.", "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}], "steps": ["Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"question": "Rename the `final_action` node to `policy_check_action` for better clarity.", "queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"], "documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}]}
{"question": "Rename the `final_action` node to `policy_check_action` for better clarity."}
{"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}
{"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_VBnRpwF00YrEWl8YDjkE62Jd", "function": {"arguments": "{\"queries\":[\"how to rename a node in a programming context\",\"best practices for naming nodes in code\",\"importance of clear naming conventions in programming\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-102cd220-d41a-4600-b2d7-f4d0bbc8caf3", "tool_calls": [{"name": "Response", "args": {"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}, "id": "call_VBnRpwF00YrEWl8YDjkE62Jd", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}
{"queries": ["how to rename a node in a programming context", "best practices for naming nodes in code", "importance of clear naming conventions in programming"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to rename a node in a programming context'))", "Send(node='retrieve_documents', arg=QueryState(query='best practices for naming nodes in code'))", "Send(node='retrieve_documents', arg=QueryState(query='importance of clear naming conventions in programming'))"]}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}]}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}]}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}]}
{"documents": [{"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "c6f90d44-d75d-5850-8e21-fe4a62bc6f11"}, "page_content": "Examples:\n            ```pycon\n            >>> from langgraph.graph import START, StateGraph\n            ...\n            >>> def my_node(state, config):\n            ...    return {\"x\": state[\"x\"] + 1}\n            ...\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(my_node)  # node name will be 'my_node'\n            >>> builder.add_edge(START, \"my_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n            Customize the name:\n\n            ```pycon\n            >>> builder = StateGraph(dict)\n            >>> builder.add_node(\"my_fair_node\", my_node)\n            >>> builder.add_edge(START, \"my_fair_node\")\n            >>> graph = builder.compile()\n            >>> graph.invoke({\"x\": 1})\n            {'x': 2}\n            ```\n\n        Returns:\n            StateGraph\n        \"\"\"\n        if not isinstance(node, str):\n            action = node\n            if isinstance(action, Runnable):\n                node = action.get_name()\n            else:\n                node = getattr(action, \"__name__\", action.__class__.__name__)\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if node in self.channels:\n            raise ValueError(f\"'{node}' is already being used as a state key\")\n        if self.compiled:\n            logger.warning(\n                \"Adding a node to a graph that has already been compiled. This will \"\n                \"not be reflected in the compiled graph.\"\n            )\n        if not isinstance(node, str):\n            action = node\n            node = cast(str, getattr(action, \"name\", getattr(action, \"__name__\", None)))\n            if node is None:\n                raise ValueError(\n                    \"Node name must be provided if action is not a function\"\n                )\n        if action is None:\n            raise RuntimeError\n        if node in self.nodes:\n            raise ValueError(f\"Node `{node}` already present.\")\n        if node == END or node == START:\n            raise ValueError(f\"Node `{node}` is reserved.\")\n\n        for character in (NS_SEP, NS_END):\n            if character in cast(str, node):\n                raise ValueError(\n                    f\"'{character}' is a reserved character and is not allowed in the node names.\"\n                )\n\n        ends = EMPTY_SEQ\n        try:\n            if (isfunction(action) or ismethod(getattr(action, \"__call__\", None))) and (\n                hints := get_type_hints(getattr(action, \"__call__\"))\n                or get_type_hints(action)\n            ):\n                if input is None:\n                    first_parameter_name = next(\n                        iter(\n                            inspect.signature(\n                                cast(FunctionType, action)\n                            ).parameters.keys()\n                        )\n                    )\n                    if input_hint := hints.get(first_parameter_name):\n                        if isinstance(input_hint, type) and get_type_hints(input_hint):\n                            input = input_hint\n                if (\n                    (rtn := hints.get(\"return\"))\n                    and get_origin(rtn) in (Command, GraphCommand)\n                    and (rargs := get_args(rtn))\n                    and get_origin(rargs[0]) is Literal\n                    and (vals := get_args(rargs[0]))\n                ):\n                    ends = vals\n        except (TypeError, StopIteration):\n            pass\n        if input is not None:\n            self._add_schema(input)\n        self.nodes[cast(str, node)] = StateNodeSpec(\n            coerce_to_runnable(action, name=cast(str, node), trace=False),\n            metadata,\n            input=input or self.schema,\n            retry_policy=retry,\n            ends=ends,\n        )\n        return self", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/graphs/", "title": "Graphs | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/graphs/", "uuid": "6588b9de-33e7-5c51-be6f-0b5238fb6fd9"}, "page_content": "Adds a new node to the state graph.\nWill take the name of the function/runnable as the node name.\nParameters:\n\nnode\n              (Union[str, RunnableLike)])\n          \u2013\n          \nThe function or runnable this node will run.\n\naction\n              (Optional[RunnableLike], default:\n                  None\n)\n          \u2013\n          \nThe action associated with the node. (default: None)\n\nmetadata\n              (Optional[dict[str, Any]], default:\n                  None\n)\n          \u2013\n          \nThe metadata associated with the node. (default: None)\n\ninput\n              (Optional[Type[Any]], default:\n                  None\n)\n          \u2013\n          \nThe input schema for the node. (default: the graph's input schema)\n\nretry\n              (Optional[RetryPolicy], default:\n                  None\n)\n          \u2013\n          \nThe policy for retrying the node. (default: None)\n\nRaises:\n    ValueError: If the key is already being used as a state key.\nExamples:\n>>> from langgraph.graph import START, StateGraph\n...\n>>> def my_node(state, config):\n...    return {\"x\": state[\"x\"] + 1}\n...\n>>> builder = StateGraph(dict)\n>>> builder.add_node(my_node)  # node name will be 'my_node'\n>>> builder.add_edge(START, \"my_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nCustomize the name:\n>>> builder = StateGraph(dict)\n>>> builder.add_node(\"my_fair_node\", my_node)\n>>> builder.add_edge(START, \"my_fair_node\")\n>>> graph = builder.compile()\n>>> graph.invoke({\"x\": 1})\n{'x': 2}\n\nReturns:\n\nSelf\n          \u2013\n          \nStateGraph\n\nSource code in libs/langgraph/langgraph/graph/state.py\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412def add_node(\n    self,\n    node: Union[str, RunnableLike],\n    action: Optional[RunnableLike] = None,\n    *,\n    metadata: Optional[dict[str, Any]] = None,\n    input: Optional[Type[Any]] = None,\n    retry: Optional[RetryPolicy] = None,\n) -> Self:\n    \"\"\"Adds a new node to the state graph.\n\n    Will take the name of the function/runnable as the node name.\n\n    Args:\n        node (Union[str, RunnableLike)]: The function or runnable this node will run.\n        action (Optional[RunnableLike]): The action associated with the node. (default: None)\n        metadata (Optional[dict[str, Any]]): The metadata associated with the node. (default: None)\n        input (Optional[Type[Any]]): The input schema for the node. (default: the graph's input schema)\n        retry (Optional[RetryPolicy]): The policy for retrying the node. (default: None)\n    Raises:\n        ValueError: If the key is already being used as a state key.\n\n    Examples:\n        ```pycon\n        >>> from langgraph.graph import START, StateGraph\n        ...\n        >>> def my_node(state, config):\n        ...    return {\"x\": state[\"x\"] + 1}\n        ...\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(my_node)  # node name will be 'my_node'\n        >>> builder.add_edge(START, \"my_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```\n        Customize the name:\n\n        ```pycon\n        >>> builder = StateGraph(dict)\n        >>> builder.add_node(\"my_fair_node\", my_node)\n        >>> builder.add_edge(START, \"my_fair_node\")\n        >>> graph = builder.compile()\n        >>> graph.invoke({\"x\": 1})\n        {'x': 2}\n        ```", "type": "Document"}, {"metadata": {"language": "en", "description": "langgraph", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/langgraph", "title": "How to evaluate a langgraph graph | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "26ce0504-6c6e-55aa-9514-8d7e3327eac3"}, "page_content": "Running and evaluating individual nodes\u200b\nSometimes you want to evaluate a single node directly to save time and costs. langgraph makes it easy to do this.\nIn this case we can even continue using the evaluators we've been using.\nPythonnode_target = example_to_state | app.nodes[\"agent\"]node_experiment_results = await aevaluate(    node_target,    data=\"weather agent\",    evaluators=[right_tool_from_run],    max_concurrency=4,  # optional    experiment_prefix=\"claude-3.5-model-node\",  # optional)\nRelated\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "Neo4j is a graph database management system developed by Neo4j, Inc.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "title": "Neo4j | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/graphs/neo4j_cypher/", "uuid": "5eb12c5d-6706-58e5-8e62-9c3549e6e7bd"}, "page_content": "- [Ignore specified node and relationship types](#ignore-specified-node-and-relationship-types)\n\n- [Validate generated Cypher statements](#validate-generated-cypher-statements)\n\n- [Provide context from database results as tool/function output](#provide-context-from-database-results-as-toolfunction-output)", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted below.", "changefreq": null, "source": "https://docs.smith.langchain.com/administration/how_to_guides/organization_management/manage_organization_by_api", "title": "Manage your organization using the API | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "cd89709f-0016-5447-9ab3-2e9efb7db0fa"}, "page_content": "Sample Code\u200b\nThe sample code below goes through a few common workflows related to organization management.\nMake sure to make necessary replacements wherever <replace_me> is in the code.", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "4997eea3-74a6-5adb-ae92-6c424b87b63e"}, "page_content": "managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de facto way to organize instructions almost regardless of programming language.  The Functional Programming (FP) community has elevated the venerable \u201cfunction\u201d to almost religious status - but that just might be the final blowoff top, much like the fate of OO before it. I\u2019m not going to waste too much time on FP terminology since the hardcore FP zealots will just strawman all day instead of directly answering the tough questions. Most developers don\u2019t care about FP anyway. Cats just view functions as glorified instruction containers. Yes, functions can still play an important role but not quite as central as you think. Thanks to automation, we can now consider alternate ways to manage piles of instructions without always having to resort to functions.  Functions have obviously served the computing industry well for 70 years, so clearly they work in most situations. However, they are starting to show their age. To glue two functions together, you generally need to rely on a third \u201cwrapper\u201d function that shuffles output from one function to the other. This approach is generally put under the heading of lambda calculus and famously credited for allowing programmers to reuse mountains of code. However, it still has a number of drawbacks that become apparent over time:   Functions do not scale across hardware or programming languages because they are hard-wired - and so you eventually end up with the notorious \u201cmonolith\u201d until you break things up into services. But now your programmers are having to deal with things you normally pay an operating system to handle   Functions place the burden on the caller to manage the parameters needed to call the function. So while you achieve better code reuse, the benefit is largely offset by having to explicitly manage all the parameters   between   functions. This wiring tends to be pretty dumb too - you can\u2019t even query it like an RDBMS - so you have to treat it much like a 1960s era network database with glorified pointers everywhere. Imagine a rat\u2019s nest of cables. Great if you are a C programmer but not so much fun for everyone else since you\u2019ve moved a lot of problems outside the function where it\u2019s harder to diagnose   Functions have a structural reuse limitation: a single entry/exit - you either take all of the instructions or nothing, there is no middle ground. In many situations that\u2019s fine but what if it\u2019s a big function and you just need a few lines \u2026 not the entire thing \u2026 ? You must manually break up the function via \u201crefactoring\u201d   To its credit, FP tries to avoid the use of explicit wrappers by letting you chain instructions directly (almost like mini-scripts) but since these chains are also hardwired, it\u2019s hard to see much advantage. Monoliths are infamous for being mysterious black boxes because functions by themselves give almost no visibility into what\u2019s going on unless you break things up or write print statements to a log. Because functions have math-y roots from FORTRAN, it is tempting to treat code almost like math proofs e.g. prove that code is doing what you think it is doing. In practice, this has mixed success. Certainly compilers, type checkers and program verification tools do this sort of stuff anyway but the average developer does not have a math degree and risks not getting the math right. So in many cases, introducing math either increases overall complexity or just moves the problem. Debugging strange errors in your type system can be really difficult. Here\u2019s the issue: the math folks have been warning programmers for years that computer functions are not really the same as math functions. Yeah they look the same but trying to overly conflate the two eventually leads to trouble. It might be more intellectually honest if we at least called them \u2018procedures\u2019 instead. Sure,", "type": "Document"}, {"metadata": {"description": "Exa is a search engine fully designed for use by LLMs. Search for documents on the internet using natural language queries, then retrieve cleaned HTML content from desired documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/exa_search/", "title": "Exa Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/exa_search/", "uuid": "a9e1bbdd-f06d-50d5-92af-6541b7c06067"}, "page_content": "WTF does that even mean? u eastern europeans r sooo dumb. At this point you may want to stop, shut out the noise of the world and whatever Nancy Pelosi is insider trading this week and read that last statement again.    What if we could reduce or avoid the use of functions entirely?    Clearly, this is not going to be your usual Hacker News pablum article. We\u2019ve already lost most of the west coast audience by now so we can grab a coffee and continue in peace.  Note that you already can write a shell script or SQL script without writing a single function, yet somehow get work done. AI certainly does not need functions. So clearly, computing can survive without functions.   Yes, computing can live without \u201cfunctions\u201d.     But computing cannot live without \u201ccats\u201d.   Cats? First of all, functions - as you know them - are just one flavor of \u201ccat\u201d or \u201ccategory\u201d and a somewhat inferior one at that. Some in Estonia - obviously crazy people - think that cats and categorical crafting - based on the insights of a Russian mathematician named Vladimir Voevodsky - are really the future of computing. A trend toward Minecraft\u2122-style crafting (at the code level) is a vision of what a post Silicon Valley world might look like.  Crafting? What a stupid idea, right? Hence this crazy plan might just work.  Cat are an emerging and almost opposing view of traditional computer science. Cats come from Category Theory (CT), which - at least for computing - studies the transformation of code into a runtime. You can think of CT as Compiler Theory on steroids / drugs. But while classic compiler theory just focuses on compiling your code, CT considers the larger \u201cholistic\u201d problem of building software: from programming language algebras to Developer Experience (DX) to the notorious \u201cmonads\u201d crunching your code to the difficult task of deploying all that crap to hardware in the cloud. CT also considers the data side too, which often gets ignored. The blockchain - a strange mix of code and data - is very much a cat. In that sense, CT is a \u201cconvergence\u201d technology.  Convergence seeks to identify common overlap and resolve redundancies \u2026 which means if you solve one problem you might be able to solve a whole bunch of others at the same time ... including the problem of how to dislodge Big Tech.  Cats address a number of thorny edge cases where traditional computing falls short:   Greater code reuse   Greater ability to reason about what code is doing   Better system integration   Greater interactivity with external systems such as robotics, databases, crypto, APIs etc.   Leverage \u201cconvergence\u201d hardware such as NVRAM   AI automation   One heck of an ambitious list. I should mention that convergence has a stormy past in the industry because our brains like to compartmentalize, not converge - which is like thinking in the opposite direction. Unless you are on some good drugs, the brain usually cannot hop between compartments fast enough to see how things might be connected, much like the parable of the Blind Men and the Elephant. Worse, we are all so deep in our day-to-day tech weeds we no longer see the forest for the trees. So if you are already getting a headache, please bear with me! First of all, let\u2019s define a cat as a runnable chunk of statements or instructions. Common examples of cats are things like scripts or code snippets. They might live within the context of a larger system or separately but they are generally smaller than entire \u201cprograms\u201d. For readability purposes in this paper, I will interchange \u201cinstructions\u201d and \u201ccats\u201d. Unfortunately, having lots of individual instructions or scripts lying about creates a management nightmare. Hence, most of your programming career has probably been focused around things called functions. In the early days of computing, managing ever increasing piles of computer instructions and punchcards became overwhelming for humans. In the 1950s, functions were introduced by FORTRAN and became wildly popular as the de", "type": "Document"}], "steps": ["Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "steps": []}
{"question": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message.", "queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"], "documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}]}
{"question": "Modify the `call_policy_api` method to return the URLs of the policies instead of a string message."}
{"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}
{"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_2XUi37EfO58MXkEUCZ2cnYSS", "function": {"arguments": "{\"queries\":[\"how to modify a method in Python to return a list of URLs\",\"Python return values from a method\",\"best practices for API response formatting in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f7fd7489-b7dc-44a7-869d-dc009f7f6e7a", "tool_calls": [{"name": "Response", "args": {"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}, "id": "call_2XUi37EfO58MXkEUCZ2cnYSS", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}
{"queries": ["how to modify a method in Python to return a list of URLs", "Python return values from a method", "best practices for API response formatting in Python"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to modify a method in Python to return a list of URLs'))", "Send(node='retrieve_documents', arg=QueryState(query='Python return values from a method'))", "Send(node='retrieve_documents', arg=QueryState(query='best practices for API response formatting in Python'))"]}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}]}
{"documents": [{"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}]}
{"documents": [{"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "d4571b3a-c672-52ef-b78a-684ef2f3dbfd"}, "page_content": "## Selenium URL Loader\u200b\n\nThis covers how to load HTML documents from a list of URLs using the `SeleniumURLLoader`.\n\nUsing `Selenium` allows us to load pages that require JavaScript to render.\n\nTo use the `SeleniumURLLoader`, you have to install `selenium` and `unstructured`.\n\n```python\n%pip install --upgrade --quiet selenium unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import SeleniumURLLoader\n\nurls = [\n    \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n    \"https://goo.gl/maps/NDSHwePEyaHMFGwh8\",\n]\n\nloader = SeleniumURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[1]\n```", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "b0c10902-3336-56c0-9659-5012713c7ae0"}, "page_content": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# URL\n\nThis example covers how to load `HTML` documents from a list of `URLs` into the `Document` format that we can use downstream.\n\n## Unstructured URL Loader\u200b\n\nFor the examples below, please install the `unstructured` library and see [this guide](/docs/integrations/providers/unstructured/) for more instructions on setting up Unstructured locally, including setting up required system dependencies:\n\n```python\n%pip install --upgrade --quiet unstructured\n```\n\n```python\nfrom langchain_community.document_loaders import UnstructuredURLLoader\n\nurls = [\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-8-2023\",\n    \"https://www.understandingwar.org/backgrounder/russian-offensive-campaign-assessment-february-9-2023\",\n]\n```\n\n**API Reference:**[UnstructuredURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html)\nPass in ssl_verify=False with headers=headers to get past ssl_verification errors.\n\n```python\nloader = UnstructuredURLLoader(urls=urls)\n\ndata = loader.load()\n\ndata[0]\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "8d30a275-35ff-590f-8728-5ca65c7533d2"}, "page_content": "```python\nprint(docs[0].metadata)\n```\n\n```output\n{'source': 'https://www.espn.com/', 'title': 'ESPN - Serving Sports Fans. Anytime. Anywhere.', 'description': 'Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.', 'language': 'en'}\n```\n\n### Load multiple urls concurrently\u200b\n\nYou can speed up the scraping process by scraping and parsing multiple urls concurrently.\n\nThere are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren't concerned about being a good citizen, or you control the server you are scraping and don't care about load, you can change the `requests_per_second` parameter to increase the max concurrent requests.  Note, while this will speed up the scraping process, but may cause the server to block you.  Be careful!\n\n```python\n%pip install -qU  nest_asyncio\n\n# fixes a bug with asyncio and jupyter\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n```output\nRequirement already satisfied: nest_asyncio in /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages (1.5.6)\n```\n\n```python\nloader = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\nloader.requests_per_second = 1\ndocs = loader.aload()\ndocs\n```", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.BaseSingleActionAgent.html", "title": "langchain.agents.agent.BaseSingleActionAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "aed6d9b6-a459-5078-8121-0814447332ba"}, "page_content": "Returns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None[source]\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict[source]\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/agents/langchain.agents.xml.base.XMLAgent.html", "title": "langchain.agents.xml.base.XMLAgent \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "c2f4279b-41b1-50da-acf0-e85973463c85"}, "page_content": "Returns\nAction specifying what tool to use.\n\nReturn type\nUnion[AgentAction, AgentFinish]\n\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) \u2192 AgentFinish\u00b6\nReturn response when agent has been stopped due to max iterations.\n\nParameters\n\nearly_stopping_method (str) \u2013 Method to use for early stopping.\nintermediate_steps (List[Tuple[AgentAction, str]]) \u2013 Steps the LLM has taken to date,\nalong with observations.\n**kwargs (Any) \u2013 User inputs.\n\nReturns\nAgent finish object.\n\nReturn type\nAgentFinish\n\nRaises\nValueError \u2013 If early_stopping_method is not supported.\n\nsave(file_path: Union[Path, str]) \u2192 None\u00b6\nSave the agent.\n\nParameters\nfile_path (Union[Path, str]) \u2013 Path to file to save the agent to.\n\nReturn type\nNone\n\nExample:\n.. code-block:: python\n\n# If working with agent executor\nagent.agent.save(file_path=\u201dpath/agent.yaml\u201d)\n\ntool_run_logging_kwargs() \u2192 Dict\u00b6\nReturn logging kwargs for tool run.\n\nReturn type\nDict\n\nproperty return_values: List[str]\u00b6\nReturn values of the agent.\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": null, "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html", "title": "langchain.evaluation.regex_match.base.RegexMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "fb0d42c9-b109-55f7-a93e-10f117edb3c3"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"description": null, "language": "en", "changefreq": null, "source": "https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html", "title": "langchain.evaluation.exact_match.base.ExactMatchStringEvaluator \u2014 \ud83e\udd9c\ud83d\udd17 LangChain 0.2.17", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "5e5d05ee-97f8-59da-aab0-eecbd5c33d84"}, "page_content": "Returns\nThe evaluation results containing the score or value.\n\nReturn type\ndict\n\n                                \u00a9 2023, LangChain, Inc.\n                                    .\n                            Last updated\n                                on Dec 02, 2024.", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "title": "How to parse YAML output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_yaml/", "uuid": "a88ab41d-40f7-52a0-b666-77e025c83ee4"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\\n\\n# Examples\\n## Schema\\n\\`\\`\\`\\n{\"title\": \"Players\", \"description\": \"A list of players\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Player\"}, \"definitions\": {\"Player\": {\"title\": \"Player\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Player name\", \"type\": \"string\"}, \"avg\": {\"title\": \"Avg\", \"description\": \"Batting average\", \"type\": \"number\"}}, \"required\": [\"name\", \"avg\"]}}}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\n- name: John Doe\\n  avg: 0.3\\n- name: Jane Maxfield\\n  avg: 1.4\\n\\`\\`\\`\\n\\n## Schema\\n\\`\\`\\`\\n{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}, \"required\": [\"habit\", \"sustainable_alternative\"]}\\n\\`\\`\\`\\n## Well formatted instance\\n\\`\\`\\`\\nhabit: Using disposable water bottles for daily hydration.\\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\\n\\`\\`\\` \\n\\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`\\n\\nMake sure to always enclose the YAML output in triple backticks (\\`\\`\\`). Please do not add anything other than valid YAML output!'\n```\n\nYou can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.\n\n## Next steps\u200b\n\nYou've now learned how to prompt a model to return XML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.\n\n#### Was this page helpful?\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"language": "en", "description": "you.com API is a suite of tools designed to help developers ground the output of LLMs in the most recent, most accurate, most relevant information that may not have been included in their training dataset.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "title": "You.com | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/you-retriever/", "uuid": "70ba3a01-9214-56f1-970b-adcee018ef60"}, "page_content": "```python\n# .results returns parsed results with each snippet in a Document\nresponse = utility.results(query=\"What is the weather in NY\")\n\n# .results should have a Document for each `snippet`\nprint(len(response))\n\nprint(response)\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/output_parser_json/", "title": "How to parse JSON output | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/output_parser_json/", "uuid": "cf859c93-e5e9-5957-83fa-bdde0553bd66"}, "page_content": "```python\nparser.get_format_instructions()\n```\n\n```output\n'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n\\`\\`\\`\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n\\`\\`\\`'\n```\n\n## Streaming\u200b\n\nAs mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:\n\n```python\nfor s in chain.stream({\"query\": joke_query}):\n    print(s)\n```\n\n```output\n{}\n{'setup': ''}\n{'setup': 'Why'}\n{'setup': 'Why couldn'}\n{'setup': \"Why couldn't\"}\n{'setup': \"Why couldn't the\"}\n{'setup': \"Why couldn't the bicycle\"}\n{'setup': \"Why couldn't the bicycle stand\"}\n{'setup': \"Why couldn't the bicycle stand up\"}\n{'setup': \"Why couldn't the bicycle stand up by\"}\n{'setup': \"Why couldn't the bicycle stand up by itself\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\"}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': ''}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired'}\n{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two tired!'}\n```\n\n## Without Pydantic\u200b\n\nYou can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.\n\n```python\njoke_query = \"Tell me a joke.\"\n\nparser = JsonOutputParser()\n\nprompt = PromptTemplate(\n    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n    input_variables=[\"query\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n)\n\nchain = prompt | model | parser\n\nchain.invoke({\"query\": joke_query})\n```\n\n```output\n{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}\n```\n\n## Next steps\u200b\n\nYou've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.\n\n#### Was this page helpful?\n\n- [Streaming](#streaming)\n\n- [Without Pydantic](#without-pydantic)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/structured_outputs/", "title": "Structured outputs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/structured_outputs/", "uuid": "b6e0b467-5080-5317-82c2-11d51204ec9b"}, "page_content": "```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nThe arguments of the tool call are already extracted as a dictionary.\nThis dictionary can be optionally parsed into a Pydantic object, matching our original `ResponseFormatter` schema.\n\n```python\n# Get the tool call arguments\nai_msg.tool_calls[0][\"args\"]\n{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\",\n 'followup_question': 'What is the function of ATP in the cell?'}\n# Parse the dictionary into a pydantic object\npydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n```\n\n### JSON mode\u200b\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`.\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model=\"gpt-4o\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\nai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\nai_msg.content\n'\\n{\\n  \"random_ints\": [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]\\n}'\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nOne important point to flag: the model _still_ returns a string, which needs to be parsed into a JSON object.\nThis can, of course, simply use the `json` library or a JSON output parser if you need more advanced functionality.\nSee this [how-to guide on the JSON output parser](/docs/how_to/output_parser_json/) for more details.\n\n```python\nimport json\njson_object = json.loads(ai_msg.content)\n{'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n## Structured output method\u200b\n\nThere are a few challenges when producing structured output with the above methods:\n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n(2) In addition, the model needs to be instructed to _always_ use the tool when we want to enforce structured output, which is a provider specific setting.\n\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n\nWith these challenges in mind, LangChain provides a helper function (`with_structured_output()`) to streamline the process.\n\n![Diagram of with structured output](/assets/images/with_structured_output-4fd0fdc94f644554d52c6a8dee96ea21.png)\n\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\n\n```python\n# Bind the schema to the model\nmodel_with_structure = model.with_structured_output(ResponseFormatter)\n# Invoke the model\nstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")\n# Get back the pydantic object\nstructured_output\nResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n```\n\nFurther readingFor more details on usage, see our [how-to guide](/docs/how_to/structured_output/#the-with_structured_output-method).\n\n#### Was this page helpful?\n\n- [Overview](#overview)", "type": "Document"}], "steps": []}
{"output": "respond"}
{"messages": [{"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "num_response_attempts": 1}
{"generations": [{"text": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "tool_calls": [], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"messages": [{"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "num_response_attempts": 1}
{"output": "end"}
{"output": {"binary_score": "yes"}}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_j3zt8hgpY8XRjRo4eG6GWGT6", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-fd581450-17d8-456d-8095-e0c52becd073", "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_j3zt8hgpY8XRjRo4eG6GWGT6", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"output": {"binary_score": "yes"}}
