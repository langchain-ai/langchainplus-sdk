{"messages": [{"content": "How do I use a RecursiveUrlLoader to load content from a page?", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "6f9448c7-d940-46e0-b5f2-d08f53aca085", "example": false}, {"content": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c009b756-1f8f-4fad-b2aa-cebae7aab05f", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "steps": [], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "answer": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 1}
{"messages": [{"role": "user", "content": "How do I use a RecursiveUrlLoader to load content from a page?"}]}
{"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": "delete", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}
{"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_QVF5RSFXkFAO4UG2GKDlX8bS", "function": {"arguments": "{\"steps\":[\"Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.\",\"Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.\",\"Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-521dde06-84f7-4bbf-ae8a-dfb4af6a538b", "tool_calls": [{"name": "Plan", "args": {"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}, "id": "call_QVF5RSFXkFAO4UG2GKDlX8bS", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"steps": ["Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."], "documents": "delete", "query": "How do I use a RecursiveUrlLoader to load content from a page?", "num_response_attempts": 0}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}], "steps": ["Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"question": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader.", "queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}]}
{"question": "Review the Conceptual docs to understand the purpose and functionality of RecursiveUrlLoader."}
{"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}
{"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_TAc7T2pGmw37TvLwrxfs5SFq", "function": {"arguments": "{\"queries\":[\"RecursiveUrlLoader functionality documentation\",\"RecursiveUrlLoader use cases and examples\",\"Understanding RecursiveUrlLoader in Conceptual docs\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-0bea5009-22ad-4bad-905b-9432ba8246d7", "tool_calls": [{"name": "Response", "args": {"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}, "id": "call_TAc7T2pGmw37TvLwrxfs5SFq", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}
{"queries": ["RecursiveUrlLoader functionality documentation", "RecursiveUrlLoader use cases and examples", "Understanding RecursiveUrlLoader in Conceptual docs"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='RecursiveUrlLoader functionality documentation'))", "Send(node='retrieve_documents', arg=QueryState(query='RecursiveUrlLoader use cases and examples'))", "Send(node='retrieve_documents', arg=QueryState(query='Understanding RecursiveUrlLoader in Conceptual docs'))"]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"language": "en", "description": "Nuclia automatically indexes your unstructured data from any internal and external source, providing optimized search results and generative answers. It can handle video and audio transcription, image content extraction, and document parsing.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/nuclia/", "title": "Nuclia Understanding | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/nuclia/", "uuid": "0b22f469-33c4-5bab-909f-44f40db838a3"}, "page_content": "## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Retrieved information](#retrieved-information)\n\n- [Related](#related)", "type": "Document"}], "steps": ["Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}], "steps": ["Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"question": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page.", "queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}]}
{"question": "Check the How-to guides for specific examples and code snippets on using RecursiveUrlLoader to load content from a page."}
{"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}
{"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_i8alqc3m8ua1MbkaAqL546Ee", "function": {"arguments": "{\"queries\":[\"RecursiveUrlLoader how-to guide\",\"RecursiveUrlLoader code examples\",\"Using RecursiveUrlLoader to load web content\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-315664d7-2800-4856-b20e-3fc1295f5a18", "tool_calls": [{"name": "Response", "args": {"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}, "id": "call_i8alqc3m8ua1MbkaAqL546Ee", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}
{"queries": ["RecursiveUrlLoader how-to guide", "RecursiveUrlLoader code examples", "Using RecursiveUrlLoader to load web content"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='RecursiveUrlLoader how-to guide'))", "Send(node='retrieve_documents', arg=QueryState(query='RecursiveUrlLoader code examples'))", "Send(node='retrieve_documents', arg=QueryState(query='Using RecursiveUrlLoader to load web content'))"]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"language": "en", "description": "Note: This connector-specific loader is deprecated. Please use AirbyteLoader instead.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "title": "Airbyte Gong (Deprecated) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/airbyte_gong/", "uuid": "303552cf-c14a-5615-b73b-6072a5ac090b"}, "page_content": "- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Installation](#installation)\n\n- [Example](#example)\n\n- [Incremental loads](#incremental-loads)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "title": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/async_html/", "uuid": "5f09bbe8-c4dc-5e2e-91ee-0a8889375ab8"}, "page_content": "AsyncHtml | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# AsyncHtml\n\n`AsyncHtmlLoader` loads raw HTML from a list of URLs concurrently.\n\n```python\nfrom langchain_community.document_loaders import AsyncHtmlLoader\n```\n\n**API Reference:**[AsyncHtmlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.async_html.AsyncHtmlLoader.html)\n```python\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\nloader = AsyncHtmlLoader(urls)\n# If you need to use the proxy to make web requests, for example using http_proxy/https_proxy environmental variables,\n# please set trust_env=True explicitly here as follows:\n# loader = AsyncHtmlLoader(urls, trust_env=True)\n# Otherwise, loader.load() may stuck becuase aiohttp session does not recognize the proxy by default\ndocs = loader.load()\n```\n\n```output\nFetching pages: 100%|############| 2/2 [00:00<00:00,  9.96it/s]\n```\n\n```python\ndocs[0].page_content[1000:2000]\n```\n\n```output\n' news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta property=\"og:image\" content=\"https://a1.espncdn.com/combiner/i?img=%2Fi%2Fespn%2Fespn_logos%2Fespn_red.png\"/>\\n<meta property=\"og:image:width\" content=\"1200\" />\\n<meta property=\"og:image:height\" content=\"630\" />\\n<meta property=\"og:type\" content=\"website\" />\\n<meta name=\"twitter:site\" content=\"espn\" />\\n<meta name=\"twitter:url\" content=\"https://www.espn.com\" />\\n<meta name=\"twitter:title\" content=\"ESPN - Serving Sports Fans. Anytime. Anywhere.\"/>\\n<meta name=\"twitter:description\" content=\"Visit ESPN for live scores, highlights and sports news. Stream exclusive games on ESPN+ and play fantasy sports.\" />\\n<meta name=\"twitter:card\" content=\"summary\">\\n<meta name=\"twitter:app:name:iphone\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:iphone\" content=\"317469184\"/>\\n<meta name=\"twitter:app:name:googleplay\" content=\"ESPN\"/>\\n<meta name=\"twitter:app:id:googleplay\" content=\"com.espn.score_center\"/>\\n<meta name=\"title\" content=\"ESPN - '\n```\n\n```python\ndocs[1].page_content[1000:2000]\n```\n\n```output\n'al\" href=\"https://lilianweng.github.io/posts/2023-06-23-agent/\" />\\n<link crossorigin=\"anonymous\" href=\"/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css\" integrity=\"sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=\" rel=\"preload stylesheet\" as=\"style\">\\n<script defer crossorigin=\"anonymous\" src=\"/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js\" integrity=\"sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI=\"\\n    onload=\"hljs.initHighlightingOnLoad();\"></script>\\n<link rel=\"icon\" href=\"https://lilianweng.github.io/favicon_peach.ico\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"16x16\" href=\"https://lilianweng.github.io/favicon-16x16.png\">\\n<link rel=\"icon\" type=\"image/png\" sizes=\"32x32\" href=\"https://lilianweng.github.io/favicon-32x32.png\">\\n<link rel=\"apple-touch-icon\" href=\"https://lilianweng.github.io/apple-touch-icon.png\">\\n<link rel=\"mask-icon\" href=\"https://lilianweng.github.io/safari-pinned-tab.'\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as IMSDbLoader, AZLyricsLoader, and CollegeConfidentialLoader.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "title": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/web_base/", "uuid": "9e6977c1-8cd3-5e48-a465-a937d97029c2"}, "page_content": "WebBaseLoader | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# WebBaseLoader\n\nThis covers how to use `WebBaseLoader` to load all text from `HTML` webpages into a document format that we can use downstream. For more custom logic for loading webpages look at some child class examples such as `IMSDbLoader`, `AZLyricsLoader`, and `CollegeConfidentialLoader`.\n\nIf you don't want to worry about website crawling, bypassing JS-blocking sites, and data cleaning, consider using `FireCrawlLoader` or the faster option `SpiderLoader`.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n- TODO: Fill in table features.\n\n- TODO: Remove JS support link if not relevant, otherwise ensure link is correct.\n\n- TODO: Make sure API reference links are correct.\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| WebBaseLoader | langchain_community | \u2705 | \u274c | \u274c |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| WebBaseLoader | \u2705 | \u2705 |\n\n## Setup\u200b\n\n### Credentials\u200b\n\n`WebBaseLoader` does not require any credentials.\n\n### Installation\u200b\n\nTo use the `WebBaseLoader` you first need to install the `langchain-community` python package.\n\n```python\n%pip install -qU langchain_community beautifulsoup4\n```\n\n## Initialization\u200b\n\nNow we can instantiate our model object and load documents:\n\n```python\nfrom langchain_community.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader(\"https://www.espn.com/\")\n```\n\n**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)\nTo bypass SSL verification errors during fetching, you can set the \"verify\" option:\n\n`loader.requests_kwargs = {'verify':False}`\n\n### Initialization with multiple pages\u200b\n\nYou can also pass in a list of pages to load from.\n\n```python\nloader_multiple_pages = WebBaseLoader([\"https://www.espn.com/\", \"https://google.com\"])\n```\n\n## Load\u200b\n\n```python\ndocs = loader.load()\n\ndocs[0]\n```", "type": "Document"}], "steps": ["Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "steps": []}
{"question": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader.", "queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"], "documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}]}
{"question": "Look into the Integration docs for any additional configuration or dependencies required for using RecursiveUrlLoader."}
{"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}
{"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_MlxoUaokud5JSHcOyld8pisp", "function": {"arguments": "{\"queries\":[\"RecursiveUrlLoader integration documentation\",\"dependencies for RecursiveUrlLoader\",\"configuration settings for RecursiveUrlLoader usage\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-e0b0e8ca-d3cf-4bde-a514-9e2a66b53aa6", "tool_calls": [{"name": "Response", "args": {"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}, "id": "call_MlxoUaokud5JSHcOyld8pisp", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}
{"queries": ["RecursiveUrlLoader integration documentation", "dependencies for RecursiveUrlLoader", "configuration settings for RecursiveUrlLoader usage"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='RecursiveUrlLoader integration documentation'))", "Send(node='retrieve_documents', arg=QueryState(query='dependencies for RecursiveUrlLoader'))", "Send(node='retrieve_documents', arg=QueryState(query='configuration settings for RecursiveUrlLoader usage'))"]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"language": "en", "description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "6d7b01f7-0f34-5cc2-8e6b-bece0e4d6c46"}, "page_content": "For detailed information on configuring and calling the `RecursiveUrlLoader`, please see the API reference: [https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html).\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Instantiation](#instantiation)\n\n- [Load](#load)\n\n- [Lazy loading](#lazy-loading)\n\n- [Adding an Extractor](#adding-an-extractor)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "84143e77-2eef-5124-804b-be76667edaf2"}, "page_content": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Recursive URL\n\nThe `RecursiveUrlLoader` lets you recursively scrape all child links from a root URL and parse them into Documents.\n\n## Overview\u200b\n\n### Integration details\u200b\n\n| Class | Package | Local | Serializable | JS support |\n| ---- | ---- | ---- | ---- | ---- |\n| RecursiveUrlLoader | langchain_community | \u2705 | \u274c | \u2705 |\n\n### Loader features\u200b\n\n| Source | Document Lazy Loading | Native Async Support |\n| ---- | ---- | ---- |\n| RecursiveUrlLoader | \u2705 | \u274c |\n\n## Setup\u200b\n\n### Credentials\u200b\n\nNo credentials are required to use the `RecursiveUrlLoader`.\n\n### Installation\u200b\n\nThe `RecursiveUrlLoader` lives in the `langchain-community` package. There's no other required packages, though you will get richer default Document metadata if you have ``beautifulsoup4` installed as well.\n\n```python\n%pip install -qU langchain-community beautifulsoup4\n```\n\n## Instantiation\u200b\n\nNow we can instantiate our document loader object and load Documents:\n\n```python\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nloader = RecursiveUrlLoader(\n    \"https://docs.python.org/3.9/\",\n    # max_depth=2,\n    # use_async=False,\n    # extractor=None,\n    # metadata_extractor=None,\n    # exclude_dirs=(),\n    # timeout=10,\n    # check_response_status=True,\n    # continue_on_failure=True,\n    # prevent_outside=True,\n    # base_url=None,\n    # ...\n)\n```\n\n**API Reference:**[RecursiveUrlLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html)\n## Load\u200b\n\nUse `.load()` to synchronously load into memory all Documents, with one\nDocument per visited URL. Starting from the initial URL, we recurse through\nall linked URLs up to the specified max_depth.\n\nLet's run through a basic example of how to use the `RecursiveUrlLoader` on the [Python 3.9 Documentation](https://docs.python.org/3.9/).\n\n```python\ndocs = loader.load()\ndocs[0].metadata\n```\n\n```output\n/Users/bagatur/.pyenv/versions/3.9.1/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/',\n 'content_type': 'text/html',\n 'title': '3.9.19 Documentation',\n 'language': None}\n```\n\nGreat! The first document looks like the root page we started from. Let's look at the metadata of the next document\n\n```python\ndocs[1].metadata\n```\n\n```output\n{'source': 'https://docs.python.org/3.9/using/index.html',\n 'content_type': 'text/html',\n 'title': 'Python Setup and Usage \u2014 Python 3.9.19 documentation',\n 'language': None}\n```\n\nThat url looks like a child of our root page, which is great! Let's move on from metadata to examine the content of one of our documents\n\n```python\nprint(docs[0].page_content[:300])\n```\n\n```output\n\n<!DOCTYPE html>\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n  <head>\n    <meta charset=\"utf-8\" /><title>3.9.19 Documentation</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    \n    <link rel=\"stylesheet\" href=\"_static/pydoctheme.css\" type=\"text/css\" />\n    <link rel=\n```\n\nThat certainly looks like HTML that comes from the url [https://docs.python.org/3.9/](https://docs.python.org/3.9/), which is what we expected. Let's now look at some variations we can make to our basic example that can be helpful in different situations.\n\n## Lazy loading\u200b", "type": "Document"}, {"metadata": {"description": "The RecursiveUrlLoader lets you recursively scrape all child links from a root URL and parse them into Documents.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "title": "Recursive URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/recursive_url/", "uuid": "fb260844-e344-5e15-beb8-d242dae38764"}, "page_content": "## Lazy loading\u200b\n\nIf we're loading a  large number of Documents and our downstream operations can be done over subsets of all loaded Documents, we can lazily load our Documents one at a time to minimize our memory footprint:\n\n```python\npages = []\nfor doc in loader.lazy_load():\n    pages.append(doc)\n    if len(pages) >= 10:\n        # do some paged operation, e.g.\n        # index.upsert(page)\n\n        pages = []\n```\n\n```output\n/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_73962/2110507528.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n```\n\nIn this example we never have more than 10 Documents loaded into memory at a time.\n\n## Adding an Extractor\u200b\n\nBy default the loader sets the raw HTML from each link as the Document page content. To parse this HTML into a more human/LLM-friendly format you can pass in a custom `extractor` method:\n\n```python\nimport re\n\nfrom bs4 import BeautifulSoup\n\ndef bs4_extractor(html: str) -> str:\n    soup = BeautifulSoup(html, \"lxml\")\n    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n\nloader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\", extractor=bs4_extractor)\ndocs = loader.load()\nprint(docs[0].page_content[:200])\n```\n\n```output\n/var/folders/td/vzm913rx77x21csd90g63_7c0000gn/T/ipykernel_10935/1083427287.py:6: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  soup = BeautifulSoup(html, \"lxml\")\n/Users/isaachershenson/.pyenv/versions/3.11.9/lib/python3.11/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n  k = self.parse_starttag(i)\n``````output\n3.9.19 Documentation\n\nDownload\nDownload these documents\nDocs by version\n\nPython 3.13 (in development)\nPython 3.12 (stable)\nPython 3.11 (security-fixes)\nPython 3.10 (security-fixes)\nPython 3.9 (securit\n```\n\nThis looks much nicer!\n\nYou can similarly pass in a `metadata_extractor` to customize how Document metadata is extracted from the HTTP response. See the [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.recursive_url_loader.RecursiveUrlLoader.html) for more on this.\n\n## API reference\u200b\n\nThese examples show just a few of the ways in which you can modify the default `RecursiveUrlLoader`, but there are many more modifications that can be made to best fit your use case. Using the parameters `link_regex` and `exclude_dirs` can help you filter out unwanted URLs, `aload()` and `alazy_load()` can be used for aynchronous loading, and more.", "type": "Document"}, {"metadata": {"description": "Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "title": "PDFPlumber | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfplumber/", "uuid": "1cca1842-3793-59d9-babc-5daf57e13dbf"}, "page_content": "## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Overview](#overview)- [Integration details](#integration-details)\n\n- [Loader features](#loader-features)\n\n- [Setup](#setup)- [Credentials](#credentials)\n\n- [Installation](#installation)\n\n- [Initialization](#initialization)\n\n- [Load](#load)\n\n- [Lazy Load](#lazy-load)\n\n- [API reference](#api-reference)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "This example covers how to load HTML documents from a list of URLs into the Document format that we can use downstream.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/url/", "title": "URL | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/url/", "uuid": "005a63b4-8392-5045-82f2-b81186f74af5"}, "page_content": "**API Reference:**[SeleniumURLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url_selenium.SeleniumURLLoader.html)\n```output", "type": "Document"}, {"metadata": {"description": "Within the settings of the LangSmith playground, you can save your model configuration for later use.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/prompt_engineering/how_to_guides/playground/save_model_configuration", "title": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "b31f2ef0-7681-5425-b4fd-ab095f0ae993"}, "page_content": "Save settings configuration | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith\n\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUse custom TLS certificatesSave settings configurationPromptsTutorialsOptimize a classifierDeployment (LangGraph Cloud)AdministrationSelf-hostingReferencePrompt EngineeringHow-to GuidesPlaygroundSave settings configurationSave settings configuration\nWithin the settings of the LangSmith playground, you can save your model configuration for later use.\nThis helps you quickly apply your frequently-used settings without having to re-enter the details each time.\nTo save the current playground configuration, click on the Save button in the top right corner of settings.\nYou can name the configuration and easily access it later.\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextCreate a promptCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}], "steps": []}
{"output": "respond"}
{"messages": [{"content": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c009b756-1f8f-4fad-b2aa-cebae7aab05f", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "num_response_attempts": 1}
{"generations": [{"text": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c009b756-1f8f-4fad-b2aa-cebae7aab05f", "tool_calls": [], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"messages": [{"content": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c009b756-1f8f-4fad-b2aa-cebae7aab05f", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To use the `RecursiveUrlLoader` in LangChain to load content from a webpage, follow these steps:\n\n1. **Installation**: First, ensure you have the necessary packages installed. You can do this by running:\n   ```bash\n   %pip install -qU langchain-community beautifulsoup4\n   ```\n\n2. **Import the Loader**: Import the `RecursiveUrlLoader` from the `langchain_community.document_loaders` module:\n   ```python\n   from langchain_community.document_loaders import RecursiveUrlLoader\n   ```\n\n3. **Instantiate the Loader**: Create an instance of `RecursiveUrlLoader` by providing the root URL from which you want to start scraping. You can also customize parameters like `max_depth`, `use_async`, and others as needed:\n   ```python\n   loader = RecursiveUrlLoader(\"https://docs.python.org/3.9/\")\n   ```\n\n4. **Load Documents**: Use the `.load()` method to load the documents. This method will recursively scrape all child links from the specified root URL:\n   ```python\n   docs = loader.load()\n   ```\n\n5. **Access the Content**: After loading, you can access the content and metadata of the documents. For example, to print the metadata of the first document:\n   ```python\n   print(docs[0].metadata)\n   ```\n\n6. **Example of Content Access**: You can also print a portion of the content from the loaded documents:\n   ```python\n   print(docs[0].page_content[:300])  # Prints the first 300 characters of the content\n   ```\n\nThis process allows you to scrape and load content from a webpage and its linked pages into a format that can be used for further processing or analysis [1][2].", "num_response_attempts": 1}
{"output": "end"}
{"output": {"binary_score": "yes"}}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_XhVw2v9JmIJOfflcqh7Mi9Nb", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-2ff2fc50-9c13-46a5-8183-3faef04fef22", "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_XhVw2v9JmIJOfflcqh7Mi9Nb", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"output": {"binary_score": "yes"}}
