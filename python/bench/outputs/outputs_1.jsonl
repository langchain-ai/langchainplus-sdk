{"messages": [{"content": "Tengo el siguiente grafo:\n\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: END}\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\nQuiero agregarle que luego de que el modelo acabe de responder en el loop de razonamiento, agregarle un paso al final que haga una acci\u00f3n, en vez de ir a \"END\" que vaya a ese nodo, y luego recien vaya a end", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "b3db5afb-8b5b-44db-844a-ff456bfbd6ab", "example": false}, {"content": "Para agregar un paso adicional al final del ciclo de razonamiento en tu grafo, puedes crear un nuevo nodo que represente la acci\u00f3n que deseas realizar despu\u00e9s de que el modelo haya respondido. Luego, debes modificar la l\u00f3gica de las aristas condicionales para que, en lugar de ir directamente a `END`, vayan a este nuevo nodo de acci\u00f3n y, despu\u00e9s de ejecutar esa acci\u00f3n, se dirijan a `END`.\n\nAqu\u00ed tienes un ejemplo de c\u00f3mo podr\u00edas implementar esto:\n\n1. **Define un nuevo m\u00e9todo para la acci\u00f3n final** que deseas ejecutar despu\u00e9s de que el modelo responda.\n2. **Agrega un nuevo nodo** para esta acci\u00f3n en el grafo.\n3. **Modifica las aristas condicionales** para que apunten al nuevo nodo en lugar de `END`.\n\nAqu\u00ed est\u00e1 el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Aqu\u00ed puedes definir la l\u00f3gica de la acci\u00f3n final que deseas realizar\n        print(\"Ejecutando acci\u00f3n final...\")\n        # Realiza la acci\u00f3n final aqu\u00ed\n        return {'messages': [\"Acci\u00f3n final ejecutada.\"]}\n```\n\n### Resumen de los cambios:\n- Se agreg\u00f3 un nuevo nodo llamado `final_action` que ejecuta la l\u00f3gica deseada despu\u00e9s de que el modelo responde.\n- Se modific\u00f3 la arista condicional para que, si no hay acci\u00f3n, se dirija a `final_action` en lugar de `END`.\n- Se conect\u00f3 `final_action` a `END`, asegurando que el flujo del grafo contin\u00fae correctamente despu\u00e9s de ejecutar la acci\u00f3n final.\n\nCon estos cambios, tu grafo ahora ejecutar\u00e1 una acci\u00f3n adicional antes de finalizar.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-79ae8c39-0d85-4849-88aa-3ad8ead2884e", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "Ahora cambia, para que ese 'final_action' su tarea sea revisar si se ha consultado alguna informaci\u00f3n de polizas  (debera registrarse en el state el numero de poliza o numeros, si es mas de una) dentro del agente, y si es asi, que llame a una api, y concatene al output final del modelo, las polizas consultadas", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ab7120ce-b4c3-41d2-a232-546ca1b24313", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que revise si se ha consultado informaci\u00f3n sobre p\u00f3lizas y, en caso afirmativo, llame a una API para obtener informaci\u00f3n adicional, puedes seguir estos pasos:\n\n1. **Actualizar el estado** para registrar los n\u00fameros de p\u00f3liza consultados.\n2. **Implementar la l\u00f3gica** en `final_action` para verificar si se han consultado p\u00f3lizas.\n3. **Llamar a la API** si se han consultado p\u00f3lizas y concatenar la informaci\u00f3n obtenida al output final del modelo.\n\nAqu\u00ed tienes el c\u00f3digo modificado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"final_action\", self.final_action)  # Nuevo nodo para la acci\u00f3n final\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"final_action\"}  # Cambiar END por final_action\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"final_action\", END)  # Conectar final_action a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def final_action(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            api_response = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            final_message = f\"Informaci\u00f3n de p\u00f3lizas consultadas: {api_response}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Por ejemplo, podr\u00edas usar requests para hacer una llamada a la API\n        # response = requests.get(f\"https://api.example.com/policies?numbers={','.join(policy_numbers)}\")\n        # return response.json()  # O el formato que devuelva la API\n        return f\"Detalles de p\u00f3lizas para: {', '.join(policy_numbers)}\"  # Simulaci\u00f3n de respuesta\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `policy_numbers`** en `AgentState` para almacenar los n\u00fameros de p\u00f3liza consultados.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza si la acci\u00f3n es de consulta de p\u00f3liza.\n- **En el m\u00e9todo `final_action`**, se verifica si hay p\u00f3lizas consultadas. Si es as\u00ed, se llama a una API (simulada en este caso) para obtener informaci\u00f3n sobre las p\u00f3lizas y se concatena al mensaje final.\n- **Se implement\u00f3 el m\u00e9todo `call_policy_api`** para simular la llamada a la API que obtendr\u00eda informaci\u00f3n sobre las p\u00f3lizas.\n\nCon estos cambios, el agente ahora puede registrar y consultar informaci\u00f3n sobre p\u00f3lizas, y el resultado final incluir\u00e1 detalles sobre las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-5866d7f0-ef7e-4ce1-a53d-3a9ef0cc4135", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "pero quiero, que el mensaje generado en 'final_Action' se conctane al resultado que se tenia en el nodo anterior, como ultimo mensaje de modelo. ademas ponle un mejor nombre en ingles a ese nodo, y que la APi de polizas, lo que te devolvera es la URL de las polizas, para abrir el adjunto", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "09a1a749-83b6-4b6f-900b-abb114281527", "example": false}, {"content": "Para modificar el nodo `final_action` de manera que el mensaje generado se concatene al resultado del nodo anterior y para renombrar el nodo a un nombre m\u00e1s apropiado en ingl\u00e9s, puedes seguir estos pasos:\n\n1. **Renombrar el nodo** de `final_action` a `fetch_policy_info`.\n2. **Concatenar el mensaje** generado en `fetch_policy_info` al \u00faltimo mensaje del modelo que se obtuvo del nodo anterior.\n3. **Modificar la l\u00f3gica de la API** para que devuelva una URL de las p\u00f3lizas.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\n\nclass AgentState(TypedDict):\n    messages: Annotated[list[AnyMessage], operator.add]\n    policy_numbers: list[str]  # Agregar un campo para almacenar n\u00fameros de p\u00f3liza\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Renombrado el nodo** de `final_action` a `fetch_policy_info` para reflejar mejor su prop\u00f3sito.\n- **Concatenaci\u00f3n del mensaje**: El mensaje final ahora se construye tomando el \u00faltimo mensaje del modelo y a\u00f1adiendo la informaci\u00f3n de las p\u00f3lizas consultadas.\n- **Modificaci\u00f3n de la API**: La funci\u00f3n `call_policy_api` ahora simula la devoluci\u00f3n de una lista de URLs para las p\u00f3lizas consultadas.\n\nCon estos cambios, el agente ahora puede consultar informaci\u00f3n sobre p\u00f3lizas y devolver un mensaje final que incluye tanto la respuesta del modelo como la informaci\u00f3n de las p\u00f3lizas consultadas.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-1c396baf-77fb-4f92-807a-1a715cf4ca6d", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "En el state quiero tener guardao lo siguiente, el documento de identidad del cliente, que puede ser estos:\n\ntipoDocumento: str = Field(description=(\"Type of document, it should be only one of the values in this list \" \n                                            \"[R, D, P, CE, CI] where R=RUC, D=DNI, P=Pasaporte, CE=Carnet de extranjeria, \"\n                                            \"CI=Carnet de identidad. This value is only going to be filled when the 'ramo' is either 'VIDA' \"\n                                            \"and 'SALUD'\"))\n\ny tambien quiero tener el numero de poliza consultada, o numero de todas las polizas, si no se especifico de na", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "e80883cf-130a-4bac-b1a6-4e1ef2be350e", "example": false}, {"content": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-af441c77-eee6-41ea-b651-70f192e505bf", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "steps": [], "documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}], "answer": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "query": "En el state quiero tener guardao lo siguiente, el documento de identidad del cliente, que puede ser estos:\n\ntipoDocumento: str = Field(description=(\"Type of document, it should be only one of the values in this list \" \n                                            \"[R, D, P, CE, CI] where R=RUC, D=DNI, P=Pasaporte, CE=Carnet de extranjeria, \"\n                                            \"CI=Carnet de identidad. This value is only going to be filled when the 'ramo' is either 'VIDA' \"\n                                            \"and 'SALUD'\"))\n\ny tambien quiero tener el numero de poliza consultada, o numero de todas las polizas, si no se especifico de na", "num_response_attempts": 1}
{"messages": [{"role": "user", "content": "En el state quiero tener guardao lo siguiente, el documento de identidad del cliente, que puede ser estos:\n\ntipoDocumento: str = Field(description=(\"Type of document, it should be only one of the values in this list \" \n                                            \"[R, D, P, CE, CI] where R=RUC, D=DNI, P=Pasaporte, CE=Carnet de extranjeria, \"\n                                            \"CI=Carnet de identidad. This value is only going to be filled when the 'ramo' is either 'VIDA' \"\n                                            \"and 'SALUD'\"))\n\ny tambien quiero tener el numero de poliza consultada, o numero de todas las polizas, si no se especifico de na"}]}
{"steps": ["1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).", "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."], "documents": "delete", "query": "En el state quiero tener guardao lo siguiente, el documento de identidad del cliente, que puede ser estos:\n\ntipoDocumento: str = Field(description=(\"Type of document, it should be only one of the values in this list \" \n                                            \"[R, D, P, CE, CI] where R=RUC, D=DNI, P=Pasaporte, CE=Carnet de extranjeria, \"\n                                            \"CI=Carnet de identidad. This value is only going to be filled when the 'ramo' is either 'VIDA' \"\n                                            \"and 'SALUD'\"))\n\ny tambien quiero tener el numero de poliza consultada, o numero de todas las polizas, si no se especifico de na", "num_response_attempts": 0}
{"steps": ["1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).", "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_VFS99htB7uGkIIuEnzYrqROt", "function": {"arguments": "{\"steps\":[\"1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).\",\"2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.\",\"3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-fddd4b10-5fb8-494d-a034-6963155f34c3", "tool_calls": [{"name": "Plan", "args": {"steps": ["1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).", "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}, "id": "call_VFS99htB7uGkIIuEnzYrqROt", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"steps": ["1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).", "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}
{"steps": ["1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).", "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."], "documents": "delete", "query": "En el state quiero tener guardao lo siguiente, el documento de identidad del cliente, que puede ser estos:\n\ntipoDocumento: str = Field(description=(\"Type of document, it should be only one of the values in this list \" \n                                            \"[R, D, P, CE, CI] where R=RUC, D=DNI, P=Pasaporte, CE=Carnet de extranjeria, \"\n                                            \"CI=Carnet de identidad. This value is only going to be filled when the 'ramo' is either 'VIDA' \"\n                                            \"and 'SALUD'\"))\n\ny tambien quiero tener el numero de poliza consultada, o numero de todas las polizas, si no se especifico de na", "num_response_attempts": 0}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}], "steps": ["2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}
{"question": "1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI).", "queries": ["Python TypedDict update example", "Python TypedDict with specific string values", "Defining TypedDict with multiple fields in Python"], "documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}]}
{"question": "1. Update the `AgentState` TypedDict to include fields for `document_type` and `policy_numbers`, ensuring that `document_type` is a string that can only take specific values (R, D, P, CE, CI)."}
{"queries": ["Python TypedDict update example", "Python TypedDict with specific string values", "Defining TypedDict with multiple fields in Python"]}
{"queries": ["Python TypedDict update example", "Python TypedDict with specific string values", "Defining TypedDict with multiple fields in Python"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_TpsMU82HruPHyp0jKgaur1zt", "function": {"arguments": "{\"queries\":[\"Python TypedDict update example\",\"Python TypedDict with specific string values\",\"Defining TypedDict with multiple fields in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-e476ca3a-1571-43eb-99d5-596967dc6af3", "tool_calls": [{"name": "Response", "args": {"queries": ["Python TypedDict update example", "Python TypedDict with specific string values", "Defining TypedDict with multiple fields in Python"]}, "id": "call_TpsMU82HruPHyp0jKgaur1zt", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["Python TypedDict update example", "Python TypedDict with specific string values", "Defining TypedDict with multiple fields in Python"]}
{"queries": ["Python TypedDict update example", "Python TypedDict with specific string values", "Defining TypedDict with multiple fields in Python"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='Python TypedDict update example'))", "Send(node='retrieve_documents', arg=QueryState(query='Python TypedDict with specific string values'))", "Send(node='retrieve_documents', arg=QueryState(query='Defining TypedDict with multiple fields in Python'))"]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "980ad685-84fb-5dbb-9517-6ff0b7ac36a7"}, "page_content": "```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots']},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\n## Injecting arguments at runtime\u200b\n\nIf we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:\n\n```python\nfrom copy import deepcopy\n\nfrom langchain_core.runnables import chain\n\n@chain\ndef inject_user_id(ai_msg):\n    tool_calls = []\n    for tool_call in ai_msg.tool_calls:\n        tool_call_copy = deepcopy(tool_call)\n        tool_call_copy[\"args\"][\"user_id\"] = user_id\n        tool_calls.append(tool_call_copy)\n    return tool_calls\n\ninject_user_id.invoke(ai_msg)\n```\n\n**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)\n```output\n[{'name': 'update_favorite_pets',\n  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},\n  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',\n  'type': 'tool_call'}]\n```\n\nAnd now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:\n\n```python\ntool_map = {tool.name: tool for tool in tools}\n\n@chain\ndef tool_router(tool_call):\n    return tool_map[tool_call[\"name\"]]\n\nchain = llm_with_tools | inject_user_id | tool_router.map()\nchain.invoke(\"my favorite animals are cats and parrots\")\n```\n\n```output\n[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]\n```\n\nLooking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:\n\n```python\nuser_to_pets\n```\n\n```output\n{'123': ['cats', 'parrots']}\n```\n\n## Other ways of annotating args\u200b\n\nHere are a few other ways of annotating our tool args:\n\n```python\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nclass UpdateFavoritePetsSchema(BaseModel):\n    \"\"\"Update list of favorite pets\"\"\"\n\n    pets: List[str] = Field(..., description=\"List of favorite pets to set.\")\n    user_id: Annotated[str, InjectedToolArg] = Field(..., description=\"User's ID.\")\n\n@tool(args_schema=UpdateFavoritePetsSchema)\ndef update_favorite_pets(pets, user_id):\n    user_to_pets[user_id] = pets\n\nupdate_favorite_pets.get_input_schema().schema()\n```\n\n**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nupdate_favorite_pets.tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nfrom typing import Optional, Type\n\nclass UpdateFavoritePets(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema\n\n    def _run(self, pets, user_id):\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets().get_input_schema().schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'description': \"User's ID.\",\n   'title': 'User Id',\n   'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'UpdateFavoritePetsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets().tool_call_schema.schema()\n```", "type": "Document"}, {"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "a2c55608-2304-5afb-ac9e-7816f83c5cf9"}, "page_content": "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\n### Pydantic class\u200b\n\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n\n```python\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field\n\n# Pydantic\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nJoke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n```\n\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n\n### TypedDict or JSON Schema\u200b\n\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is _not_ filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n\nRequirements- Core: `langchain-core>=0.2.26`\n\n- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\n# TypedDict\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n\n    # Alternatively, we could have specified setup as:\n\n    # setup: str                    # no default, no description\n    # setup: Annotated[str, ...]    # no default, no description\n    # setup: Annotated[str, \"foo\"]  # default, no description\n\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/structured_output/", "title": "How to return structured data from a model | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/structured_output/", "uuid": "4f666cb0-c597-5a4f-adb6-4e6bc5432b37"}, "page_content": "punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nstructured_llm = llm.with_structured_output(Joke)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!',\n 'rating': 7}\n```\n\nEquivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n\n```python\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\n            \"type\": \"string\",\n            \"description\": \"The setup of the joke\",\n        },\n        \"punchline\": {\n            \"type\": \"string\",\n            \"description\": \"The punchline to the joke\",\n        },\n        \"rating\": {\n            \"type\": \"integer\",\n            \"description\": \"How funny the joke is, from 1 to 10\",\n            \"default\": None,\n        },\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\nstructured_llm = llm.with_structured_output(json_schema)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'setup': 'Why was the cat sitting on the computer?',\n 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n```\n\n### Choosing between multiple schemas\u200b\n\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.\n\n#### Using Pydantic\u200b\n\n```python\nfrom typing import Union\n\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(\n        default=None, description=\"How funny the joke is, from 1 to 10\"\n    )\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\nFinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\nFinalResponse(final_output=ConversationalResponse(response=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"))\n```\n\n#### Using TypedDict\u200b\n\n```python\nfrom typing import Optional, Union\n\nfrom typing_extensions import Annotated, TypedDict\n\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass ConversationalResponse(TypedDict):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n\n    response: Annotated[str, ..., \"A conversational response to the user's query\"]\n\nclass FinalResponse(TypedDict):\n    final_output: Union[Joke, ConversationalResponse]\n\nstructured_llm = llm.with_structured_output(FinalResponse)\n\nstructured_llm.invoke(\"Tell me a joke about cats\")\n```\n\n```output\n{'final_output': {'setup': 'Why was the cat sitting on the computer?',\n  'punchline': 'Because it wanted to keep an eye on the mouse!',\n  'rating': 7}}\n```\n\n```python\nstructured_llm.invoke(\"How are you today?\")\n```\n\n```output\n{'final_output': {'response': \"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!\"}}\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "title": "SDK (Python) | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/", "uuid": "486a3a21-f8e0-5865-90bb-a91d0e4d2019"}, "page_content": "{\n                                    'title': 'Model Name',\n                                    'enum': ['anthropic', 'openai'],\n                                    'type': 'string'\n                                }\n                        }\n                }\n        }", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_calling/", "title": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_calling/", "uuid": "d09793b6-9ef6-5879-b337-e5d47d5b8383"}, "page_content": "How to use chat models to call tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to use chat models to call tools\n\nPrerequisitesThis guide assumes familiarity with the following concepts:\n\n- [Chat models](/docs/concepts/chat_models/)\n\n- [Tool calling](/docs/concepts/tool_calling/)\n\n- [Tools](/docs/concepts/tools/)\n\n- [Output parsers](/docs/concepts/output_parsers/)\n\n[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by \"calling a tool\".\n\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\n\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).\n\n![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n\nIf you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).\n\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).\n\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\n\n## Defining tool schemas\u200b\n\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.\n\n### Python functions\u200b\n\nOur tool schemas can be Python functions:\n\n```python\n# The function name, type hints, and docstring are all part of the tool\n# schema that's passed to the model. Defining good, descriptive schemas\n# is an extension of prompt engineering and is an important part of\n# getting models to perform well.\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\n\n    Args:\n        a: First integer\n        b: Second integer\n    \"\"\"\n    return a * b\n```\n\n### LangChain Tool\u200b\n\nLangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.\n\n### Pydantic class\u200b\n\nYou can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).\n\nNote that all fields are `required` unless provided a default value.\n\n```python\nfrom pydantic import BaseModel, Field\n\nclass add(BaseModel):\n    \"\"\"Add two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n\nclass multiply(BaseModel):\n    \"\"\"Multiply two integers.\"\"\"\n\n    a: int = Field(..., description=\"First integer\")\n    b: int = Field(..., description=\"Second integer\")\n```\n\n### TypedDict class\u200b\n\nRequires `langchain-core>=0.2.25`\nOr using TypedDicts and annotations:\n\n```python\nfrom typing_extensions import Annotated, TypedDict\n\nclass add(TypedDict):\n    \"\"\"Add two integers.\"\"\"", "type": "Document"}, {"metadata": {"description": "Recall, understand, and extract data from chat histories. Power personalized AI experiences.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "title": "Zep Cloud Memory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/memory/zep_memory_cloud/", "uuid": "be170b97-dd78-5e5e-8114-9554210d8d9d"}, "page_content": "749 def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901\n    750     \"\"\"Main mapping of types to schemas.\n    751 \n    752     The general structure is a series of if statements starting with the simple cases\n--> 753     (non-generic primitive types) and then handling generics and other more complex cases.\n    754 \n    755     Each case either generates a schema directly, calls into a public user-overridable method\n    756     (like `GenerateSchema.tuple_variable_schema`) or calls into a private method that handles some\n    757     boilerplate before calling into the user-facing method (e.g. `GenerateSchema._tuple_schema`).\n    758 \n    759     The idea is that we'll evolve this into adding more and more user facing methods over time\n    760     as they get requested and we figure out what the right API for them is.\n    761     \"\"\"\n    762     if obj is str:\n    763         return self.str_schema()\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in _model_schema(self, cls)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:580, in <dictcomp>(.0)\n    574         inner_schema = new_inner_schema\n    575     inner_schema = apply_model_validators(inner_schema, model_validators, 'inner')\n    577     model_schema = core_schema.model_schema(\n    578         cls,\n    579         inner_schema,\n--> 580         custom_init=getattr(cls, '__pydantic_custom_init__', None),\n    581         root_model=False,\n    582         post_init=getattr(cls, '__pydantic_post_init__', None),\n    583         config=core_config,\n    584         ref=model_ref,\n    585         metadata=metadata,\n    586     )\n    588 schema = self._apply_model_serializers(model_schema, decorators.model_serializers.values())\n    589 schema = apply_model_validators(schema, model_validators, 'outer')\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:916, in _generate_md_field_schema(self, name, field_info, decorators)\n    906     common_field = self._common_field_schema(name, field_info, decorators)\n    907     return core_schema.model_field(\n    908         common_field['schema'],\n    909         serialization_exclude=common_field['serialization_exclude'],\n   (...)\n    913         metadata=common_field['metadata'],\n    914     )\n--> 916 def _generate_dc_field_schema(\n    917     self,\n    918     name: str,\n    919     field_info: FieldInfo,\n    920     decorators: DecoratorInfos,\n    921 ) -> core_schema.DataclassField:\n    922     \"\"\"Prepare a DataclassField to represent the parameter/field, of a dataclass.\"\"\"\n    923     common_field = self._common_field_schema(name, field_info, decorators)\n``````output\nFile ~/job/zep-proprietary/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1114, in _common_field_schema(self, name, field_info, decorators)\n   1108 json_schema_extra = field_info.json_schema_extra\n   1110 metadata = build_metadata_dict(\n   1111     js_annotation_functions=[get_json_schema_update_func(json_schema_updates, json_schema_extra)]\n   1112 )\n-> 1114 alias_generator = self._config_wrapper.alias_generator", "type": "Document"}], "steps": ["2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}], "steps": ["3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}
{"question": "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted.", "queries": ["how to modify a method in Python to capture additional parameters", "best practices for storing multiple values in a dictionary in Python", "Python class method to handle multiple policy numbers efficiently"], "documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}]}
{"question": "2. Modify the `take_action` method to capture the `document_type` when relevant actions are invoked, and ensure that the policy numbers are stored correctly regardless of whether one or multiple policies are consulted."}
{"queries": ["how to modify a method in Python to capture additional parameters", "best practices for storing multiple values in a dictionary in Python", "Python class method to handle multiple policy numbers efficiently"]}
{"queries": ["how to modify a method in Python to capture additional parameters", "best practices for storing multiple values in a dictionary in Python", "Python class method to handle multiple policy numbers efficiently"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_kABh2lely1iudJ8ea1No32nz", "function": {"arguments": "{\"queries\":[\"how to modify a method in Python to capture additional parameters\",\"best practices for storing multiple values in a dictionary in Python\",\"Python class method to handle multiple policy numbers efficiently\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-13365ec4-9acc-4ade-b3a2-43a05ccaec10", "tool_calls": [{"name": "Response", "args": {"queries": ["how to modify a method in Python to capture additional parameters", "best practices for storing multiple values in a dictionary in Python", "Python class method to handle multiple policy numbers efficiently"]}, "id": "call_kABh2lely1iudJ8ea1No32nz", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to modify a method in Python to capture additional parameters", "best practices for storing multiple values in a dictionary in Python", "Python class method to handle multiple policy numbers efficiently"]}
{"queries": ["how to modify a method in Python to capture additional parameters", "best practices for storing multiple values in a dictionary in Python", "Python class method to handle multiple policy numbers efficiently"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to modify a method in Python to capture additional parameters'))", "Send(node='retrieve_documents', arg=QueryState(query='best practices for storing multiple values in a dictionary in Python'))", "Send(node='retrieve_documents', arg=QueryState(query='Python class method to handle multiple policy numbers efficiently'))"]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide assumes familiarity with the following concepts:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/binding/", "title": "How to add default invocation args to a Runnable | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/binding/", "uuid": "c1c190c3-474a-5b99-9513-e378f7dbf27d"}, "page_content": "```python\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n```\n\n```python\nmodel = ChatOpenAI(model=\"gpt-4o-mini\").bind(tools=tools)\nmodel.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n```output\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{\"location\": \"New York, NY\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])\n```\n\n## Next steps\u200b\n\nYou now know how to bind runtime arguments to a Runnable.\n\nTo learn more, see the other how-to guides on runnables in this section, including:\n\n- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime\n\n#### Was this page helpful?\n\n- [Binding stop sequences](#binding-stop-sequences)\n\n- [Attaching OpenAI tools](#attaching-openai-tools)\n\n- [Next steps](#next-steps)", "type": "Document"}, {"metadata": {"description": "There are several ways to log traces to LangSmith.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "<Prerequisites titlesAndLinks={[", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "Clarifai is an AI Platform that provides the full AI lifecycle ranging from data exploration, data labeling, model training, evaluation, and inference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/clarifai/", "title": "Clarifai | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/clarifai/", "uuid": "4e1ba79d-8431-5912-bdcc-c55ea70f6bfa"}, "page_content": "```python\n# Initialize the parameters as dict.\nparams = dict(temperature=str(0.3), max_tokens=100)\n```\n\n```python\nclarifai_llm = Clarifai(user_id=USER_ID, app_id=APP_ID, model_id=MODEL_ID)\nllm_chain = LLMChain(\n    prompt=prompt, llm=clarifai_llm, llm_kwargs={\"inference_params\": params}\n)\n```\n\n```python\nquestion = \"How many 3 digit even numbers you can form that if one of the digits is 5 then the following digit must be 7?\"\n\nllm_chain.run(question)\n```\n\n```output\n'Step 1: The first digit can be any even number from 1 to 9, except for 5. So there are 4 choices for the first digit.\\n\\nStep 2: If the first digit is not 5, then the second digit must be 7. So there is only 1 choice for the second digit.\\n\\nStep 3: The third digit can be any even number from 0 to 9, except for 5 and 7. So there are '\n```\n\nGenerate responses for list of prompts\n\n```python\n# We can use _generate to generate the response for list of prompts.\nclarifai_llm._generate(\n    [\n        \"Help me summarize the events of american revolution in 5 sentences\",\n        \"Explain about rocket science in a funny way\",\n        \"Create a script for welcome speech for the college sports day\",\n    ],\n    inference_params=params,\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Upstash Vector is a serverless vector database designed for working with vector embeddings.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "title": "Upstash Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/upstash/", "uuid": "b7f2a686-8ea2-5e3d-a5c9-49f4a959a0f4"}, "page_content": "```python\ninserted_vectors = store.add_documents(docs)\n\ninserted_vectors[:5]\n```\n\n```output\n['247aa3ae-9be9-43e2-98e4-48f94f920749',\n 'c4dfc886-0a2d-497c-b2b7-d923a5cb3832',\n '0350761d-ca68-414e-b8db-7eca78cb0d18',\n '902fe5eb-8543-486a-bd5f-79858a7a8af1',\n '28875612-c672-4de4-b40a-3b658c72036a']\n```\n\n## Querying\u200b\n\nThe database can be queried using a vector or a text prompt.\nIf a text prompt is used, it's first converted into embedding and then queried.\n\nThe `k` parameter specifies how many results to return from the query.\n\n```python\nresult = store.similarity_search(\"technology\", k=5)\nresult\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "b131754c-6a21-5ba2-a74a-0a3637797152"}, "page_content": "# TODO: Example output.\\r\\n\\r\\n    Delete keys:\\r\\n        ..code-block:: python\\r\\n\\r\\n            kv_store.mdelete([\\\"key1\\\", \\\"key2\\\"])\\r\\n\\r\\n        ..code-block:: python\\r\\n    \\\"\\\"\\\"  # noqa: E501\\r\\n\\`\\`\\`\", \"comments\": \"[]\", \"opened_by\": \"jacoblee93\"}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/reference/store/", "title": "Storage | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/reference/store/", "uuid": "c65e61d3-8874-5626-ba11-fd4e30107172"}, "page_content": "Storage\n\nStorage\u00b6\n\nBase classes and types for persistent key-value stores.\nStores provide long-term memory that persists across threads and conversations.\nSupports hierarchical namespaces, key-value storage, and optional vector search.\nCore types:\n- BaseStore: Store interface with sync/async operations\n- Item: Stored key-value pairs with metadata\n- Op: Get/Put/Search/List operations\n\n NamespacePath = tuple[Union[str, Literal['*']], ...]\n\nmodule-attribute\n\n\u00b6\n\nA tuple representing a namespace path that can include wildcards.\nExamples:\n(\"users\",)  # Exact users namespace\n(\"documents\", \"\")  # Any sub-namespace under documents\n(\"cache\", \"\", \"v1\")  # Any cache category with v1 version\n\n NamespaceMatchType = Literal['prefix', 'suffix']\n\nmodule-attribute\n\n\u00b6\n\nSpecifies how to match namespace paths.\n\nValues\n\"prefix\": Match from the start of the namespace\n\"suffix\": Match from the end of the namespace\n \n\n Item\n\u00b6\n\nRepresents a stored item with metadata.\nParameters:\n\nvalue\n              (dict[str, Any])\n          \u2013\n          \nThe stored data as a dictionary. Keys are filterable.\n\nkey\n              (str)\n          \u2013\n          \nUnique identifier within the namespace.\n\nnamespace\n              (tuple[str, ...])\n          \u2013\n          \nHierarchical path defining the collection in which this document resides.\nRepresented as a tuple of strings, allowing for nested categorization.\nFor example: (\"documents\", 'user123')\n\ncreated_at\n              (datetime)\n          \u2013\n          \nTimestamp of item creation.\n\nupdated_at\n              (datetime)\n          \u2013\n          \nTimestamp of last update.\n\nSource code in libs/checkpoint/langgraph/store/base/__init__.py\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88class Item:\n    \"\"\"Represents a stored item with metadata.\n\n    Args:\n        value (dict[str, Any]): The stored data as a dictionary. Keys are filterable.\n        key (str): Unique identifier within the namespace.\n        namespace (tuple[str, ...]): Hierarchical path defining the collection in which this document resides.\n            Represented as a tuple of strings, allowing for nested categorization.\n            For example: (\"documents\", 'user123')\n        created_at (datetime): Timestamp of item creation.\n        updated_at (datetime): Timestamp of last update.\n    \"\"\"\n\n    __slots__ = (\"value\", \"key\", \"namespace\", \"created_at\", \"updated_at\")\n\n    def __init__(\n        self,\n        *,\n        value: dict[str, Any],\n        key: str,\n        namespace: tuple[str, ...],\n        created_at: datetime,\n        updated_at: datetime,\n    ):\n        self.value = value\n        self.key = key\n        # The casting from json-like types is for if this object is\n        # deserialized.\n        self.namespace = tuple(namespace)\n        self.created_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(created_at, str)\n            else created_at\n        )\n        self.updated_at = (\n            datetime.fromisoformat(cast(str, created_at))\n            if isinstance(updated_at, str)\n            else updated_at\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Item):\n            return False\n        return (\n            self.value == other.value\n            and self.key == other.key\n            and self.namespace == other.namespace\n            and self.created_at == other.created_at\n            and self.updated_at == other.updated_at\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.namespace, self.key))\n\n    def dict(self) -> dict:\n        return {\n            \"value\": self.value,\n            \"key\": self.key,\n            \"namespace\": list(self.namespace),\n            \"created_at\": self.created_at.isoformat(),\n            \"updated_at\": self.updated_at.isoformat(),\n        }\n\n SearchItem\n\u00b6", "type": "Document"}, {"metadata": {"language": "en", "description": "Overview", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/key_value_stores/", "title": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/key_value_stores/", "uuid": "e0107575-760e-5375-a67d-88c46d6b8ca6"}, "page_content": "Key-value stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Key-value stores\n\n## Overview\u200b\n\nLangChain provides a key-value store interface for storing and retrieving data.\n\nLangChain includes a [BaseStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface,\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\nmore specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of\nencoding and decoding data for their specific needs.\n\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\n\n## Usage\u200b\n\nThe key-value store interface in LangChain is used primarily for:\n\n1. \nCaching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.\n\n2. \nAs a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.\n\nPlease see these how-to guides for more information:\n\n- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).\n\n- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).\n\n## Interface\u200b\n\nAll [BaseStores](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:\n\n- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist\n\n- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys\n\n- `mdelete(key: Sequence[str]) -> None`: delete multiple keys\n\n- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix\n\n## Integrations\u200b\n\nPlease reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Usage](#usage)\n\n- [Interface](#interface)\n\n- [Integrations](#integrations)", "type": "Document"}, {"metadata": {"language": "", "description": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "title": "Competitive Programming | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/", "uuid": "ce5a12e2-3bab-5883-bfa7-14be2b535dbb"}, "page_content": "The second line consists of $c_1, c_2, c_3 \\dots c_N$ ($1 \\leq c_i \\leq 10^6$).\n\nThe third line consists of $t_1, t_2, t_3 \\dots t_N$ ($1 \\leq t_i \\leq 10^6$).\n\nThe next $Q$ lines each consist of two integers $V$ ($1 \\leq V \\leq N$) and $S$\n($1 \\leq S \\leq 10^6$).\n\nOUTPUT FORMAT (print output to the terminal / stdout):\nFor each of the $Q$ queries, output YES or NO on a new line.\n\nSAMPLE INPUT:\n5 5\n3 5 7 9 12\n4 2 3 3 8\n1 5\n1 6\n3 3\n4 2\n5 1\nSAMPLE OUTPUT: \nYES\nNO\nYES\nYES\nNO\n\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\nso she will only get to visit farm $4$ on time before FJ closes the farm.\n\nFor the second query, Bessie will not be able to visit any of the farms on time.\n\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\n\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\nfarm on time.\n\nSCORING:\nInputs 2-4: $N,Q\\le 10^3$Inputs 5-9: $c_i, t_i \\le 20$Inputs 10-17: No additional constraints.\n\nProblem credits: Chongtian Ma\n\nPretty difficult! Let's run our simple \"zero-shot\" agent below to see how it fares. It most likely will not be able to solve this question (unless you are using a more powerful model than what I had available at the time of writing this tutorial (2024/04/20).\nWe will trace the trajectory to LangSmith to review the series of submissions. To reduce the packet size, we will use \"hide_inputs\" and filter out the test_cases. All this is optional but useful for development. \nNote: We expect a GraphRecursionError here from it not being able to answer it correctly in the allocated number of steps.\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langsmith import Client\n\n# We don't need to include all the test cases in our traces.\ndef _hide_test_cases(inputs):\n    copied = inputs.copy()\n    # These are tens of MB in size. No need to send them up\n    copied[\"test_cases\"] = \"...\"\n    return copied\n\nclient = Client(hide_inputs=_hide_test_cases, hide_outputs=_hide_test_cases)\nwith tracing_v2_enabled(client=client):\n    events = graph.stream(input_state)\n    for event in events:\n        for value in event.values():\n            messages = value.get(\"messages\")\n            if messages:\n                if isinstance(messages, list):\n                    messages = value[\"messages\"][-1]\n                print(\n                    \"Assistant:\",\n                    str(messages.content).replace(\"\\n\", \"\\\\n\")[:50],\n                )\n\nAssistant: [{'text': '<thinking>\\nThe key steps to solve this\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01KimhKt8aqQjGZJmrHVnAtE', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01CMZTqAd7BZQ2nSgtk9djRW', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01Kbaq9gX4BnHvps6TMfVGHL', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01MiSnpiGK5Yy4Cpp6GGbjmT', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01GWuvJezXLMVurUBG84odDP', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01W8DGmhcpFVctySmx58scf9', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_018bhYtCKDK6S4MHiAxUZCrb', 'input':\nAssistant: KeyError('code')\\nMake all fixes using the writePy\nAssistant: [{'id': 'toolu_01LCwaCjX9uZBV3jt9eAkmAa', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01WqJvdE2WDeTZXoKp2V7PWb', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_01DGevkunt9zWx7SVDCHdBuv', 'input':\nAssistant: Incorrect submission. Please respond with updated \nAssistant: [{'id': 'toolu_013comYKVxNSzTM4ZbH3L3FP', 'input':\nAssistant: Incorrect submission. Please respond with updated", "type": "Document"}, {"metadata": {"language": "en", "description": "In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/mask_inputs_outputs", "title": "Prevent logging of sensitive data in traces | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "288cf4df-4af5-53f1-bc48-05b1354ccd5b"}, "page_content": "You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.\nimport reimport openaifrom langsmith import Clientfrom langsmith.wrappers import wrap_openai# Define regex patterns for various PIISSN_PATTERN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')CREDIT_CARD_PATTERN = re.compile(r'\\b(?:\\d[ -]*?){13,16}\\b')EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')FULL_NAME_PATTERN = re.compile(r'\\b([A-Z][a-z]*\\s[A-Z][a-z]*)\\b')def regex_anonymize(text):    \"\"\"    Anonymize sensitive information in the text using regex patterns.    Args:        text (str): The input text to be anonymized.    Returns:        str: The anonymized text.    \"\"\"    # Replace sensitive information with placeholders    text = SSN_PATTERN.sub('[REDACTED SSN]', text)    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)    return textdef recursive_anonymize(data, depth=10):    \"\"\"    Recursively traverse the data structure and anonymize sensitive information.    Args:        data (any): The input data to be anonymized.        depth (int): The current recursion depth to prevent excessive recursion.    Returns:        any: The anonymized data.    \"\"\"    if depth == 0:        return data    if isinstance(data, dict):        anonymized_dict = {}        for k, v in data.items():            anonymized_value = recursive_anonymize(v, depth - 1)            anonymized_dict[k] = anonymized_value        return anonymized_dict    elif isinstance(data, list):        anonymized_list = []        for item in data:            anonymized_item = recursive_anonymize(item, depth - 1)            anonymized_list.append(anonymized_item)        return anonymized_list    elif isinstance(data, str):        anonymized_data = regex_anonymize(data)        return anonymized_data    else:        return dataopenai_client = wrap_openai(openai.Client())# Initialize the LangSmith client with the anonymization functionslangsmith_client = Client(    hide_inputs=recursive_anonymize, hide_outputs=recursive_anonymize)# The trace produced will have its metadata present, but the inputs and outputs will be anonymizedresponse_with_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],    langsmith_extra={\"client\": langsmith_client},)# The trace produced will not have anonymized inputs and outputsresponse_without_anonymization = openai_client.chat.completions.create(    model=\"gpt-4o-mini\",    messages=[        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},        {\"role\": \"user\", \"content\": \"My name is John Doe, my SSN is 123-45-6789, my credit card number is 4111 1111 1111 1111, my email is john.doe@example.com, and my phone number is (123) 456-7890.\"},    ],)\nThe anonymized run will look like this in LangSmith:", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "title": "Build a Customer Support Bot | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/", "uuid": "4a5ee8dc-7c60-58ad-b929-8ed0575ac12a"}, "page_content": "# Hotel Booking Assistant\nbook_hotel_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling hotel bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'i need to figure out transportation while i'm there'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Hotel booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_hotel_safe_tools = [search_hotels]\nbook_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\nbook_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\nbook_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n    book_hotel_tools + [CompleteOrEscalate]\n)\n\n# Car Rental Assistant\nbook_car_rental_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant for handling car rental bookings. \"\n            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n            \"\\nCurrent time: {time}.\"\n            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n            \" - 'what's the weather like this time of year?'\\n\"\n            \" - 'What flights are available?'\\n\"\n            \" - 'nevermind i think I'll book separately'\\n\"\n            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n            \" - 'Car rental booking confirmed'\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now)\n\nbook_car_rental_safe_tools = [search_car_rentals]\nbook_car_rental_sensitive_tools = [\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n]\nbook_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\nbook_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n    book_car_rental_tools + [CompleteOrEscalate]\n)\n\n# Excursion Assistant", "type": "Document"}], "steps": ["3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}], "steps": []}
{"question": "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary.", "queries": ["how to update method in Python to handle new data structure", "best practices for including document type in API response", "Python fetch method examples with updated state structure"], "documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}]}
{"question": "3. Update the `fetch_policy_info` method to handle the new state structure and include the document type in the final output if necessary."}
{"queries": ["how to update method in Python to handle new data structure", "best practices for including document type in API response", "Python fetch method examples with updated state structure"]}
{"queries": ["how to update method in Python to handle new data structure", "best practices for including document type in API response", "Python fetch method examples with updated state structure"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_g43XsMXsYb0oJIeSOh4t2GAS", "function": {"arguments": "{\"queries\":[\"how to update method in Python to handle new data structure\",\"best practices for including document type in API response\",\"Python fetch method examples with updated state structure\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-605864a8-dbc6-4143-ba36-439593367507", "tool_calls": [{"name": "Response", "args": {"queries": ["how to update method in Python to handle new data structure", "best practices for including document type in API response", "Python fetch method examples with updated state structure"]}, "id": "call_g43XsMXsYb0oJIeSOh4t2GAS", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to update method in Python to handle new data structure", "best practices for including document type in API response", "Python fetch method examples with updated state structure"]}
{"queries": ["how to update method in Python to handle new data structure", "best practices for including document type in API response", "Python fetch method examples with updated state structure"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to update method in Python to handle new data structure'))", "Send(node='retrieve_documents', arg=QueryState(query='best practices for including document type in API response'))", "Send(node='retrieve_documents', arg=QueryState(query='Python fetch method examples with updated state structure'))"]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}]}
{"documents": [{"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "<Prerequisites titlesAndLinks={[", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tool_runtime/", "title": "How to pass run time values to tools | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tool_runtime/", "uuid": "a6c56983-ca28-50d5-b299-4b26180bbaa6"}, "page_content": "```python\nUpdateFavoritePets().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'description': 'List of favorite pets to set.',\n   'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n```python\nclass UpdateFavoritePets2(BaseTool):\n    name: str = \"update_favorite_pets\"\n    description: str = \"Update list of favorite pets\"\n\n    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:\n        user_to_pets[user_id] = pets\n\nUpdateFavoritePets2().get_input_schema().schema()\n```\n\n```output\n{'description': 'Use the tool.\\n\\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\\nto child implementations to enable tracing.',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'},\n  'user_id': {'title': 'User Id', 'type': 'string'}},\n 'required': ['pets', 'user_id'],\n 'title': 'update_favorite_petsSchema',\n 'type': 'object'}\n```\n\n```python\nUpdateFavoritePets2().tool_call_schema.schema()\n```\n\n```output\n{'description': 'Update list of favorite pets',\n 'properties': {'pets': {'items': {'type': 'string'},\n   'title': 'Pets',\n   'type': 'array'}},\n 'required': ['pets'],\n 'title': 'update_favorite_pets',\n 'type': 'object'}\n```\n\n#### Was this page helpful?\n\n- [Hiding arguments from the model](#hiding-arguments-from-the-model)\n\n- [Injecting arguments at runtime](#injecting-arguments-at-runtime)\n\n- [Other ways of annotating args](#other-ways-of-annotating-args)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "65cd272f-88db-5377-80b6-a6e1210414ed"}, "page_content": "Bulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_examples(  example_ids=[example.id, example_2.id],  inputs=[{\"input\": \"updated input 1\"}, {\"input\": \"updated input 2\"}],  outputs=[      {\"output\": \"updated output 1\"},      {\"output\": \"updated output 2\"},  ],  metadata=[{\"foo\": \"baz\"}, {\"foo\": \"qux\"}],  splits=[[\"training\", \"foo\"], \"training\"] # Splits can be arrays or standalone strings)await client.updateExamples([{  id: example.id,  inputs: { input: \"updated input 1\" },  outputs: { output: \"updated output 1\" },  metadata: { foo: \"baz\" },  split: [\"training\", \"foo\"] // Splits can be arrays or standalone strings},{  id: example2.id,  inputs: { input: \"updated input 2\" },  outputs: { output: \"updated output 2\" },  metadata: { foo: \"qux\" },  split: \"training\"},])Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to bind an evaluator to a dataset in the UINextHow to run an evaluation from the prompt playgroundCreate a dataset from list of valuesCreate a dataset from tracesCreate a dataset from a CSV fileCreate a dataset from pandas DataFrame (Python only)Fetch datasetsQuery all datasetsList datasets by nameList datasets by typeFetch examplesList all examples for a datasetList examples by idList examples by metadataList examples by structured filterUpdate examplesBulk update examplesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Intel's VDMS is a storage solution for efficient access of big-\u201dvisual\u201d-data that aims to achieve cloud scale by searching for relevant visual data via visual metadata stored as a graph and enabling machine friendly enhancements to visual data for faster access. VDMS is licensed under MIT.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "title": "Intel's Visual Data Management System (VDMS) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/vdms/", "uuid": "279535fa-628b-5f32-aa07-5c4e90259458"}, "page_content": "Tonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n\nMetadata:\n\tid:\t32\n\tpage_number:\t32\n\tpresident_included:\tTrue\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.4952471256\n\nContent:\n\tAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they\u2019re conducting on our children for profit. \n\nIt\u2019s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \n\nAnd let\u2019s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \n\nThird, support our veterans. \n\nVeterans are the best of us. \n\nI\u2019ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \n\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \n\nOur troops in Iraq and Afghanistan faced many dangers.\n\nMetadata:\n\tid:\t37\n\tpage_number:\t37\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n\nScore:\t1.5008399487\n\nContent:\n\tA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n\nMetadata:\n\tid:\t33\n\tpage_number:\t33\n\tpresident_included:\tFalse\n\tsource:\t../../how_to/state_of_the_union.txt\n--------------------------------------------------\n```\n\n### Update and Delete\u200b\n\nWhile building toward a real application, you want to go beyond adding data, and also update and delete data.\n\nHere is a basic example showing how to do so.  First, we will update the metadata for the document most relevant to the query by adding a date.\n\n```python\nfrom datetime import datetime\n\ndoc = db_FaissFlat.similarity_search(query)[0]\nprint(f\"Original metadata: \\n\\t{doc.metadata}\")\n\n# Update the metadata for a document by adding last datetime document read\ndatetime_str = datetime(2024, 5, 1, 14, 30, 0).isoformat()\ndoc.metadata[\"last_date_read\"] = {\"_date\": datetime_str}\nprint(f\"new metadata: \\n\\t{doc.metadata}\")\nprint(f\"{DELIMITER}\\n\")\n\n# Update document in VDMS\nid_to_update = doc.metadata[\"id\"]\ndb_FaissFlat.update_document(collection_name, id_to_update, doc)\nresponse, response_array = db_FaissFlat.get(\n    collection_name,\n    constraints={\n        \"id\": [\"==\", id_to_update],\n        \"last_date_read\": [\">=\", {\"_date\": \"2024-05-01T00:00:00\"}],\n    },\n)", "type": "Document"}, {"metadata": {"language": "en", "description": "This notebook covers how to get started with the Chroma vector store.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "title": "Chroma | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/chroma/", "uuid": "5481e4f6-540a-55d3-b42d-632b3d1e3dbd"}, "page_content": "Fake\n```bash\npip install -qU langchain-core\n```\n\n```python\nfrom langchain_core.embeddings import DeterministicFakeEmbedding\n\nembeddings = DeterministicFakeEmbedding(size=4096)\n```\n\n```python\nfrom langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n```\n\n### Initialization from client\u200b\n\nYou can also initialize from a `Chroma` client, which is particularly useful if you want easier access to the underlying database.\n\n```python\nimport chromadb\n\npersistent_client = chromadb.PersistentClient()\ncollection = persistent_client.get_or_create_collection(\"collection_name\")\ncollection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n\nvector_store_from_client = Chroma(\n    client=persistent_client,\n    collection_name=\"collection_name\",\n    embedding_function=embeddings,\n)\n```\n\n## Manage vector store\u200b\n\nOnce you have created your vector store, we can interact with it by adding and deleting different items.\n\n### Add items to vector store\u200b\n\nWe can add items to our vector store by using the `add_documents` function.\n\n```python\nfrom uuid import uuid4\n\nfrom langchain_core.documents import Document\n\ndocument_1 = Document(\n    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata={\"source\": \"tweet\"},\n    id=1,\n)\n\ndocument_2 = Document(\n    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata={\"source\": \"news\"},\n    id=2,\n)\n\ndocument_3 = Document(\n    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n    metadata={\"source\": \"tweet\"},\n    id=3,\n)\n\ndocument_4 = Document(\n    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n    metadata={\"source\": \"news\"},\n    id=4,\n)\n\ndocument_5 = Document(\n    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n    metadata={\"source\": \"tweet\"},\n    id=5,\n)\n\ndocument_6 = Document(\n    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n    metadata={\"source\": \"website\"},\n    id=6,\n)\n\ndocument_7 = Document(\n    page_content=\"The top 10 soccer players in the world right now.\",\n    metadata={\"source\": \"website\"},\n    id=7,\n)\n\ndocument_8 = Document(\n    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n    metadata={\"source\": \"tweet\"},\n    id=8,\n)\n\ndocument_9 = Document(\n    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n    metadata={\"source\": \"news\"},\n    id=9,\n)\n\ndocument_10 = Document(\n    page_content=\"I have a bad feeling I am going to get deleted :(\",\n    metadata={\"source\": \"tweet\"},\n    id=10,\n)\n\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\nuuids = [str(uuid4()) for _ in range(len(documents))]\n\nvector_store.add_documents(documents=documents, ids=uuids)\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output\n['f22ed484-6db3-4b76-adb1-18a777426cd6',\n 'e0d5bab4-6453-4511-9a37-023d9d288faa',\n '877d76b8-3580-4d9e-a13f-eed0fa3d134a',\n '26eaccab-81ce-4c0a-8e76-bf542647df18',\n 'bcaa8239-7986-4050-bf40-e14fb7dab997',\n 'cdc44b38-a83f-4e49-b249-7765b334e09d',\n 'a7a35354-2687-4bc2-8242-3849a4d18d34',\n '8780caf1-d946-4f27-a707-67d037e9e1d8',\n 'dec6af2a-7326-408f-893d-7d7d717dfda9',\n '3b18e210-bb59-47a0-8e17-c8e51176ea5e']\n```\n\n### Update items in vector store\u200b\n\nNow that we have added documents to our vector store, we can update existing documents by using the `update_documents` function.", "type": "Document"}, {"metadata": {"description": "Overview", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "title": "PDFMiner | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/pdfminer/", "uuid": "bc052e1c-7a1a-59bb-8b52-822d2ed87c1c"}, "page_content": "**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n```output", "type": "Document"}, {"metadata": {"description": "DocumentLoaders load data into the standard LangChain Document format.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/", "title": "Document loaders | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/", "uuid": "d45c6b8b-153e-54ec-bb30-b19850ba5410"}, "page_content": "| Document Loader | API reference |\n| ---- | ---- |\n| Twitter | TwitterTweetLoader |\n| Reddit | RedditPostsLoader |\n\n## Messaging Services\u200b\n\nThe below document loaders allow you to load data from different messaging platforms.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Telegram | TelegramChatFileLoader |\n| WhatsApp | WhatsAppChatLoader |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\n\n## Productivity tools\u200b\n\nThe below document loaders allow you to load data from commonly used productivity tools.\n\n| Document Loader | API reference |\n| ---- | ---- |\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\n\n## Common File Types\u200b\n\nThe below document loaders allow you to load data from common data formats.\n\n| Document Loader | Data Type |\n| ---- | ---- |\n| CSVLoader | CSV files |\n| DirectoryLoader | All files in a given directory |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n\n## All document loaders\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "We can extract useful features of documents using the Doctran library, which uses OpenAI's function calling feature to extract specific metadata.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "title": "Doctran: extract properties | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/doctran_extract_properties/", "uuid": "2dada460-3085-5315-bee7-4bc1acb83712"}, "page_content": "Please treat the information in this document with utmost confidentiality and ensure that it is not shared with unauthorized individuals. If you have any questions or concerns regarding the topics discussed, please do not hesitate to reach out to me directly.\n\nThank you for your attention, and let's continue to work together to achieve our goals.\n\nBest regards,\n\nJason Fan\nCofounder & CEO\nPsychic\njason@psychic.dev\n```\n\n```python\ndocuments = [Document(page_content=sample_text)]\nproperties = [\n    {\n        \"name\": \"category\",\n        \"description\": \"What type of email this is.\",\n        \"type\": \"string\",\n        \"enum\": [\"update\", \"action_item\", \"customer_feedback\", \"announcement\", \"other\"],\n        \"required\": True,\n    },\n    {\n        \"name\": \"mentions\",\n        \"description\": \"A list of all people mentioned in this email.\",\n        \"type\": \"array\",\n        \"items\": {\n            \"name\": \"full_name\",\n            \"description\": \"The full name of the person mentioned.\",\n            \"type\": \"string\",\n        },\n        \"required\": True,\n    },\n    {\n        \"name\": \"eli5\",\n        \"description\": \"Explain this email to me like I'm 5 years old.\",\n        \"type\": \"string\",\n        \"required\": True,\n    },\n]\nproperty_extractor = DoctranPropertyExtractor(properties=properties)\n```\n\n## Output\u200b\n\nAfter extracting properties from a document, the result will be returned as a new document with properties provided in the metadata\n\n```python\nextracted_document = property_extractor.transform_documents(\n    documents, properties=properties\n)\n```\n\n```python\nprint(json.dumps(extracted_document[0].metadata, indent=2))\n```\n\n```output\n{\n  \"extracted_properties\": {\n    \"category\": \"update\",\n    \"mentions\": [\n      \"John Doe\",\n      \"Jane Smith\",\n      \"Michael Johnson\",\n      \"Sarah Thompson\",\n      \"David Rodriguez\"\n    ],\n    \"eli5\": \"This email provides important updates and discussions on various topics. It mentions the implementation of security and privacy measures, HR updates and employee benefits, marketing initiatives and campaigns, and research and development projects. It recognizes the contributions of John Doe, Jane Smith, Michael Johnson, Sarah Thompson, and David Rodriguez. It also reminds everyone to adhere to data protection policies, enroll in the employee benefits program, attend the upcoming product launch event, and share ideas for new projects during the R&D brainstorming session.\"\n  }\n}\n```\n\n#### Was this page helpful?\n\n- [Input](#input)\n\n- [Output](#output)", "type": "Document"}, {"metadata": {"description": "Microsoft OneDrive (formerly SkyDrive) is a file hosting service operated by Microsoft.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "title": "Microsoft OneDrive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/microsoft_onedrive/", "uuid": "696df900-62ad-5b3e-98e3-672942b596cf"}, "page_content": "loader = OneDriveLoader(document_library_id=\"...\",\n                            handlers=handlers # pass handlers to OneDriveLoader\n                            )\n```\n\nIn case multiple file extensions map to the same MIME type, the last dictionary item will\napply.\nExample:\n\n```python\n# 'jpg' and 'jpeg' both map to 'image/jpeg' MIME type. SecondParser() will be used \n# to parse all jpg/jpeg files.\nhandlers = {\n    \"jpg\": FirstParser(),\n    \"jpeg\": SecondParser()\n}\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [\ud83e\uddd1 Instructions for ingesting your documents from OneDrive](#-instructions-for-ingesting-your-documents-from-onedrive)- [\ud83d\udd11 Authentication](#-authentication)\n\n- [\ud83d\uddc2\ufe0f Documents loader](#\ufe0f-documents-loader)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically", "title": "How to manage datasets programmatically | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "02d58a65-2ae7-548d-9665-5784c40f176e"}, "page_content": "List examples by id\u200b\nYou can also list multiple examples all by ID.\nPythonTypeScriptexample_ids = ['734fc6a0-c187-4266-9721-90b7a025751a','d6b4c1b9-6160-4d63-9b61-b034c585074f','4d31df4e-f9c3-4a6e-8b6c-65701c2fed13',]examples = client.list_examples(example_ids=example_ids)const exampleIds = [\"734fc6a0-c187-4266-9721-90b7a025751a\",\"d6b4c1b9-6160-4d63-9b61-b034c585074f\",\"4d31df4e-f9c3-4a6e-8b6c-65701c2fed13\",];const examples = await client.listExamples({exampleIds: exampleIds});\nList examples by metadata\u200b\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair.\nUnder the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.\nFor example, if you have an example with metadata {\"foo\": \"bar\", \"baz\": \"qux\"}, both {foo: bar} and {baz: qux} would match, as would {foo: bar, baz: qux}.\nPythonTypeScriptexamples = client.list_examples(dataset_name=dataset_name, metadata={\"foo\": \"bar\"})const examples = await client.listExamples({datasetName: datasetName, metadata: {foo: \"bar\"}});\nList examples by structured filter\u200b\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.\nnoteThis is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for metadata fields.\nYou can use the has operator to fetch examples with metadata fields that contain specific key/value pairs and the exists operator to fetch examples with metadata fields that contain a specific key.\nAdditionally, you can also chain multiple filters together using the and operator and negate a filter using the not operator.\nPythonTypeScriptexamples = client.list_examples(  dataset_name=dataset_name,  filter='and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))')const examples = await client.listExamples({datasetName: datasetName, filter: 'and(not(has(metadata, \\'{\"foo\": \"bar\"}\\')), exists(metadata, \"tenant_id\"))'});\nUpdate examples\u200b\nYou can programmatically update examples from LangSmith using the update_example/updateExample method in the Python and TypeScript SDKs. Below is an example.\nPythonTypeScriptclient.update_example(  example_id=example.id,  inputs={\"input\": \"updated input\"},  outputs={\"output\": \"updated output\"},  metadata={\"foo\": \"bar\"},  split=\"train\")await client.updateExample(example.id, {inputs: { input: \"updated input\" },outputs: { output: \"updated output\" },metadata: { \"foo\": \"bar\" },split: \"train\",})\nBulk update examples\u200b\nYou can also programmatically update multiple examples in a single request with the update_examples/updateExamples method in the Python and TypeScript SDKs. Below is an example.", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "content_type": null, "lastmod": "2024-12-03", "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "539e5580-d19c-5109-805c-e8f50a50c71b"}, "page_content": "Output:\n{'agent': {'messages': [{'content': [{'text': \"Certainly! I'll use the search function to look up the current weather in San Francisco for you. Let me do that now.\", 'type': 'text'}, {'id': 'toolu_011vroKUtWU7SBdrngpgpFMn', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}], 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-ee639877-d97d-40f8-96dc-d0d1ae22d203', 'example': False, 'tool_calls': [{'name': 'search', 'args': {'query': 'current weather in San Francisco'}, 'id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': '7bad0e72-5ebe-4b08-9b8a-b99b0fe22fb7', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news for outdoor activities and enjoying the city's beautiful sights.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which isn't typically part of a weather report. This might be due to the search engine including some astrological information or a joke in its results. However, for the purpose of answering your question about the weather, we can focus on the fact that it's sunny in San Francisco.\\n\\nIf you need any more specific information about the weather in San Francisco, such as temperature, wind speed, or forecast for the coming days, please let me know, and I'd be happy to search for that information for you.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-dbac539a-33c8-4f0c-9e20-91f318371e7c', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nNow let's get our list of states, and invoke from the third state (right before the tool get called):\nPythonJavascriptCURL\n\nstates = await client.threads.get_history(thread['thread_id'])\n\n# We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nstate_to_replay = states[2]\nprint(state_to_replay['next'])\n\nconst states = await client.threads.getHistory(thread['thread_id']);\n\n// We can confirm that this state is correct by checking the 'next' attribute and seeing that it is the tool call node\nconst stateToReplay = states[2];\nconsole.log(stateToReplay['next']);\n\ncurl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -r '.[2].next'\n\nOutput:\n['action']\n\nTo rerun from a state, we need first issue an empty update to the thread state. Then we need to pass in the resulting checkpoint_id as follows:\nPythonJavascriptCURL\n\nstate_to_replay = states[2]\nupdated_config = await client.threads.update_state(\n    thread[\"thread_id\"],\n    {\"messages\": []},\n    checkpoint_id=state_to_replay[\"checkpoint_id\"]\n)\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id, # graph_id\n    input=None,\n    stream_mode=\"updates\",\n    checkpoint_id=updated_config[\"checkpoint_id\"]\n):\n    if chunk.data and chunk.event != \"metadata\": \n        print(chunk.data)\n\nconst stateToReplay = states[2];\nconst config = await client.threads.updateState(thread[\"thread_id\"], { values: {\"messages\": [] }, checkpointId: stateToReplay[\"checkpoint_id\"] });\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: null,\n    streamMode: \"updates\",\n    checkpointId: config[\"checkpoint_id\"]\n  }\n);\nfor await (const chunk of streamResponse) {\n  if (chunk.data && chunk.event !== \"metadata\") {\n    console.log(chunk.data);\n  }\n}", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "title": "How to Replay and Branch from Prior States | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_time_travel/", "uuid": "e654c95d-4dd9-5686-8555-c58e4dd331d7"}, "page_content": "curl --request GET --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/history | jq -c '\n    .[2] as $state_to_replay |\n    {\n        values: { messages: .[2].values.messages[-1] },\n        checkpoint_id: $state_to_replay.checkpoint_id\n    }' | \\\ncurl --request POST \\\n    --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/state \\\n    --header 'Content-Type: application/json' \\\n    --data @- | jq .checkpoint_id | \\\ncurl --request POST \\\n --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\n --header 'Content-Type: application/json' \\\n --data \"{\n   \\\"assistant_id\\\": \\\"agent\\\",\n   \\\"checkpoint_id\\\": \\\"$1\\\",\n   \\\"stream_mode\\\": [\n     \\\"updates\\\"\n   ]\n }\" | \\\n sed 's/\\r$//' | \\\n awk '\n /^event:/ {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n     sub(/^event: /, \"\", $0)\n     event_type = $0\n     data_content = \"\"\n }\n /^data:/ {\n     sub(/^data: /, \"\", $0)\n     data_content = $0\n }\n END {\n     if (data_content != \"\" && event_type != \"metadata\") {\n         print data_content \"\\n\"\n     }\n }\n '\n\nOutput:\n{'action': {'messages': [{'content': '[\"I looked up: current weather in San Francisco. Result: It\\'s sunny in San Francisco, but you better look out if you\\'re a Gemini \ud83d\ude08.\"]', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'search', 'id': 'eba650e5-400e-4938-8508-f878dcbcc532', 'tool_call_id': 'toolu_011vroKUtWU7SBdrngpgpFMn'}]}}\n{'agent': {'messages': [{'content': \"Based on the search results, I can provide you with information about the current weather in San Francisco:\\n\\nThe weather in San Francisco is currently sunny. This is great news if you're planning any outdoor activities or simply want to enjoy a pleasant day in the city.\\n\\nIt's worth noting that the search result included an unusual comment about Geminis, which doesn't seem directly related to the weather. This appears to be a playful or humorous addition to the weather report, possibly from the source where this information was obtained.\\n\\nIs there anything else you'd like to know about the weather in San Francisco or any other information you need?\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-bc6dca3f-a1e2-4f59-a69b-fe0515a348bb', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n\nAs we can see, the graph restarted from the tool node with the same input as our original graph run.\nBranch off from previous state\u00b6\nUsing LangGraph's checkpointing, you can do more than just replay past states. You can branch off previous locations to let the agent explore alternate trajectories or to let a user \"version control\" changes in a workflow.\nLet's show how to do this to edit the state at a particular point in time. Let's update the state to change the input to the tool\nPythonJavascriptCURL\n\n# Let's now get the last message in the state\n# This is the one with the tool calls that we want to update\nlast_message = state_to_replay['values']['messages'][-1]\n\n# Let's now update the args for that tool call\nlast_message['tool_calls'][0]['args'] = {'query': 'current weather in SF'}\n\nconfig = await client.threads.update_state(thread['thread_id'],{\"messages\":[last_message]},checkpoint_id=state_to_replay['checkpoint_id'])\n\n// Let's now get the last message in the state\n// This is the one with the tool calls that we want to update\nlet lastMessage = stateToReplay['values']['messages'][-1];\n\n// Let's now update the args for that tool call\nlastMessage['tool_calls'][0]['args'] = { 'query': 'current weather in SF' };\n\nconst config = await client.threads.updateState(thread['thread_id'], { values: { \"messages\": [lastMessage] }, checkpointId: stateToReplay['checkpoint_id'] });", "type": "Document"}], "steps": []}
{"output": "respond"}
{"messages": [{"content": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-af441c77-eee6-41ea-b651-70f192e505bf", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "num_response_attempts": 1}
{"generations": [{"text": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-af441c77-eee6-41ea-b651-70f192e505bf", "tool_calls": [], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"messages": [{"content": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-af441c77-eee6-41ea-b651-70f192e505bf", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "Para almacenar el documento de identidad del cliente y los n\u00fameros de p\u00f3liza consultados en el estado del agente, puedes modificar la definici\u00f3n de `AgentState` para incluir estos nuevos campos. A continuaci\u00f3n, se muestra c\u00f3mo puedes implementar estos cambios:\n\n1. **Agregar campos para el documento de identidad** y los n\u00fameros de p\u00f3liza en `AgentState`.\n2. **Actualizar la l\u00f3gica en el m\u00e9todo `take_action`** para registrar el documento de identidad y los n\u00fameros de p\u00f3liza consultados.\n\nAqu\u00ed tienes el c\u00f3digo actualizado:\n\n```python\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom langchain_core.messages import AnyMessage, SystemMessage, ToolMessage\nfrom pydantic import Field\n\nclass AgentState(TypedDict):\n    messages: Annotated[List[AnyMessage], operator.add]\n    policy_numbers: List[str]  # Almacenar n\u00fameros de p\u00f3liza\n    document_type: str  # Tipo de documento de identidad del cliente\n\nclass Agent:\n\n    def __init__(self, model, tools, system=\"\"):\n        self.system = system\n        graph = StateGraph(AgentState)\n        graph.add_node(\"llm\", self.call_openai)\n        graph.add_node(\"action\", self.take_action)\n        graph.add_node(\"fetch_policy_info\", self.fetch_policy_info)  # Renombrar el nodo\n        graph.add_conditional_edges(\n            \"llm\",\n            self.exists_action,\n            {True: \"action\", False: \"fetch_policy_info\"}  # Cambiar END por fetch_policy_info\n        )\n        graph.add_edge(\"action\", \"llm\")\n        graph.add_edge(\"fetch_policy_info\", END)  # Conectar fetch_policy_info a END\n        graph.set_entry_point(\"llm\")\n        self.graph = graph.compile()\n        self.tools = {t.name: t for t in tools}\n        self.model = model.bind_tools(tools)\n\n    def exists_action(self, state: AgentState):\n        result = state['messages'][-1]\n        return len(result.tool_calls) > 0\n\n    def call_openai(self, state: AgentState):\n        messages = state['messages']\n        if self.system:\n            messages = [SystemMessage(content=self.system)] + messages\n        message = self.model.invoke(messages)\n        return {'messages': [message]}\n\n    def take_action(self, state: AgentState):\n        tool_calls = state['messages'][-1].tool_calls\n        results = []\n        for t in tool_calls:\n            print(f\"Calling: {t}\")\n            if not t['name'] in self.tools:      # check for bad tool name from LLM\n                print(\"\\n ....bad tool name....\")\n                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n            else:\n                result = self.tools[t['name']].invoke(t['args'])\n                # Si la acci\u00f3n es de consulta de p\u00f3liza, registrar el n\u00famero de p\u00f3liza\n                if t['name'] == \"consult_policy\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    policy_number = t['args'].get('policy_number')\n                    if policy_number:\n                        state.setdefault('policy_numbers', []).append(policy_number)\n                # Si la acci\u00f3n es de registrar documento de identidad\n                if t['name'] == \"register_document\":  # Suponiendo que este es el nombre de la acci\u00f3n\n                    document_type = t['args'].get('document_type')\n                    if document_type:\n                        state['document_type'] = document_type  # Guardar el tipo de documento\n            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n        print(\"Back to the model!\")\n        print(results)\n        return {'messages': results}\n\n    def fetch_policy_info(self, state: AgentState):\n        # Revisar si se han consultado p\u00f3lizas\n        if 'policy_numbers' in state and state['policy_numbers']:\n            policy_numbers = state['policy_numbers']\n            print(f\"Consultando informaci\u00f3n de p\u00f3lizas: {policy_numbers}\")\n            # Llamar a la API para obtener informaci\u00f3n de las p\u00f3lizas\n            policy_urls = self.call_policy_api(policy_numbers)\n            # Concatenar la informaci\u00f3n de la API al output final del modelo\n            last_message = state['messages'][-1].content  # Obtener el \u00faltimo mensaje del modelo\n            final_message = f\"{last_message}\\nInformaci\u00f3n de p\u00f3lizas consultadas: {policy_urls}\"\n        else:\n            final_message = \"No se consultaron p\u00f3lizas.\"\n\n        # Retornar el mensaje final\n        return {'messages': [final_message]}\n\n    def call_policy_api(self, policy_numbers):\n        # Aqu\u00ed ir\u00eda la l\u00f3gica para llamar a la API y obtener informaci\u00f3n de las p\u00f3lizas\n        # Simulaci\u00f3n de respuesta de la API que devuelve URLs\n        return [f\"https://api.example.com/policies/{number}\" for number in policy_numbers]\n```\n\n### Resumen de los cambios:\n- **Se agreg\u00f3 un campo `document_type`** en `AgentState` para almacenar el tipo de documento de identidad del cliente.\n- **En el m\u00e9todo `take_action`**, se registran los n\u00fameros de p\u00f3liza y el tipo de documento de identidad si las acciones correspondientes son invocadas.\n- **Se asumi\u00f3 que hay una acci\u00f3n llamada `register_document`** que se encarga de registrar el tipo de documento de identidad.\n\nCon estos cambios, el estado del agente ahora puede almacenar tanto el tipo de documento de identidad del cliente como los n\u00fameros de p\u00f3liza consultados, permitiendo un manejo m\u00e1s completo de la informaci\u00f3n del cliente.", "num_response_attempts": 1}
{"output": "end"}
{"output": {"binary_score": "yes"}}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_VFS99htB7uGkIIuEnzYrqROt", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-6e13d733-12ea-4dad-88ac-3f400ff91f83", "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_VFS99htB7uGkIIuEnzYrqROt", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"output": {"binary_score": "yes"}}
