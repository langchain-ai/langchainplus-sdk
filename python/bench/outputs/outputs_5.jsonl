{"messages": [{"content": "I'm trying to use neo4j import. I'm using a .env file \n\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username= \"neo4j\", password= os.getenv('NEO4J_PASSWORD'))\n\nI'm getting this error\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \",\n\t\"stack\": \"---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:393, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    392 try:\n--> 393     self.refresh_schema()\n    394 except neo4j.exceptions.ClientError as e:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:475, in Neo4jGraph.refresh_schema(self)\n    471 from neo4j.exceptions import ClientError, CypherTypeError\n    473 node_properties = [\n    474     el[\\\"output\\\"]\n--> 475     for el in self.query(\n    476         node_properties_query,\n    477         params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_LABELS + [BASE_ENTITY_LABEL]},\n    478     )\n    479 ]\n    480 rel_properties = [\n    481     el[\\\"output\\\"]\n    482     for el in self.query(\n    483         rel_properties_query, params={\\\"EXCLUDED_LABELS\\\": EXCLUDED_RELS}\n    484     )\n    485 ]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:431, in Neo4jGraph.query(self, query, params)\n    430 try:\n--> 431     data, _, _ = self._driver.execute_query(\n    432         Query(text=query, timeout=self.timeout),\n    433         database_=self._database,\n    434         parameters_=params,\n    435     )\n    436     json_data = [r.data() for r in data]\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:969, in Driver.execute_query(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\n    968 with session._pipelined_begin:\n--> 969     return session._run_transaction(\n    970         access_mode,\n    971         TelemetryAPI.DRIVER,\n    972         work,\n    973         (query_str, parameters, result_transformer_),\n    974         {},\n    975     )\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/session.py:581, in Session._run_transaction(self, access_mode, api, transaction_function, args, kwargs)\n    580 try:\n--> 581     result = transaction_function(tx, *args, **kwargs)\n    582 except asyncio.CancelledError:\n    583     # if cancellation callback has not been called yet:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_work/query.py:144, in unit_of_work.<locals>.wrapper.<locals>.wrapped(*args, **kwargs)\n    143 def wrapped(*args, **kwargs):\n--> 144     return f(*args, **kwargs)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/driver.py:1305, in _work(tx, query, parameters, transformer)\n   1299 def _work(\n   1300     tx: ManagedTransaction,\n   1301     query: te.LiteralString,\n   1302     parameters: dict[str, t.Any],\n   1303     transformer: t.Callable[[Result], t.Union[_T]],\n   1304 ) -> _T:\n-> 1305     res = tx.run(query, parameters)\n   1306     return transformer(res)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/transaction.py:195, in TransactionBase.run(self, query, parameters, **kwparameters)\n    194 parameters = dict(parameters or {}, **kwparameters)\n--> 195 result._tx_ready_run(query, parameters)\n    197 return result\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:175, in Result._tx_ready_run(self, query, parameters)\n    171 def _tx_ready_run(self, query, parameters):\n    172     # BEGIN+RUN does not carry any extra on the RUN message.\n    173     # BEGIN {extra}\n    174     # RUN \\\"query\\\" {parameters} {extra}\n--> 175     self._run(query, parameters, None, None, None, None, None, None)\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:231, in Result._run(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\n    230 self._connection.send_all()\n--> 231 self._attach()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/work/result.py:425, in Result._attach(self)\n    424 while self._attached is False:\n--> 425     self._connection.fetch_message()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:184, in ConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner(*args, **kwargs)\n    183 try:\n--> 184     func(*args, **kwargs)\n    185 except (Neo4jError, ServiceUnavailable, SessionExpired) as exc:\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt.py:994, in Bolt.fetch_message(self)\n    991 tag, fields = self.inbox.pop(\n    992     hydration_hooks=self.responses[0].hydration_hooks\n    993 )\n--> 994 res = self._process_message(tag, fields)\n    995 self.idle_since = monotonic()\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_bolt5.py:496, in Bolt5x0._process_message(self, tag, fields)\n    495 try:\n--> 496     response.on_failure(summary_metadata or {})\n    497 except (ServiceUnavailable, DatabaseUnavailable):\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/neo4j/_sync/io/_common.py:254, in Response.on_failure(self, metadata)\n    253 Util.callback(handler)\n--> 254 raise self._hydrate_error(metadata)\n\nClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\nCell In[16], line 12\n      9 # Set OpenAI API key from environment variable\n     10 openai.api_key = os.getenv('OPENAI_API_KEY')\n---> 12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username= \\\"neo4j\\\", password= os.getenv('NEO4J_PASSWORD'))\n\nFile ~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/langchain_neo4j/graphs/neo4j_graph.py:396, in Neo4jGraph.__init__(self, url, username, password, database, timeout, sanitize, refresh_schema, driver_config, enhanced_schema)\n    394 except neo4j.exceptions.ClientError as e:\n    395     if e.code == \\\"Neo.ClientError.Procedure.ProcedureNotFound\\\":\n--> 396         raise ValueError(\n    397             \\\"Could not use APOC procedures. \\\"\n    398             \\\"Please ensure the APOC plugin is installed in Neo4j and that \\\"\n    399             \\\"'apoc.meta.data()' is allowed in Neo4j configuration \\\"\n    400         )\n    401     raise e\n\nValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration \"\n}\n", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "36908bed-79db-414c-a58c-fb3f4a7fcba5", "example": false}, {"content": "The error message you are encountering indicates that the APOC (Awesome Procedures on Cypher) procedures are not available in your Neo4j instance. This is a common issue when the APOC plugin is not installed or not properly configured. Here are the steps you can take to resolve this issue:\n\n1. **Install the APOC Plugin**:\n   - If you are using Neo4j Desktop, you can install the APOC plugin through the \"Plugins\" tab for your database. Simply find APOC in the list and install it.\n   - If you are using a Docker container, you can include the APOC plugin by adding it to your Docker run command. For example:\n     ```bash\n     docker run \\\n         --name neo4j \\\n         -p 7474:7474 -p 7687:7687 \\\n         -d \\\n         -e NEO4J_AUTH=neo4j/password \\\n         -e NEO4J_PLUGINS=\"[\\\"apoc\\\"]\" \\\n         neo4j:latest\n     ```\n\n2. **Configure Neo4j to Allow APOC Procedures**:\n   - After installing the APOC plugin, you need to ensure that the configuration allows the use of APOC procedures. You can do this by modifying the `neo4j.conf` file (found in the `conf` directory of your Neo4j installation) to include the following line:\n     ```\n     dbms.security.procedures.unrestricted=apoc.*\n     ```\n   - If you are using Neo4j Desktop, you can find the configuration settings in the database settings.\n\n3. **Restart Neo4j**:\n   - After making changes to the configuration or installing the APOC plugin, restart your Neo4j instance to apply the changes.\n\n4. **Verify APOC Installation**:\n   - You can verify that the APOC procedures are available by running the following Cypher query in the Neo4j browser:\n     ```cypher\n     CALL apoc.help(\"apoc\")\n     ```\n   - If the procedures are installed correctly, you should see a list of available APOC procedures.\n\n5. **Retry Your Code**:\n   - Once you have confirmed that the APOC plugin is installed and configured correctly, try running your code again.\n\nBy following these steps, you should be able to resolve the error related to the APOC procedures in your Neo4j setup. If you continue to experience issues, please check the Neo4j logs for any additional error messages that may provide further insight.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-c73aeab8-1eb5-45b5-8c56-64d8a6d3a8ae", "example": false, "tool_calls": [], "invalid_tool_calls": []}, {"content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "ecc5e583-00e1-42b8-8231-7c340f43ab63", "example": false}, {"content": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f908b8eb-ef1a-4047-8d0c-e1b06520851c", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "steps": [], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "answer": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 1}
{"messages": [{"role": "user", "content": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}"}]}
{"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."], "documents": "delete", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}
{"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_GfXoS8ACVmmOnx9OceiQwmf4", "function": {"arguments": "{\"steps\":[\"Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.\",\"Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.\",\"After making the necessary imports and installations, run your code again to see if the error persists.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-17a6ba50-d1ec-4bfb-be56-10a48fa5bd8f", "tool_calls": [{"name": "Plan", "args": {"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}, "id": "call_GfXoS8ACVmmOnx9OceiQwmf4", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}
{"steps": ["Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."], "documents": "delete", "query": "import os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url= \"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n\nNow getting this error \n{\n\t\"name\": \"NameError\",\n\t\"message\": \"name 'openai' is not defined\",\n\t\"stack\": \"---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 10\n      7 load_dotenv()\n      9 # Set OpenAI API key from environment variable\n---> 10 openai.api_key = os.getenv('OPENAI_API_KEY')\n     12 graph = Neo4jGraph(url= \\\"bolt://localhost:7687\\\", username=\\\"neo4j\\\", password=os.getenv('NEO4J_PASSWORD'))\n\nNameError: name 'openai' is not defined\"\n}", "num_response_attempts": 0}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}], "steps": ["Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}
{"question": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script.", "queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"], "documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}]}
{"question": "Check if the OpenAI library is imported in your code. You need to import the OpenAI library before using it. Add 'import openai' at the beginning of your script."}
{"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}
{"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_8z70ag95p8RqJhRoJ3dFO4F9", "function": {"arguments": "{\"queries\":[\"How to check if a library is imported in Python?\",\"Python import statement for OpenAI library\",\"Best practices for organizing Python scripts with imports\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-fa8f6dc5-f7a7-4a24-abcb-c9252b717cd7", "tool_calls": [{"name": "Response", "args": {"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}, "id": "call_8z70ag95p8RqJhRoJ3dFO4F9", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}
{"queries": ["How to check if a library is imported in Python?", "Python import statement for OpenAI library", "Best practices for organizing Python scripts with imports"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='How to check if a library is imported in Python?'))", "Send(node='retrieve_documents', arg=QueryState(query='Python import statement for OpenAI library'))", "Send(node='retrieve_documents', arg=QueryState(query='Best practices for organizing Python scripts with imports'))"]}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}]}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide walks through how to run the repository locally and check in your first code.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "title": "Setup | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/contributing/how_to/code/setup/", "uuid": "6ed90ecd-1415-547c-be14-24fcc4c21400"}, "page_content": "```bash\nmake format\n```\n\nTo run formatting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake format\n```\n\nAdditionally, you can run the formatter only on the files that have been modified in your current branch as compared to the master branch using the format_diff command:\n\n```bash\nmake format_diff\n```\n\nThis is especially useful when you have made changes to a subset of the project and want to ensure your changes are properly formatted without affecting the rest of the codebase.\n\n#### Linting\u200b\n\nLinting for this project is done via a combination of [ruff](https://docs.astral.sh/ruff/rules/) and [mypy](http://mypy-lang.org/).\n\nTo run linting for docs, cookbook and templates:\n\n```bash\nmake lint\n```\n\nTo run linting for a library, run the same command from the relevant library directory:\n\n```bash\ncd libs/{LIBRARY}\nmake lint\n```\n\nIn addition, you can run the linter only on the files that have been modified in your current branch as compared to the master branch using the lint_diff command:\n\n```bash\nmake lint_diff\n```\n\nThis can be very helpful when you've made changes to only certain parts of the project and want to ensure your changes meet the linting standards without having to check the entire codebase.\n\nWe recognize linting can be annoying - if you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.\n\n### Spellcheck\u200b\n\nSpellchecking for this project is done via [codespell](https://github.com/codespell-project/codespell).\nNote that `codespell` finds common typos, so it could have false-positive (correctly spelled but rarely used) and false-negatives (not finding misspelled) words.\n\nTo check spelling for this project:\n\n```bash\nmake spell_check\n```\n\nTo fix spelling in place:\n\n```bash\nmake spell_fix\n```\n\nIf codespell is incorrectly flagging a word, you can skip spellcheck for that word by adding it to the codespell config in the `pyproject.toml` file.\n\n```python\n[tool.codespell]\n...\n# Add here:\nignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure'\n```\n\n## Working with Optional Dependencies\u200b\n\n`langchain`, `langchain-community`, and `langchain-experimental` rely on optional dependencies to keep these packages lightweight.\n\n`langchain-core` and partner packages **do not use** optional dependencies in this way.\n\nYou'll notice that `pyproject.toml` and `poetry.lock` are **not** touched when you add optional dependencies below.\n\nIf you're adding a new dependency to Langchain, assume that it will be an optional dependency, and\nthat most users won't have it installed.\n\nUsers who do not have the dependency installed should be able to **import** your code without\nany side effects (no warnings, no errors, no exceptions).\n\nTo introduce the dependency to a library, please do the following:\n\n1. Open extended_testing_deps.txt and add the dependency\n\n2. Add a unit test that the very least attempts to import the new code. Ideally, the unit\ntest makes use of lightweight fixtures to test the logic of the code.\n\n3. Please use the `@pytest.mark.requires(package_name)` decorator for any unit tests that require the dependency.\n\n## Adding a Jupyter Notebook\u200b\n\nIf you are adding a Jupyter Notebook example, you'll want to install the optional `dev` dependencies.\n\nTo install dev dependencies:\n\n```bash\npoetry install --with dev\n```\n\nLaunch a notebook:\n\n```bash\npoetry run jupyter notebook\n```\n\nWhen you run `poetry install`, the `langchain` package is installed as editable in the virtualenv, so your new logic can be imported into the notebook.\n\n#### Was this page helpful?\n\n- [Dependency Management: Poetry and other env/dependency managers](#dependency-management-poetry-and-other-envdependency-managers)\n\n- [Different packages](#different-packages)\n\n- [Local Development Dependencies](#local-development-dependencies)", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide assumes familiarity with the following concepts:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/tools_builtin/", "title": "How to use built-in tools and toolkits | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/tools_builtin/", "uuid": "97360d72-03fa-5fa7-8011-11e168ee6a72"}, "page_content": "## How to use built-in toolkits\u200b\n\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\n\nAll Toolkits expose a `get_tools` method which returns a list of tools.\n\nYou're usually meant to use them this way:\n\n```python\n# Initialize a toolkit\ntoolkit = ExampleTookit(...)\n\n# Get list of tools\ntools = toolkit.get_tools()\n```\n\n#### Was this page helpful?\n\n- [Tools](#tools)\n\n- [Customizing Default Tools](#customizing-default-tools)\n\n- [How to use built-in toolkits](#how-to-use-built-in-toolkits)", "type": "Document"}, {"metadata": {"language": "en", "description": "There are several ways to log traces to LangSmith.", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/tracing/annotate_code", "title": "Annotate code for tracing | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "lastmod": null, "content_type": "text/html; charset=utf-8", "priority": null, "loc": null, "uuid": "7985443c-fb90-5bb8-a406-f416a274d65e"}, "page_content": "Example usage\u200b\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions:\nTrace any public method in a class:\nfrom typing import Any, Callable, Type, TypeVarT = TypeVar(\"T\")def traceable_cls(cls: Type[T]) -> Type[T]:    \"\"\"Instrument all public methods in a class.\"\"\"    def wrap_method(name: str, method: Any) -> Any:        if callable(method) and not name.startswith(\"__\"):            return traceable(name=f\"{cls.__name__}.{name}\")(method)        return method    # Handle __dict__ case    for name in dir(cls):        if not name.startswith(\"_\"):            try:                method = getattr(cls, name)                setattr(cls, name, wrap_method(name, method))            except AttributeError:                # Skip attributes that can't be set (e.g., some descriptors)                pass    # Handle __slots__ case    if hasattr(cls, \"__slots__\"):        for slot in cls.__slots__:  # type: ignore[attr-defined]            if not slot.startswith(\"__\"):                try:                    method = getattr(cls, slot)                    setattr(cls, slot, wrap_method(slot, method))                except AttributeError:                    # Skip slots that don't have a value yet                    pass    return cls@traceable_clsclass MyClass:    def __init__(self, some_val: int):        self.some_val = some_val    def combine(self, other_val: int):        return self.some_val + other_val# See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/rMyClass(13).combine(29)Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability how-to guidesNextToggle tracing on and offUse @traceable / traceableUse the trace context manager (Python only)Wrap the OpenAI clientUse the RunTree APIExample usageCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright \u00a9 2024 LangChain, Inc.", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}, {"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/sql_csv/", "title": "How to do question answering over CSVs | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/sql_csv/", "uuid": "6758f6ac-ce1e-5e03-8cd0-55b0ae83c739"}, "page_content": "```python\ndb = SQLDatabase(engine=engine)\nprint(db.dialect)\nprint(db.get_usable_table_names())\nprint(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))\n```\n\n```output\nsqlite\n['titanic']\n[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n```\n\nAnd create a [SQL agent](/docs/tutorials/sql_qa/) to interact with it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "8af62e09-443d-5cc9-9c24-a8bd13563ed0"}, "page_content": "will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\\\\\nInclude module dependency or package manager dependency definition file.\\\\\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\\\\\n\\\\\\\\nUseful to know:\\\\\\\\nYou almost always put different classes in different files.\\\\\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\\\\\nYou try to add comments explaining very complex bits of logic.\\\\\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\\\\\npackage/project.\\\\\\\\n\\\\\\\\n\\\\\\\\nPython toolbelt preferences:\\\\\\\\n- pytest\\\\\\\\n- dataclasses\\\\\\\\n\\\\\" }, # \u2026 same conversation as earlier, ended with \\\\\"Make your own assumptions and state them explicitly before starting\\\\\". { \\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": \\\\\"Assumptions:\\\\\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\\\\\n\\\\\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\\\\\" }, { \\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Please now remember the steps:\\\\\\\\n\\\\\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\\\\\n\\\\\\\\nThen you will output the content of each file including ALL code.\\\\\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\\\\\nFILENAME is the lowercase file name including the file extension,\\\\\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\\\\\n\\\\\\\\nFILENAME\\\\\\\\n\\`\\`\\`LANG\\\\\\\\nCODE\\\\\\\\n\\`\\`\\`\\\\\\\\n\\\\\\\\nPlease note that the code should be fully functional. No placeholders.\\\\\\\\n\\\\\\\\nYou will start with the \\\\\\\\\\\\\"entrypoint\\\\\\\\\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files", "type": "Document"}, {"metadata": {"language": "en", "description": "markdownify is a Python package that converts HTML documents to Markdown format with customizable options for handling tags (links, images, ...), heading styles and other.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "title": "Markdownify | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_transformers/markdownify/", "uuid": "7b547e20-3a35-5cb0-881c-341c25231f06"}, "page_content": "of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n&#34;\\n  },\\n #  \u2026 same conversation as earlier, ended with &#34;Make your own assumptions and state them explicitly before starting&#34;.\\n  {\\n    &#34;role&#34;: &#34;assistant&#34;,\\n    &#34;content&#34;: &#34;Assumptions:\\\\n1. Model: The model will contain the game&#39;s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game&#39;s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?&#34;\\n  },\\n  {\\n    &#34;role&#34;: &#34;user&#34;,\\n    &#34;content&#34;: &#34;Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code&#39;s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n\\`\\`\\`LANG\\\\nCODE\\\\n\\`\\`\\`\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\&#34;entrypoint\\\\&#34; file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture", "type": "Document"}, {"metadata": {"language": "en", "description": "Last updated: 09.16.24", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_3/", "title": "LangChain v0.3 | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_3/", "uuid": "23e8bb05-3597-5c72-974c-eaa037023f7c"}, "page_content": "### 3. Sub-classing LangChain models\u200b\n\nAny sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`)\nshould upgrade to use Pydantic 2 features.\n\nFor example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should\nbe updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to\n`pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced\nwith imports from `pydantic`.\n\n```python\nfrom pydantic.v1 import validator, Field # if pydantic 2 is installed\n# from pydantic import validator, Field # if pydantic 1 is installed\n# from langchain_core.pydantic_v1 import validator, Field\n# from langchain.pydantic_v1 import validator, Field\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @validator('x') # v1 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n```\n\nShould change to:\n\n```python\nfrom pydantic import Field, field_validator # pydantic v2\nfrom langchain_core.pydantic_v1 import BaseTool\n\nclass CustomTool(BaseTool): # BaseTool is v1 code\n    x: int = Field(default=1)\n\n    def _run(*args, **kwargs):\n        return \"hello\"\n\n    @field_validator('x') # v2 code\n    @classmethod\n    def validate_x(cls, x: int) -> int:\n        return 1\n\nCustomTool(\n    name='custom_tool',\n    description=\"hello\",\n    x=1,\n)\n```\n\n### 4. model_rebuild()\u200b\n\nWhen sub-classing from LangChain models, users may need to add relevant imports\nto the file and rebuild the model.\n\nYou can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).\n\n```python\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\nNew code:\n\n```python\nfrom typing import Optional as Optional\n\nfrom langchain_core.output_parsers import BaseOutputParser\n\nclass FooParser(BaseOutputParser):\n    ...\n\nFooParser.model_rebuild()\n```\n\n**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)\n## Migrate using langchain-cli\u200b\n\nThe `langchain-cli` can help update deprecated LangChain imports in your code automatically.\n\nPlease note that the `langchain-cli` only handles deprecated LangChain imports and cannot\nhelp to upgrade your code from pydantic 1 to pydantic 2.\n\nFor help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).\n\nAs of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.\n\n### Installation\u200b\n\n```bash\npip install -U langchain-cli\nlangchain-cli --version # <-- Make sure the version is at least 0.0.31\n```\n\n### Usage\u200b\n\nGiven that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).\n\nThe `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well\nas older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),\n\nYou will need to run the migration script **twice** as it only applies one import replacement per run.\n\nFor example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:\n\nAfter the first run, you\u2019ll get: `from langchain_community.chat_models import ChatOpenAI`\nAfter the second run, you\u2019ll get: `from langchain_openai import ChatOpenAI`\n\n```bash\n# Run a first time\n# Will replace from langchain.chat_models import ChatOpenAI\nlangchain-cli migrate --help [path to code] # Help\nlangchain-cli migrate [path to code] # Apply", "type": "Document"}, {"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}], "steps": ["Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "After making the necessary imports and installations, run your code again to see if the error persists."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}], "steps": ["After making the necessary imports and installations, run your code again to see if the error persists."]}
{"question": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'.", "queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"], "documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}]}
{"question": "Verify that the OpenAI library is installed in your environment. If it's not installed, you can install it using pip: 'pip install openai'."}
{"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}
{"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_WTxtVvhnonOy3cqPhrnCEQ60", "function": {"arguments": "{\"queries\":[\"How to check if OpenAI library is installed in Python?\",\"What command to use for installing OpenAI library with pip?\",\"Troubleshooting OpenAI library installation issues in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-9df9f557-1ac8-4164-b61f-3ebd6f3192d4", "tool_calls": [{"name": "Response", "args": {"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}, "id": "call_WTxtVvhnonOy3cqPhrnCEQ60", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}
{"queries": ["How to check if OpenAI library is installed in Python?", "What command to use for installing OpenAI library with pip?", "Troubleshooting OpenAI library installation issues in Python"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='How to check if OpenAI library is installed in Python?'))", "Send(node='retrieve_documents', arg=QueryState(query='What command to use for installing OpenAI library with pip?'))", "Send(node='retrieve_documents', arg=QueryState(query='Troubleshooting OpenAI library installation issues in Python'))"]}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "All functionality related to OpenAI", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "All functionality related to OpenAI", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}]}
{"documents": [{"metadata": {"description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "All functionality related to OpenAI", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "All functionality related to OpenAI", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/providers/openai/", "title": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/providers/openai/", "uuid": "b7d0a36c-1004-55bd-b931-52db68a7c3b2"}, "page_content": "OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# OpenAI\n\nAll functionality related to OpenAI\n\n[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\nconsisting of the non-profit `OpenAI Incorporated`\nand its for-profit subsidiary corporation `OpenAI Limited Partnership`.\n`OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\n`OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.\n\nThe [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\n\n[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.\n\n## Installation and Setup\u200b\n\nInstall the integration package with\n\n```bash\npip install langchain-openai\n```\n\nGet an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\n\n## Chat model\u200b\n\nSee a [usage example](/docs/integrations/chat/openai/).\n\n```python\nfrom langchain_openai import ChatOpenAI\n```\n\n**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureChatOpenAI\n```\n\n**API Reference:**[AzureChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/chat/azure_chat_openai/).\n\n## LLM\u200b\n\nSee a [usage example](/docs/integrations/llms/openai/).\n\n```python\nfrom langchain_openai import OpenAI\n```\n\n**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\n\n```python\nfrom langchain_openai import AzureOpenAI\n```\n\n**API Reference:**[AzureOpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.azure.AzureOpenAI.html)\nFor a more detailed walkthrough of the `Azure` wrapper, see [here](/docs/integrations/llms/azure_openai/).\n\n## Embedding Model\u200b\n\nSee a [usage example](/docs/integrations/text_embedding/openai/)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\n```\n\n**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n## Document Loader\u200b\n\nSee a [usage example](/docs/integrations/document_loaders/chatgpt_loader/).\n\n```python\nfrom langchain_community.document_loaders.chatgpt import ChatGPTLoader\n```\n\n**API Reference:**[ChatGPTLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.chatgpt.ChatGPTLoader.html)\n## Retriever\u200b\n\nSee a [usage example](/docs/integrations/retrievers/chatgpt-plugin/).\n\n```python\nfrom langchain.retrievers import ChatGPTPluginRetriever\n```\n\n**API Reference:**[ChatGPTPluginRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.chatgpt_plugin_retriever.ChatGPTPluginRetriever.html)\n## Tools\u200b\n\n### Dall-E Image Generator\u200b\n\n[OpenAI Dall-E](https://openai.com/dall-e-3) are text-to-image models developed by `OpenAI`\nusing deep learning methodologies to generate digital images from natural language descriptions,\ncalled \"prompts\".\n\nSee a [usage example](/docs/integrations/tools/dalle_image_generator/).\n\n```python\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n```\n\n**API Reference:**[DallEAPIWrapper](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.dalle_image_generator.DallEAPIWrapper.html)\n## Adapter\u200b\n\nSee a [usage example](/docs/integrations/adapters/openai/).", "type": "Document"}, {"metadata": {"description": "This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_web/", "title": "How to load web pages | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_web/", "uuid": "04bc0c1a-a685-5035-8cdc-a7f4387a8c41"}, "page_content": "```python\n%pip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```\n\n```python\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())\nretrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)\nfor doc in retrieved_docs:\n    print(f'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\n')\n```\n\n**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n```output\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n``````output\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:\n\nPage https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\n```\n\n## Other web page loaders\u200b\n\nFor a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Simple and fast text extraction](#simple-and-fast-text-extraction)\n\n- [Advanced parsing](#advanced-parsing)- [Extracting content from specific sections](#extracting-content-from-specific-sections)\n\n- [Vector search over page content](#vector-search-over-page-content)\n\n- [Other web page loaders](#other-web-page-loaders)", "type": "Document"}, {"metadata": {"description": "This notebook walks through connecting a LangChain to the Google Drive API.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/google_drive/", "title": "Google Drive | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/google_drive/", "uuid": "88f47abb-dad6-503d-9fe1-962edd2f255b"}, "page_content": "```python\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain_openai import OpenAI\n\nllm = OpenAI(temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n)\n```\n\n**API Reference:**[AgentType](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent_types.AgentType.html) | [initialize_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)\n```python\nagent.run(\"Search in google drive, who is 'Yann LeCun' ?\")\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Prerequisites](#prerequisites)\n\n- [Instructions for retrieving your Google Docs data](#instructions-for-retrieving-your-google-docs-data)\n\n- [Use within an Agent](#use-within-an-agent)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Azure OpenAI text completion models. The latest and most popular Azure OpenAI models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "title": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/azure_openai/", "uuid": "67bbea5a-555e-577f-8c50-bfe43cc3ffd9"}, "page_content": "Azure OpenAI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Azure OpenAI\n\ncautionYou are currently on a page documenting the use of Azure OpenAI [text completion models](/docs/concepts/text_llms/). The latest and most popular Azure OpenAI models are [chat completion models](/docs/concepts/chat_models/).\n\nUnless you are specifically using `gpt-3.5-turbo-instruct`, you are probably looking for [this page instead](/docs/integrations/chat/azure_chat_openai/).\n\nThis page goes over how to use LangChain with [Azure OpenAI](https://aka.ms/azure-openai).\n\nThe Azure OpenAI API is compatible with OpenAI's API.  The `openai` Python package makes it easy to use both OpenAI and Azure OpenAI.  You can call Azure OpenAI the same way you call OpenAI with the exceptions noted below.\n\n## API configuration\u200b\n\nYou can configure the `openai` package to use Azure OpenAI using environment variables.  The following is for `bash`:\n\n```bash\n# The API version you want to use: set this to `2023-12-01-preview` for the released version.\nexport OPENAI_API_VERSION=2023-12-01-preview\n# The base URL for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com\n# The API key for your Azure OpenAI resource.  You can find this in the Azure portal under your Azure OpenAI resource.\nexport AZURE_OPENAI_API_KEY=<your Azure OpenAI API key>\n```\n\nAlternatively, you can configure the API right within your running Python environment:\n\n```python\nimport os\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n```\n\n## Azure Active Directory Authentication\u200b\n\nThere are two ways you can authenticate to Azure OpenAI:\n\n- API Key\n\n- Azure Active Directory (AAD)\n\nUsing the API key is the easiest way to get started. You can find your API key in the Azure portal under your Azure OpenAI resource.\n\nHowever, if you have complex security requirements - you may want to use Azure Active Directory. You can find more information on how to use AAD with Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity).\n\nIf you are developing locally, you will need to have the Azure CLI installed and be logged in. You can install the Azure CLI [here](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli). Then, run `az login` to log in.\n\nAdd a role an Azure role assignment `Cognitive Services OpenAI User` scoped to your Azure OpenAI resource. This will allow you to get a token from AAD to use with Azure OpenAI. You can grant this role assignment to a user, group, service principal, or managed identity. For more information about Azure OpenAI RBAC roles see [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control).\n\nTo use AAD in Python with LangChain, install the `azure-identity` package. Then, set `OPENAI_API_TYPE` to `azure_ad`. Next, use the `DefaultAzureCredential` class to get a token from AAD by calling `get_token` as shown below. Finally, set the `OPENAI_API_KEY` environment variable to the token value.\n\n```python\nimport os\nfrom azure.identity import DefaultAzureCredential\n\n# Get the Azure Credential\ncredential = DefaultAzureCredential()\n\n# Set the API type to `azure_ad`\nos.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n# Set the API_KEY to the token from the Azure credential\nos.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n```\n\nThe `DefaultAzureCredential` class is an easy way to get started with AAD authentication. You can also customize the credential chain if necessary. In the example shown below, we first try Managed Identity, then fall back to the Azure CLI. This is useful if you are running your code in Azure, but want to develop locally.\n\n```python\nfrom azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential", "type": "Document"}, {"metadata": {"description": "This guide previously used the RunnableWithMessageHistory abstraction. You can access this version of the documentation in the v0.2 docs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "title": "How to add chat history | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_chat_history_how_to/", "uuid": "e26c4f35-e925-58a6-871b-32a1358c25f4"}, "page_content": "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```\n\nDatabricks\n```bash\npip install -qU databricks-langchain\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"DATABRICKS_TOKEN\"] = getpass.getpass()\n\nfrom databricks_langchain import ChatDatabricks\n\nos.environ[\"DATABRICKS_HOST\"] = \"https://example.staging.cloud.databricks.com/serving-endpoints\"\n\nllm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-70b-instruct\")\n```\n\nAn [embedding model](/docs/integrations/text_embedding/):\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nembeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```", "type": "Document"}, {"metadata": {"description": "This notebook provides a quick overview for getting started with Jina tool. For detailed documentation of all Jina features and configurations head to the API reference.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/jina_search/", "title": "Jina Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/jina_search/", "uuid": "ee54fa7b-b9f9-5a03-b91f-ad00ae989409"}, "page_content": "```output\n[{\"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"link\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"snippet\": \"<strong>LangGraph</strong> <strong>is</strong> a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\", \"content\": \"Imagine you're building a complex, multi-agent large language model (LLM) application. It's exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\n\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\n\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for de\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "The Github toolkit contains tools that enable an LLM agent to interact with a github repository.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/github/", "title": "Github Toolkit | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/github/", "uuid": "ff3eac04-7315-530a-bb30-dace51723d24"}, "page_content": "View available tools:\n\n```python\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(tool.name)\n```\n\n```output\nGet Issues\nGet Issue\nComment on Issue\nList open pull requests (PRs)\nGet Pull Request\nOverview of files included in PR\nCreate Pull Request\nList Pull Requests' Files\nCreate File\nRead File\nUpdate File\nDelete File\nOverview of existing files in Main branch\nOverview of files in current working branch\nList branches in this repository\nSet active branch\nCreate a new branch\nGet files from a directory\nSearch issues and pull requests\nSearch code\nCreate review request\n```\n\nThe purpose of these tools is as follows:\n\nEach of these steps will be explained in great detail below.\n\n1. \n**Get Issues**- fetches issues from the repository.\n\n2. \n**Get Issue**- fetches details about a specific issue.\n\n3. \n**Comment on Issue**- posts a comment on a specific issue.\n\n4. \n**Create Pull Request**- creates a pull request from the bot's working branch to the base branch.\n\n5. \n**Create File**- creates a new file in the repository.\n\n6. \n**Read File**- reads a file from the repository.\n\n7. \n**Update File**- updates a file in the repository.\n\n8. \n**Delete File**- deletes a file from the repository.\n\n## Use within an agent\u200b\n\nWe will need a LLM or chat model:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()", "type": "Document"}, {"metadata": {"language": "en", "description": "Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "title": "Tavily Search | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/tavily_search/", "uuid": "26448348-5def-505f-9ab7-db75567551f5"}, "page_content": "```python\nimport json\n\n# Abbreviate the results for demo purposes\nprint(json.dumps({k: str(v)[:200] for k, v in tool_msg.artifact.items()}, indent=2))\n```\n\n```output\n{\n  \"query\": \"euro 2024 host nation\",\n  \"follow_up_questions\": \"None\",\n  \"answer\": \"Germany will be the host nation for Euro 2024, with the tournament scheduled to take place from June 14 to July 14. The matches will be held in 10 different cities across Germany, including Berlin, Co\",\n  \"images\": \"['https://i.ytimg.com/vi/3hsX0vLatNw/maxresdefault.jpg', 'https://img.planetafobal.com/2021/10/sedes-uefa-euro-2024-alemania-fg.jpg', 'https://editorial.uefa.com/resources/0274-14fe4fafd0d4-413fc8a7b7\",\n  \"results\": \"[{'title': 'Where is Euro 2024? Country, host cities and venues', 'url': 'https://www.radiotimes.com/tv/sport/football/euro-2024-location/', 'content': \\\"Euro 2024 host cities. Germany have 10 host cit\",\n  \"response_time\": \"3.97\"\n}\n```\n\n## Chaining\u200b\n\nWe can use our tool in a chain by first binding it to a [tool-calling model](/docs/how_to/tool_calling/) and then calling it:\n\n \nOpenAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n```\n\nAnthropic\n```bash\npip install -qU langchain-anthropic\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()\n\nfrom langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n```\n\nAzure\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import AzureChatOpenAI\n\nllm = AzureChatOpenAI(\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n)\n```\n\nGoogle\n```bash\npip install -qU langchain-google-vertexai\n```\n\n```python\n# Ensure your VertexAI credentials are configured\n\nfrom langchain_google_vertexai import ChatVertexAI\n\nllm = ChatVertexAI(model=\"gemini-1.5-flash\")\n```\n\nAWS\n```bash\npip install -qU langchain-aws\n```\n\n```python\n# Ensure your AWS credentials are configured\n\nfrom langchain_aws import ChatBedrock\n\nllm = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    beta_use_converse_api=True)\n```\n\nCohere\n```bash\npip install -qU langchain-cohere\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"COHERE_API_KEY\"] = getpass.getpass()\n\nfrom langchain_cohere import ChatCohere\n\nllm = ChatCohere(model=\"command-r-plus\")\n```\n\nNVIDIA\n```bash\npip install -qU langchain-nvidia-ai-endpoints\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()\n\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\n\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n```\n\nFireworksAI\n```bash\npip install -qU langchain-fireworks\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()\n\nfrom langchain_fireworks import ChatFireworks\n\nllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")\n```\n\nGroq\n```bash\npip install -qU langchain-groq\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-8b-8192\")\n```\n\nMistralAI\n```bash\npip install -qU langchain-mistralai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n\nfrom langchain_mistralai import ChatMistralAI\n\nllm = ChatMistralAI(model=\"mistral-large-latest\")\n```\n\nTogetherAI\n```bash\npip install -qU langchain-openai\n```\n\n```python\nimport getpass\nimport os\n\nos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()\n\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n)\n```", "type": "Document"}], "steps": ["After making the necessary imports and installations, run your code again to see if the error persists."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "steps": []}
{"question": "After making the necessary imports and installations, run your code again to see if the error persists.", "queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"], "documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}]}
{"question": "After making the necessary imports and installations, run your code again to see if the error persists."}
{"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}
{"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_lV1vnlbbx3p34aRARvaCgTLr", "function": {"arguments": "{\"queries\":[\"how to troubleshoot code errors after imports\",\"common Python errors after installing packages\",\"steps to resolve import errors in Python\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f1cdd33f-10f5-4038-aa56-4d866e5223c0", "tool_calls": [{"name": "Response", "args": {"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}, "id": "call_lV1vnlbbx3p34aRARvaCgTLr", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}
{"queries": ["how to troubleshoot code errors after imports", "common Python errors after installing packages", "steps to resolve import errors in Python"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to troubleshoot code errors after imports'))", "Send(node='retrieve_documents', arg=QueryState(query='common Python errors after installing packages'))", "Send(node='retrieve_documents', arg=QueryState(query='steps to resolve import errors in Python'))"]}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}]}
{"documents": [{"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}]}
{"documents": [{"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}]}
{"documents": [{"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"language": "en", "description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "LangChain v0.2 was released in May 2024. This release includes a number of breaking changes and deprecations. This document contains a guide on upgrading to 0.2.x.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/versions/v0_2/", "title": "Migration | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/versions/v0_2/", "uuid": "c6e6c9b9-5a0b-54e5-b58b-6f7e010f90fc"}, "page_content": "# Run a second time to apply more import replacements\nlangchain-cli migrate --diff [path to code] # Preview\nlangchain-cli migrate [path to code] # Apply\n```\n\n### Other options\u200b\n\n```bash\n# See help menu\nlangchain-cli migrate --help\n# Preview Changes without applying\nlangchain-cli migrate --diff [path to code]\n# Run on code including ipython notebooks\n# Apply all import updates except for updates from langchain to langchain-core\nlangchain-cli migrate --disable langchain_to_core --include-ipynb [path to code]\n```\n\n#### Was this page helpful?\n\n- [Upgrade to new imports](#upgrade-to-new-imports)\n\n- [Installation](#installation)\n\n- [Usage](#usage)- [Other options](#other-options)", "type": "Document"}, {"metadata": {"description": "An output parser was unable to handle model output as expected.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "title": "OUTPUT_PARSING_FAILURE | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/", "uuid": "4c5dd25c-312a-58f1-95ff-a433ccc0d012"}, "page_content": "## Troubleshooting\n\nThe following may help resolve this error:\n\n- Consider using [tool calling or other structured output techniques](/docs/how_to/structured_output/) if possible without an output parser to reliably output parseable values.\n  - If you are using a prebuilt chain or agent, use [LangGraph](https://langchain-ai.github.io/langgraph/) to compose your logic explicitly instead.\n- Add more precise formatting instructions to your prompt. In the above example, adding `\"You must always return valid JSON fenced by a markdown code block. Do not return any additional text.\"` to your input may help steer the model to returning the expected format.\n- If you are using a smaller or less capable model, try using a more capable one.\n- Add [LLM-powered retries](/docs/how_to/output_parser_fixing/).\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "", "language": "", "changefreq": null, "source": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "title": "Code generation with RAG and self-correction | \ud83e\udd9c\ud83d\udd78\ufe0fLangGraph", "lastmod": "2024-12-03", "content_type": null, "priority": null, "loc": "https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/", "uuid": "8524db99-5b8a-5036-9cfd-e8c034ec4ba7"}, "page_content": "# Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n### Edges\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()\n\nAPI Reference:\nEND | StateGraph | START\n\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\nsolution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})\n\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE IMPORT CHECK: FAILED---\n---DECISION: RE-TRY SOLUTION---\n---GENERATING CODE SOLUTION---\n---CHECKING CODE---\n---CODE BLOCK CHECK: FAILED---\n---DECISION: FINISH---\n\nsolution[\"generation\"]", "type": "Document"}, {"metadata": {"language": "en", "description": "This page contains guides around resolving common errors you may find while building with LangChain.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/troubleshooting/errors/", "title": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/troubleshooting/errors/", "uuid": "8055e105-68e2-5e0f-950d-2f771934f6ee"}, "page_content": "Error reference | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Error reference\n\nThis page contains guides around resolving common errors you may find while building with LangChain.\nErrors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.\n\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)\n\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)\n\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)\n\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)\n\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)\n\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)\n\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "ManticoreSearch is an open-source search engine that offers fast, scalable, and user-friendly capabilities. Originating as a fork of Sphinx Search, it has evolved to incorporate modern search engine features and improvements. ManticoreSearch distinguishes itself with its robust performance and ease of integration into various applications.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "title": "ManticoreSearch VectorStore | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/manticore_search/", "uuid": "641239ad-2b97-57eb-84d7-b23c58f52ae2"}, "page_content": "# Restart container\n!docker restart {container_id}\n```\n\n```output\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy InRelease [3525 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]            \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \nGet:5 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \nGet:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB]\nGet:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1796 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:20 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 Packages [5020 kB]\nFetched 38.6 MB in 7s (5847 kB/s)                                              \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  manticore-columnar-lib\n0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\nNeed to get 1990 kB of archives.\nAfter this operation, 10.0 MB of additional disk space will be used.\nGet:1 http://repo.manticoresearch.com/repository/manticoresearch_jammy_dev jammy/main amd64 manticore-columnar-lib amd64 2.2.5-240217-a5342a1 [1990 kB]\nFetched 1990 kB in 1s (1505 kB/s)                 \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package manticore-columnar-lib.\n(Reading database ... 12260 files and directories currently installed.)\nPreparing to unpack .../manticore-columnar-lib_2.2.5-240217-a5342a1_amd64.deb ...\nUnpacking manticore-columnar-lib (2.2.5-240217-a5342a1) ...\nSetting up manticore-columnar-lib (2.2.5-240217-a5342a1) ...\na546aec22291\n```\n\nInstalling ManticoreSearch python client\n\n```python\n%pip install --upgrade --quiet manticoresearch-dev\n```\n\n```output\n\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m24.0[0m\n[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m\nNote: you may need to restart the kernel to use updated packages.\n```\n\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n\n```python\nfrom langchain_community.embeddings import GPT4AllEmbeddings\nfrom langchain_community.vectorstores import ManticoreSearch, ManticoreSearchSettings\nfrom langchain_text_splitters import CharacterTextSplitter\n```", "type": "Document"}, {"metadata": {"description": "E2B's cloud environments are great runtime sandboxes for LLMs.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "title": "E2B Data Analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/tools/e2b_data_analysis/", "uuid": "46934a57-f092-5a9c-b696-b962ea1b0028"}, "page_content": "```python\n# Install Python package\ne2b_data_analysis_tool.install_python_packages(\"pandas\")\n```\n\n```output\nstdout: Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.1)\nstdout: Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nstdout: Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nstdout: Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.1)\nstdout: Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\nstdout: Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n```\n\nAdditionally, you can download any file from the sandbox like this:\n\n```python\n# The path is a remote path in the sandbox\nfiles_in_bytes = e2b_data_analysis_tool.download_file(\"/home/user/netflix.csv\")\n```\n\nLastly, you can run any shell command inside the sandbox via `run_command`.\n\n```python\n# Install SQLite\ne2b_data_analysis_tool.run_command(\"sudo apt update\")\ne2b_data_analysis_tool.install_system_packages(\"sqlite3\")\n\n# Check the SQLite version\noutput = e2b_data_analysis_tool.run_command(\"sqlite3 --version\")\nprint(\"version: \", output[\"stdout\"])\nprint(\"error: \", output[\"stderr\"])\nprint(\"exit code: \", output[\"exit_code\"])\n```\n\n```output\nstderr: \nstderr: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\nstderr: \nstdout: Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\nstdout: Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\nstdout: Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\nstdout: Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: All packages are up to date.\nstdout: Reading package lists...\nstdout: Building dependency tree...\nstdout: Reading state information...\nstdout: Suggested packages:\nstdout:   sqlite3-doc\nstdout: The following NEW packages will be installed:\nstdout:   sqlite3\nstdout: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\nstdout: Need to get 768 kB of archives.\nstdout: After this operation, 1873 kB of additional disk space will be used.\nstdout: Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sqlite3 amd64 3.37.2-2ubuntu0.1 [768 kB]\nstderr: debconf: delaying package configuration, since apt-utils is not installed\nstdout: Fetched 768 kB in 0s (2258 kB/s)\nstdout: Selecting previously unselected package sqlite3.\n(Reading database ... 23999 files and directories currently installed.)\nstdout: Preparing to unpack .../sqlite3_3.37.2-2ubuntu0.1_amd64.deb ...\nstdout: Unpacking sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: Setting up sqlite3 (3.37.2-2ubuntu0.1) ...\nstdout: 3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nversion:  3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1\nerror:  \nexit code:  0\n```\n\nWhen your agent is finished, don't forget to close the sandbox\n\n```python\ne2b_data_analysis_tool.close()\n```\n\n## Related\u200b\n\n- Tool [conceptual guide](/docs/concepts/tools/)\n\n- Tool [how-to guides](/docs/how_to/#tools)\n\n#### Was this page helpful?\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "You are currently on a page documenting the use of Google Vertex text completion models. Many Google models are chat completion models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "title": "Google Cloud Vertex AI | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm/", "uuid": "6d16cf1d-801f-5486-93f7-cb3a2153a034"}, "page_content": "[[GenerationChunk(text='## Python: Pros and Cons\\n\\n### Pros:\\n\\n* **Easy to learn:** Python is often cited as one of the easiest programming languages to learn, making it a popular choice for beginners. Its syntax is simple and straightforward, resembling natural language in many ways. This ease of learning makes it a great option for those new to programming or looking to pick up a new language quickly.\\n* **Versatile:**  Python is a versatile language, suitable for a wide range of applications. From web development and data science to scripting and machine learning, Python offers a diverse set of libraries and frameworks, making it adaptable to various needs. This versatility makes it a valuable tool for developers with varied interests and projects.\\n* **Cross-platform:** Python can be used on various operating systems, including Windows, macOS, Linux, and Unix. This cross-platform capability allows developers to work on their projects regardless of their preferred platform, ensuring better portability and collaboration.\\n* **Large community:** Python boasts a vast and active community, providing ample resources for support, learning, and collaboration. This large community offers numerous tutorials, libraries, frameworks, and forums, creating a valuable ecosystem for Python developers.\\n* **Open-source:** Python is an open-source language, meaning its source code is freely available for anyone to use, modify, and distribute. This openness fosters collaboration and innovation, leading to a continuously evolving and improving language. \\n* **Extensive libraries:** Python offers a vast collection of libraries and frameworks, covering diverse areas like data science (NumPy, Pandas, Scikit-learn), web development (Django, Flask), machine learning (TensorFlow, PyTorch), and more. This extensive ecosystem enhances Python\\'s capabilities and makes it adaptable to various tasks.\\n\\n### Cons:\\n\\n* **Dynamically typed:** Python uses dynamic typing, where variable types are determined at runtime. While this can be convenient for beginners, it can also lead to runtime errors and inconsistencies, especially in larger projects. Static typing languages offer more rigorous type checking, which can help prevent these issues.\\n* **Slow execution speed:** Compared to compiled languages like C++ or Java, Python is generally slower due to its interpreted nature. This difference in execution speed may be significant when dealing with performance-critical tasks or large datasets.\\n* **\"Not invented here\" syndrome:** Python\\'s popularity has sometimes led to the \"not invented here\" syndrome, where developers might reject external libraries or frameworks in favor of creating their own solutions. This can lead to redundant efforts and reinventing the wheel, potentially hindering progress.\\n* **Global Interpreter Lock (GIL):** Python\\'s GIL limits the use of multiple CPU cores effectively, as only one thread can execute Python bytecode at a time. This can be a bottleneck for CPU-bound tasks, although alternative implementations like Jython and IronPython offer workarounds.\\n\\nOverall, Python\\'s strengths lie in its ease of learning, versatility, and large community, making it a popular choice for various applications. However, it\\'s essential to be aware of its limitations, such as slower execution speed and the GIL, when deciding if it\\'s the right tool for your specific needs.', generation_info={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 647, 'total_token_count':", "type": "Document"}, {"metadata": {"description": "LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/document_loader_directory/", "title": "How to load documents from a directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/document_loader_directory/", "uuid": "a268cec3-3331-5785-9a42-e4fe0b984d2d"}, "page_content": "```python\nfrom langchain_community.document_loaders import PythonLoader\n\nloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)\n```\n\n**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)\n## Auto-detect file encodings with TextLoader\u200b\n\n`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\n\n```python\npath = \"../../../libs/langchain/tests/unit_tests/examples/\"\n\nloader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\n```\n\n### A. Default Behavior\u200b\n\nBy default we raise an error:\n\n```python\nloader.load()\n```\n\n```output\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\n```output\n---------------------------------------------------------------------------\n``````output\nUnicodeDecodeError                        Traceback (most recent call last)\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)\n     42     with open(self.file_path, encoding=self.encoding) as f:\n---> 43         text = f.read()\n     44 except UnicodeDecodeError as e:\n``````output\nFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    321 data = self.buffer + input\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323 # keep undecoded input until the next call\n``````output\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte\n``````output\n\nThe above exception was the direct cause of the following exception:\n``````output\nRuntimeError                              Traceback (most recent call last)\n``````output\nCell In[10], line 1\n----> 1 loader.load()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)\n    115 def load(self) -> List[Document]:\n    116     \"\"\"Load documents.\"\"\"\n--> 117     return list(self.lazy_load())\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)\n    180 else:\n    181     for i in items:\n--> 182         yield from self._lazy_load_file(i, p, pbar)\n    184 if pbar:\n    185     pbar.close()\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    218     else:\n    219         logger.error(f\"Error loading file {str(item)}\")\n--> 220         raise e\n    221 finally:\n    222     if pbar:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)\n    208 loader = self.loader_cls(str(item), **self.loader_kwargs)\n    209 try:\n--> 210     for subdoc in loader.lazy_load():\n    211         yield subdoc\n    212 except NotImplementedError:\n``````output\nFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)\n     54                 continue\n     55     else:\n---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 except Exception as e:\n     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e\n``````output\nRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\n```\n\nThe file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.\n\nWith the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.\n\n### B. Silent fail\u200b", "type": "Document"}, {"metadata": {"description": "Google Cloud Storage is a managed service for storing unstructured data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "title": "Google Cloud Storage Directory | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/document_loaders/google_cloud_storage_directory/", "uuid": "281f670d-b73d-5541-9900-61af558a7784"}, "page_content": "```python\nloader = GCSDirectoryLoader(\n    project_name=\"aist\", bucket=\"testing-hwc\", continue_on_failure=True\n)\n```\n\n```python\nloader.load()\n```\n\n## Related\u200b\n\n- Document loader [conceptual guide](/docs/concepts/document_loaders/)\n\n- Document loader [how-to guides](/docs/how_to/#document-loaders)\n\n#### Was this page helpful?\n\n- [Specifying a prefix](#specifying-a-prefix)\n\n- [Continue on failure to load a single file](#continue-on-failure-to-load-a-single-file)\n\n- [Related](#related)", "type": "Document"}], "steps": []}
{"output": "respond"}
{"messages": [{"content": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f908b8eb-ef1a-4047-8d0c-e1b06520851c", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "num_response_attempts": 1}
{"generations": [{"text": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f908b8eb-ef1a-4047-8d0c-e1b06520851c", "tool_calls": [], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"messages": [{"content": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-f908b8eb-ef1a-4047-8d0c-e1b06520851c", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "The error message you are encountering, `NameError: name 'openai' is not defined`, indicates that the `openai` module has not been imported in your code. To resolve this issue, you need to import the `openai` library at the beginning of your script. Here\u2019s how you can modify your code:\n\n1. **Import the OpenAI Library**:\n   Add the import statement for the `openai` library at the top of your script.\n\nHere\u2019s the corrected code:\n\n```python\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_neo4j import GraphCypherQAChain, Neo4jGraph\nfrom langchain_openai import ChatOpenAI\nimport openai  # Import the OpenAI library\n\n# Step 1: Load Environment Variables\nload_dotenv()\n\n# Set OpenAI API key from environment variable\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ngraph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=os.getenv('NEO4J_PASSWORD'))\n```\n\n2. **Ensure OpenAI Library is Installed**:\n   If you haven't already installed the OpenAI library, you can do so using pip:\n\n   ```bash\n   pip install openai\n   ```\n\nAfter making these changes, your code should work without the `NameError`. If you encounter any further issues, please let me know!", "num_response_attempts": 1}
{"output": "end"}
{"output": {"binary_score": "yes"}}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_q1g6k272dvhQQrqGzReVa3eq", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-6e19b162-0adc-45a8-85f3-557b9c329660", "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_q1g6k272dvhQQrqGzReVa3eq", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"output": {"binary_score": "yes"}}
