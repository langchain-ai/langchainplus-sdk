{"messages": [{"content": "what should i set as the value of k for search_kwargs such that i want to retrieve all the relevant queires when looking for metadata filters in self query retrieval", "additional_kwargs": {}, "response_metadata": {}, "type": "human", "id": "713822c1-a894-42c3-850a-84d5c1c993d6", "example": false}, {"content": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-581b5b8b-4fee-444d-b8df-01bf3913d89a", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "steps": [], "documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}, {"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}, {"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}], "answer": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "query": "what should i set as the value of k for search_kwargs such that i want to retrieve all the relevant queires when looking for metadata filters in self query retrieval", "num_response_attempts": 1}
{"messages": [{"role": "user", "content": "what should i set as the value of k for search_kwargs such that i want to retrieve all the relevant queires when looking for metadata filters in self query retrieval"}]}
{"steps": ["Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.", "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."], "documents": "delete", "query": "what should i set as the value of k for search_kwargs such that i want to retrieve all the relevant queires when looking for metadata filters in self query retrieval", "num_response_attempts": 0}
{"steps": ["Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.", "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_VmCrO5BDHGYMNu1i6CxSzIN0", "function": {"arguments": "{\"steps\":[\"Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.\",\"Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.\",\"Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values.\"]}", "name": "Plan"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-7c97cf25-f80b-462d-85af-331e017ae8ec", "tool_calls": [{"name": "Plan", "args": {"steps": ["Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.", "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}, "id": "call_VmCrO5BDHGYMNu1i6CxSzIN0", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"steps": ["Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.", "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}
{"steps": ["Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.", "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."], "documents": "delete", "query": "what should i set as the value of k for search_kwargs such that i want to retrieve all the relevant queires when looking for metadata filters in self query retrieval", "num_response_attempts": 0}
{"documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}], "steps": ["Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}
{"question": "Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process.", "queries": ["self-query retrieval k value impact", "how k value influences self-query retrieval", "conceptual documentation on self-query retrieval techniques"], "documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}]}
{"question": "Review the conceptual documentation on self-query retrieval to understand how the k value affects the retrieval process."}
{"queries": ["self-query retrieval k value impact", "how k value influences self-query retrieval", "conceptual documentation on self-query retrieval techniques"]}
{"queries": ["self-query retrieval k value impact", "how k value influences self-query retrieval", "conceptual documentation on self-query retrieval techniques"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_s3EiPB8VgmzcGjzMtta9qovL", "function": {"arguments": "{\"queries\":[\"self-query retrieval k value impact\",\"how k value influences self-query retrieval\",\"conceptual documentation on self-query retrieval techniques\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-10eec39e-f724-44de-8415-677e52a0bdb5", "tool_calls": [{"name": "Response", "args": {"queries": ["self-query retrieval k value impact", "how k value influences self-query retrieval", "conceptual documentation on self-query retrieval techniques"]}, "id": "call_s3EiPB8VgmzcGjzMtta9qovL", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["self-query retrieval k value impact", "how k value influences self-query retrieval", "conceptual documentation on self-query retrieval techniques"]}
{"queries": ["self-query retrieval k value impact", "how k value influences self-query retrieval", "conceptual documentation on self-query retrieval techniques"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='self-query retrieval k value impact'))", "Send(node='retrieve_documents', arg=QueryState(query='how k value influences self-query retrieval'))", "Send(node='retrieve_documents', arg=QueryState(query='conceptual documentation on self-query retrieval techniques'))"]}
{"documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Timescale Vector is PostgreSQL++ for AI applications. It enables you to efficiently store and query billions of vector embeddings in PostgreSQL.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "title": "Timescale Vector (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/timescalevector_self_query/", "uuid": "e996e857-0874-57c1-8dfc-12a59f31fbd6"}, "page_content": "#### Was this page helpful?\n\n- [What is Timescale Vector?](#what-is-timescale-vector)\n\n- [How to access Timescale Vector](#how-to-access-timescale-vector)\n\n- [Creating a TimescaleVector vectorstore](#creating-a-timescalevector-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Self Querying Retrieval with Timescale Vector](#self-querying-retrieval-with-timescale-vector)- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "title": "Elasticsearch | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/elasticsearch_self_query/", "uuid": "723fe8fd-45cc-5c38-8965-fbd9af052703"}, "page_content": "#### Was this page helpful?\n\n- [Creating an Elasticsearch vector store](#creating-an-elasticsearch-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)\n\n- [Complex queries in Action!](#complex-queries-in-action)", "type": "Document"}, {"metadata": {"language": "en", "description": "Learn about how the self-querying retriever works here.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "title": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/", "uuid": "315c889a-0f5a-531c-93d4-6c64c77d8abf"}, "page_content": "Self-querying retrievers | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Self-querying retrievers\n\nLearn about how the self-querying retriever works [here](/docs/how_to/self_query/).", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "For more information on how to setup the SAP HANA vetor store, take a look at the documentation.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/hanavector_self_query/", "uuid": "15a7de56-1822-5bbf-8dcb-59e993b604e4"}, "page_content": "retriever = SelfQueryRetriever.from_llm(\n    llm,\n    db,\n    document_content_description,\n    metadata_field_info,\n    structured_query_translator=hana_translator,\n)\n```\n\n**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [HanaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.hanavector.HanaTranslator.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)\nLet's use this retriever to prepare a (self) query for a person:\n\n```python\nquery_prompt = \"Which person is not active?\"\n\ndocs = retriever.invoke(input=query_prompt)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content, \" \", doc.metadata)\n```\n\nWe can also take a look at how the query is being constructed:\n\n```python\nfrom langchain.chains.query_constructor.base import (\n    StructuredQueryOutputParser,\n    get_query_constructor_prompt,\n)\n\nprompt = get_query_constructor_prompt(\n    document_content_description,\n    metadata_field_info,\n)\noutput_parser = StructuredQueryOutputParser.from_components()\nquery_constructor = prompt | llm | output_parser\n\nsq = query_constructor.invoke(input=query_prompt)\n\nprint(\"Structured query: \", sq)\n\nprint(\"Translated for hana vector store: \", hana_translator.visit_structured_query(sq))\n```\n\n**API Reference:**[StructuredQueryOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html) | [get_query_constructor_prompt](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html)#### Was this page helpful?\n\n- [Self querying](#self-querying)", "type": "Document"}, {"metadata": {"description": "Head to Integrations for documentation on vector stores with built-in support for self-querying.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/self_query/", "title": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/self_query/", "uuid": "d8df7604-f958-5f8d-862c-bf9f0fac47db"}, "page_content": "How to do \"self-querying\" retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to do \"self-querying\" retrieval\n\ninfoHead to [Integrations](/docs/integrations/retrievers/self_query/) for documentation on vector stores with built-in support for self-querying.\n\nA self-querying [retriever](/docs/concepts/retrievers/) is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying [vector store](/docs/concepts/vectorstores/). This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\n\n![](/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)\n\n## Get started\u200b\n\nFor demonstration purposes we'll use a `Chroma` vector store. We've created a small demo set of documents that contain summaries of movies.\n\n**Note:** The self-query retriever requires you to have `lark` package installed.\n\n```python\n%pip install --upgrade --quiet  lark langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\ndocs = [\n    Document(\n        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n    ),\n    Document(\n        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n    ),\n    Document(\n        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n    ),\n    Document(\n        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n    ),\n    Document(\n        page_content=\"Toys come alive and have a blast doing so\",\n        metadata={\"year\": 1995, \"genre\": \"animated\"},\n    ),\n    Document(\n        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n        metadata={\n            \"year\": 1979,\n            \"director\": \"Andrei Tarkovsky\",\n            \"genre\": \"thriller\",\n            \"rating\": 9.9,\n        },\n    ),\n]\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n```\n\n**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n### Creating our self-querying retriever\u200b\n\nNow we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\n\n```python\nfrom langchain.chains.query_constructor.schema import AttributeInfo\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain_openai import ChatOpenAI", "type": "Document"}, {"metadata": {"language": "en", "description": "* Retrievers", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "2b46f509-6d7b-5a60-99b0-4e3df166409f"}, "page_content": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# Retrieval\n\nPrerequisites- [Retrievers](/docs/concepts/retrievers/)\n\n- [Vector stores](/docs/concepts/vectorstores/)\n\n- [Embeddings](/docs/concepts/embedding_models/)\n\n- [Text splitters](/docs/concepts/text_splitters/)\n\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this.\nMake sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\nFor more on general security best practices, see our [security guide](/docs/security/).\n\n## Overview\u200b\n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\nThese systems accommodate various data formats:\n\n- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n\n- Structured data is typically housed in relational or graph databases with defined schemas.\n\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\nThis translation enables more intuitive and flexible interactions with complex data structures.\n\n## Key concepts\u200b\n\n![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)\n\n(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.\n\n(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.\n\n## Query analysis\u200b\n\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n\n1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.\n\n2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).\n\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\n\n### Query re-writing\u200b\n\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\n\n1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.\n\n2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.\n\n3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.\n\n4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.\n\nVarious techniques have been developed to leverage models for query re-writing, including:", "type": "Document"}], "steps": ["Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}], "steps": ["Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}
{"question": "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs.", "queries": ["k value best practices in search_kwargs integration documentation", "recommendations for setting k value in search algorithms", "search_kwargs k value optimization techniques"], "documents": [{"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}]}
{"question": "Check the integration documentation for any specific recommendations or best practices regarding the k value in search_kwargs."}
{"queries": ["k value best practices in search_kwargs integration documentation", "recommendations for setting k value in search algorithms", "search_kwargs k value optimization techniques"]}
{"queries": ["k value best practices in search_kwargs integration documentation", "recommendations for setting k value in search algorithms", "search_kwargs k value optimization techniques"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_ZiGUaEc2tND8MS1B6mYxG0pF", "function": {"arguments": "{\"queries\":[\"k value best practices in search_kwargs integration documentation\",\"recommendations for setting k value in search algorithms\",\"search_kwargs k value optimization techniques\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-d0fda681-a887-4919-aa7e-0f1b46b764ca", "tool_calls": [{"name": "Response", "args": {"queries": ["k value best practices in search_kwargs integration documentation", "recommendations for setting k value in search algorithms", "search_kwargs k value optimization techniques"]}, "id": "call_ZiGUaEc2tND8MS1B6mYxG0pF", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["k value best practices in search_kwargs integration documentation", "recommendations for setting k value in search algorithms", "search_kwargs k value optimization techniques"]}
{"queries": ["k value best practices in search_kwargs integration documentation", "recommendations for setting k value in search algorithms", "search_kwargs k value optimization techniques"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='k value best practices in search_kwargs integration documentation'))", "Send(node='retrieve_documents', arg=QueryState(query='recommendations for setting k value in search algorithms'))", "Send(node='retrieve_documents', arg=QueryState(query='search_kwargs k value optimization techniques'))"]}
{"documents": [{"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a retriever based on the user.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/qa_per_user/", "title": "How to do per-user retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/qa_per_user/", "uuid": "23d81776-489d-5eb8-8131-92cc183fe744"}, "page_content": "We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],\n 'answer': 'The user worked at Kensho.'}\n```\n\n```python\nresult = graph.invoke(\n    {\"question\": \"Where did the user work?\"},\n    config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}},\n)\n\nresult\n```\n\n```output\n{'question': 'Where did the user work?',\n 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],\n 'answer': 'The user worked at Facebook.'}\n```\n\nFor details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).\n\n#### Was this page helpful?\n\n- [Code Example](#code-example)", "type": "Document"}, {"metadata": {"description": "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "title": "How to use a vectorstore as a retriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/vectorstore_retriever/", "uuid": "62c6d7b9-1345-54cc-abc6-08073b9aacb3"}, "page_content": "```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n```\n\n### Specifying top k\u200b\n\nWe can also limit the number of documents `k` returned by the retriever.\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n```\n\n```python\ndocs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\nlen(docs)\n```\n\n```output\n1\n```\n\n#### Was this page helpful?\n\n- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)\n\n- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)\n\n- [Passing search parameters](#passing-search-parameters)- [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)\n\n- [Specifying top k](#specifying-top-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Elasticsearch is a distributed, RESTful search and analytics engine. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It supports keyword search, vector search, hybrid search and complex filtering.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "title": "ElasticsearchRetriever | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/elasticsearch_retriever/", "uuid": "fa564a27-0746-5fc4-9e3c-10db5a06128c"}, "page_content": "vector_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=vector_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nvector_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 1.0, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='world', metadata={'_index': 'test-langchain-index', '_id': '2', '_score': 0.6770179, '_source': {'fake_embedding': [-0.7041151202179595, -1.4652961969276497, -0.25786766898672847], 'num_characters': 5}}),\n Document(page_content='hello world', metadata={'_index': 'test-langchain-index', '_id': '3', '_score': 0.4816144, '_source': {'fake_embedding': [0.42728413221815387, -1.1889908285425348, -1.445433230084671], 'num_characters': 11}}),\n Document(page_content='hello', metadata={'_index': 'test-langchain-index', '_id': '4', '_score': 0.46853775, '_source': {'fake_embedding': [-0.28560441330564046, 0.9958894823084921, 1.5489829880195058], 'num_characters': 5}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.2086992, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}})]\n```\n\n### BM25\u200b\n\nTraditional keyword matching.\n\n```python\ndef bm25_query(search_query: str) -> Dict:\n    return {\n        \"query\": {\n            \"match\": {\n                text_field: search_query,\n            },\n        },\n    }\n\nbm25_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=bm25_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nbm25_retriever.invoke(\"foo\")\n```\n\n```output\n[Document(page_content='foo', metadata={'_index': 'test-langchain-index', '_id': '0', '_score': 0.9711467, '_source': {'fake_embedding': [-2.336764233933763, 0.27510289545940503, -0.7957597268194339], 'num_characters': 3}}),\n Document(page_content='foo bar', metadata={'_index': 'test-langchain-index', '_id': '5', '_score': 0.7437035, '_source': {'fake_embedding': [0.2533670476638539, 0.08100381646160418, 0.7763644080870179], 'num_characters': 7}}),\n Document(page_content='bla bla foo', metadata={'_index': 'test-langchain-index', '_id': '6', '_score': 0.6025789, '_source': {'fake_embedding': [1.7365927060137358, -0.5230400847844948, 0.7978339724186192], 'num_characters': 11}})]\n```\n\n### Hybrid search\u200b\n\nThe combination of vector search and BM25 search using [Reciprocal Rank Fusion](https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html) (RRF) to combine the result sets.\n\n```python\ndef hybrid_query(search_query: str) -> Dict:\n    vector = embeddings.embed_query(search_query)  # same embeddings as for indexing\n    return {\n        \"retriever\": {\n            \"rrf\": {\n                \"retrievers\": [\n                    {\n                        \"standard\": {\n                            \"query\": {\n                                \"match\": {\n                                    text_field: search_query,\n                                }\n                            }\n                        }\n                    },\n                    {\n                        \"knn\": {\n                            \"field\": dense_vector_field,\n                            \"query_vector\": vector,\n                            \"k\": 5,\n                            \"num_candidates\": 10,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\nhybrid_retriever = ElasticsearchRetriever.from_es_params(\n    index_name=index_name,\n    body_func=hybrid_query,\n    content_field=text_field,\n    url=es_url,\n)\n\nhybrid_retriever.invoke(\"foo\")\n```", "type": "Document"}, {"metadata": {"description": "Vector Search introduction and langchain integration guide.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "title": "Amazon MemoryDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/memorydb/", "uuid": "f25faa39-de63-5008-9d58-af5c85d326e8"}, "page_content": "```python\nvds.index_name\n```\n\n```output\n'users'\n```\n\n## Querying\u200b\n\nThere are multiple ways to query the `InMemoryVectorStore`  implementation based on what use case you have:\n\n- `similarity_search`: Find the most similar vectors to a given vector.\n\n- `similarity_search_with_score`: Find the most similar vectors to a given vector and return the vector distance\n\n- `similarity_search_limit_score`: Find the most similar vectors to a given vector and limit the number of results to the `score_threshold`\n\n- `similarity_search_with_relevance_scores`: Find the most similar vectors to a given vector and return the vector similarities\n\n- `max_marginal_relevance_search`: Find the most similar vectors to a given vector while also optimizing for diversity\n\n```python\nresults = vds.similarity_search(\"foo\")\nprint(results[0].page_content)\n```\n\n```output\nfoo\n```\n\n```python\n# with scores (distances)\nresults = vds.similarity_search_with_score(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: bar --- Score: 0.1566\nContent: bar --- Score: 0.1566\n```\n\n```python\n# limit the vector distance that can be returned\nresults = vds.similarity_search_with_score(\"foo\", k=5, distance_threshold=0.1)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Score: {result[1]}\")\n```\n\n```output\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\nContent: foo --- Score: 0.0\n```\n\n```python\n# with scores\nresults = vds.similarity_search_with_relevance_scores(\"foo\", k=5)\nfor result in results:\n    print(f\"Content: {result[0].page_content} --- Similiarity: {result[1]}\")\n```\n\n```output\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: foo --- Similiarity: 1.0\nContent: bar --- Similiarity: 0.8434\nContent: bar --- Similiarity: 0.8434\n```\n\n```python\n# you can also add new documents as follows\nnew_document = [\"baz\"]\nnew_metadata = [{\"user\": \"sam\", \"age\": 50, \"job\": \"janitor\", \"credit_score\": \"high\"}]\n# both the document and metadata must be lists\nvds.add_texts(new_document, new_metadata)\n```\n\n```output\n['doc:users:b9c71d62a0a34241a37950b448dafd38']\n```\n\n## MemoryDB as Retriever\u200b\n\nHere we go over different options for using the vector store as a retriever.\n\nThere are three different search methods we can use to do retrieval. By default, it will use semantic similarity.\n\n```python\nquery = \"foo\"\nresults = vds.similarity_search_with_score(query, k=3, return_metadata=True)\n\nfor result in results:\n    print(\"Content:\", result[0].page_content, \" --- Score: \", result[1])\n```\n\n```output\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\nContent: foo  --- Score:  0.0\n```\n\n```python\nretriever = vds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```\n\n```output\n[Document(page_content='foo', metadata={'id': 'doc:users_modified:988ecca7574048e396756efc0e79aeca', 'user': 'john', 'job': 'engineer', 'credit_score': 'high', 'age': '18'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:009b1afeb4084cc6bdef858c7a99b48e', 'user': 'derrick', 'job': 'doctor', 'credit_score': 'low', 'age': '45'}),\n Document(page_content='foo', metadata={'id': 'doc:users_modified:7087cee9be5b4eca93c30fbdd09a2731', 'user': 'nancy', 'job': 'doctor', 'credit_score': 'high', 'age': '94'}),\n Document(page_content='bar', metadata={'id': 'doc:users_modified:01ef6caac12b42c28ad870aefe574253', 'user': 'tyler', 'job': 'engineer', 'credit_score': 'high', 'age': '100'})]\n```\n\nThere is also the `similarity_distance_threshold` retriever which allows the user to specify the vector distance\n\n```python\nretriever = vds.as_retriever(\n    search_type=\"similarity_distance_threshold\",\n    search_kwargs={\"k\": 4, \"distance_threshold\": 0.1},\n)\n```\n\n```python\ndocs = retriever.invoke(query)\ndocs\n```", "type": "Document"}, {"metadata": {"description": "Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also includes supporting code for evaluation and parameter tuning.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "title": "Faiss (Async) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/faiss_async/", "uuid": "5bc7c265-063b-573e-a08d-7f582283b5c1"}, "page_content": "Same thing can be done with the `max_marginal_relevance_search` as well.\n\n```python\nresults = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\nContent: bar, Metadata: {'page': 1}\n```\n\nHere is an example of how to set `fetch_k` parameter when calling `similarity_search`. Usually you would want the `fetch_k` parameter >> `k` parameter. This is because the `fetch_k` parameter is the number of documents that will be fetched before filtering. If you set `fetch_k` to a low number, you might not get enough documents to filter from.\n\n```python\nresults = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")\n```\n\n```output\nContent: foo, Metadata: {'page': 1}\n```\n\n## Delete\u200b\n\nYou can also delete ids. Note that the ids to delete should be the ids in the docstore.\n\n```python\ndb.delete([db.index_to_docstore_id[0]])\n```\n\n```output\nTrue\n```\n\n```python\n# Is now missing\n0 in db.index_to_docstore_id\n```\n\n```output\nFalse\n```\n\n## Related\u200b\n\n- Vector store [conceptual guide](/docs/concepts/vectorstores/)\n\n- Vector store [how-to guides](/docs/how_to/#vector-stores)\n\n#### Was this page helpful?\n\n- [Similarity Search with score](#similarity-search-with-score)\n\n- [Saving and loading](#saving-and-loading)\n\n- [Merging](#merging)\n\n- [Similarity Search with filtering](#similarity-search-with-filtering)\n\n- [Delete](#delete)\n\n- [Related](#related)", "type": "Document"}, {"metadata": {"description": "Supabase is an open-source Firebase alternative.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "fe6d6651-ca24-5666-b05c-7d9b4e9a27cc"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaurs' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'id': 1183188982475, 'text': 'A bunch of scientists bring back dinosaurs and mayhem breaks loose', 'score': 0.13394928, 'year': {'value': 1993}, 'rating': {'value': 7.7}, 'genre': '\"action\", \"science fiction\"'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.1899159, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a DingoDB index](#creating-a-dingodb-index)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}], "steps": ["Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."]}
{"output": "conduct_research"}
{"documents": [{"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}, {"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Supabase is an open-source Firebase alternative.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}], "steps": []}
{"question": "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values.", "queries": ["how to set up metadata filters in data analysis", "impact of different k values in metadata filtering", "guidelines for configuring metadata filters and k values"], "documents": [{"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}, {"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Supabase is an open-source Firebase alternative.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"question": "Look into any relevant how-to guides that discuss setting up metadata filters and the implications of different k values."}
{"queries": ["how to set up metadata filters in data analysis", "impact of different k values in metadata filtering", "guidelines for configuring metadata filters and k values"]}
{"queries": ["how to set up metadata filters in data analysis", "impact of different k values in metadata filtering", "guidelines for configuring metadata filters and k values"]}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_vkFDZcslYp0pdFlPrsmv4a2n", "function": {"arguments": "{\"queries\":[\"how to set up metadata filters in data analysis\",\"impact of different k values in metadata filtering\",\"guidelines for configuring metadata filters and k values\"]}", "name": "Response"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-980555c0-e300-462b-a693-5ea3a6f920a7", "tool_calls": [{"name": "Response", "args": {"queries": ["how to set up metadata filters in data analysis", "impact of different k values in metadata filtering", "guidelines for configuring metadata filters and k values"]}, "id": "call_vkFDZcslYp0pdFlPrsmv4a2n", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"queries": ["how to set up metadata filters in data analysis", "impact of different k values in metadata filtering", "guidelines for configuring metadata filters and k values"]}
{"queries": ["how to set up metadata filters in data analysis", "impact of different k values in metadata filtering", "guidelines for configuring metadata filters and k values"]}
{"output": ["Send(node='retrieve_documents', arg=QueryState(query='how to set up metadata filters in data analysis'))", "Send(node='retrieve_documents', arg=QueryState(query='impact of different k values in metadata filtering'))", "Send(node='retrieve_documents', arg=QueryState(query='guidelines for configuring metadata filters and k values'))"]}
{"documents": [{"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}]}
{"documents": [{"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Supabase is an open-source Firebase alternative.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Supabase is an open-source Firebase alternative.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"language": "en", "description": "Supabase is an open-source Firebase alternative.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}]}
{"documents": [{"metadata": {"language": "en", "description": "TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution, that provides dedicated and serverless options. TiDB Serverless is now integrating a built-in vector search into the MySQL landscape. With this enhancement, you can seamlessly develop AI applications using TiDB Serverless without the need for a new database or additional technical stacks. Create a free TiDB Serverless cluster and start using the vector search feature at https://pingcap.com/ai.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "title": "TiDB Vector | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/tidb_vector/", "uuid": "0eee9b07-67d6-5375-acb9-80ddbc2d66be"}, "page_content": "```output\n--------------------------------------------------------------------------------\nScore:  0.8154069850178\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you\u2019re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I\u2019d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer\u2014an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation\u2019s top legal minds, who will continue Justice Breyer\u2019s legacy of excellence.\n--------------------------------------------------------------------------------\n--------------------------------------------------------------------------------\nScore:  0.7827270056715364\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she\u2019s been nominated, she\u2019s received a broad range of support\u2014from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we\u2019ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe\u2019ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe\u2019re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe\u2019re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n--------------------------------------------------------------------------------\n```\n\n# Filter with metadata\n\nperform searches using metadata filters to retrieve a specific number of nearest-neighbor results that align with the applied filters.\n\n## Supported metadata types\u200b\n\nEach vector in the TiDB Vector Store can be paired with metadata, structured as key-value pairs within a JSON object. The keys are strings, and the values can be of the following types:\n\n- String\n\n- Number (integer or floating point)\n\n- Booleans (true, false)\n\nFor instance, consider the following valid metadata payloads:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\n## Metadata filter syntax\u200b\n\nThe available filters include:\n\n- $or - Selects vectors that meet any one of the given conditions.\n\n- $and - Selects vectors that meet all of the given conditions.\n\n- $eq - Equal to\n\n- $ne - Not equal to\n\n- $gt - Greater than\n\n- $gte - Greater than or equal to\n\n- $lt - Less than\n\n- $lte - Less than or equal to\n\n- $in - In array\n\n- $nin - Not in array\n\nAssuming one vector with metada:\n\n```json\n{\n    \"page\": 12,\n    \"book_tile\": \"Siddhartha\"\n}\n```\n\nThe following metadata filters will match the vector\n\n```json\n{\"page\": 12}\n\n{\"page\":{\"$eq\": 12}}\n\n{\"page\":{\"$in\": [11, 12, 13]}}\n\n{\"page\":{\"$nin\": [13]}}\n\n{\"page\":{\"$lt\": 11}}\n\n{\n    \"$or\": [{\"page\": 11}, {\"page\": 12}],\n    \"$and\": [{\"page\": 12}, {\"page\": 13}],\n}\n```\n\nPlease note that each key-value pair in the metadata filters is treated as a separate filter clause, and these clauses are combined using the AND logical operator.\n\n```python\ndb.add_texts(\n    texts=[\n        \"TiDB Vector offers advanced, high-speed vector processing capabilities, enhancing AI workflows with efficient data handling and analytics support.\",\n        \"TiDB Vector, starting as low as $10 per month for basic usage\",\n    ],\n    metadatas=[\n        {\"title\": \"TiDB Vector functionality\"},\n        {\"title\": \"TiDB Vector Pricing\"},\n    ],\n)\n```", "type": "Document"}, {"metadata": {"description": "Before diving into this content, it might be helpful to read the following to gain familiarity with the concepts mentioned here:", "language": "en", "changefreq": null, "source": "https://docs.smith.langchain.com/observability/how_to_guides/monitoring/filter_traces_in_application", "title": "Filter traces in the application | \ud83e\udd9c\ufe0f\ud83d\udee0\ufe0f LangSmith", "content_type": "text/html; charset=utf-8", "lastmod": null, "priority": null, "loc": null, "uuid": "ab0f2f7e-f063-5905-8624-bbd7d4700c51"}, "page_content": "Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs.\nFilter based on input / output key-value pairs\u200b\nIn addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.\nTo filter based on key-value pairs, select the Input KV or Output KV filter from the Filters dropdown.\nFor example, to match the following input:\n{  \"input\": \"What is the capital of France?\"}\nSelect Filters, Add Filter to bring up the filtering options. Then select Input KV, enter input as the key and enter What is the capital of France? as the value.\n\nYou can also match nested keys by using dot notation to selected the nested key name. For example, to match nested keys in the output:\n{  \"documents\": [    {      \"page_content\": \"The capital of France is Paris\",      \"metadata\": {},      \"type\": \"Document\"    }  ]}\nSelect Output KV, enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value.\n\nYou can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key values pairs as shown below:\n\nSaved filters\u200b\nYou can save filters to be reused in the future. Saved filters are associated with a trace project not across all trace projects. They can help you organize your traces and find relevant traces more easily.\nSave a filter\u200b\nIn the filter box, click the Save button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.\n\nUse a saved filter\u200b\nAfter saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than 3 filters saved, only 2 will be displayed while the rest are in the more menu in the same bar.\n\nUpdate a saved filter\u200b\nWith the filter selected, make any changes to filter parameters. Then click Save \u25bc \u2192 Save to update the filter.\nIn the same menu, you can also create a new saved filter by clicking Save \u25bc \u2192 Save as.\nDelete a saved filter\u200b\nWith the filter selected, click on the trash button to delete the saved filter.\nCopy the filter\u200b\nSometimes you may want to copy a filter that you have constructed. You may way to do this to share it with a co-worker, reuse it in the future, or use it in the SDK.\nIn order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.\nThis will give you a string in our query language, like and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) Please see this reference for more information on the query language.\n\nFiltering runs within the trace view\u200b\nYou can also filter runs in the trace view. This will allow you to easily sift through traces with large amount of runs. The same filters available in the main runs table view can be applied here.\nBy default, only the runs that match the filters will be shown. To see the matched runs within the context of the trace tree, switch the view option from \"Filtered Only\" to \"Show All\" or \"Most relevant\".\n\nManually specify a raw query in LangSmith query language\u200b\nIf you have copied a previous filter (see above) you may want to manually specify that raw query in a future session. You may also find it easier to modify this filter than to use the UI.\nIn order to do this, you can click on Advanced filters on the bottom. From there you can paste a raw query into the appropriate box.\nNote that this will add that query to the existing queries, not overwrite it.", "type": "Document"}, {"metadata": {"language": "en", "description": "We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "title": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/how_to/query_constructing_filters/", "uuid": "8397d33f-68f7-5f0e-b11b-06d47eea4165"}, "page_content": "How to construct filters for query analysis | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain\n\n[Skip to main content](#__docusaurus_skipToContent_fallback)# How to construct filters for query analysis\n\nWe may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.\n\nThis can be done manually, but LangChain also provides some \"Translators\" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.\n\n```python\nfrom typing import Optional\n\nfrom langchain.chains.query_constructor.ir import (\n    Comparator,\n    Comparison,\n    Operation,\n    Operator,\n    StructuredQuery,\n)\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\nfrom langchain_community.query_constructors.elasticsearch import ElasticsearchTranslator\nfrom pydantic import BaseModel\n```\n\n**API Reference:**[Comparator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparator.html) | [Comparison](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Comparison.html) | [Operation](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operation.html) | [Operator](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.Operator.html) | [StructuredQuery](https://python.langchain.com/api_reference/core/structured_query/langchain_core.structured_query.StructuredQuery.html) | [ChromaTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.chroma.ChromaTranslator.html) | [ElasticsearchTranslator](https://python.langchain.com/api_reference/community/query_constructors/langchain_community.query_constructors.elasticsearch.ElasticsearchTranslator.html)\nIn this example, `year` and `author` are both attributes to filter on.\n\n```python\nclass Search(BaseModel):\n    query: str\n    start_year: Optional[int]\n    author: Optional[str]\n```\n\n```python\nsearch_query = Search(query=\"RAG\", start_year=2022, author=\"LangChain\")\n```\n\n```python\ndef construct_comparisons(query: Search):\n    comparisons = []\n    if query.start_year is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.GT,\n                attribute=\"start_year\",\n                value=query.start_year,\n            )\n        )\n    if query.author is not None:\n        comparisons.append(\n            Comparison(\n                comparator=Comparator.EQ,\n                attribute=\"author\",\n                value=query.author,\n            )\n        )\n    return comparisons\n```\n\n```python\ncomparisons = construct_comparisons(search_query)\n```\n\n```python\n_filter = Operation(operator=Operator.AND, arguments=comparisons)\n```\n\n```python\nElasticsearchTranslator().visit_operation(_filter)\n```\n\n```output\n{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},\n   {'term': {'metadata.author.keyword': 'LangChain'}}]}}\n```\n\n```python\nChromaTranslator().visit_operation(_filter)\n```\n\n```output\n{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}\n```\n\n#### Was this page helpful?", "type": "Document"}, {"metadata": {"description": "* Retrievers", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/retrieval/", "title": "Retrieval | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/retrieval/", "uuid": "530f03c4-e1bc-51d0-9856-17862d922e5f"}, "page_content": "- **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.\n\n- **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n\n2. \n**Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.\n\n- **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).\n\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n\n| Name | When to Use | Description |\n| ---- | ---- | ---- |\n| Self Query | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |\n| Text to SQL | If users are asking questions that require information housed in a relational database, accessible via SQL. | This uses an LLM to transform user input into a SQL query. |\n| Text-to-Cypher | If users are asking questions that require information housed in a graph database, accessible via Cypher. | This uses an LLM to transform user input into a Cypher query. |\n\nAs an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.\n\n```python\nmetadata_field_info = schema_for_metadata \ndocument_content_description = \"Brief summary of a movie\"\nllm = ChatOpenAI(temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n)\n```\n\nFurther reading- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).\n\n- See our [blog post overview](https://blog.langchain.dev/query-construction/).\n\n- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).\n\n## Information retrieval\u200b\n\n### Common retrieval systems\u200b\n\n#### Lexical search indexes\u200b\n\nMany search engines are based upon matching words in a query to the words in each document.\nThis approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nThe intution is simple: a word appears frequently both in the user\u2019s query and a particular document, then this document might be a good match.\n\nThe particular data structure used to implement this is often an [inverted index](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\nFurther reading- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n\n- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n#### Vector indexes\u200b", "type": "Document"}, {"metadata": {"description": "SAP HANA Cloud Vector Engine is a vector store fully integrated into the SAP HANA Cloud database.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "title": "SAP HANA Cloud Vector Engine | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/", "uuid": "cdbeb48b-39c7-5086-a156-dd39b8c27261"}, "page_content": "query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = db.similarity_search(query, k=2)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n**API Reference:**[DistanceStrategy](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.utils.DistanceStrategy.html)\n## Maximal Marginal Relevance Search (MMR)\u200b\n\n`Maximal marginal relevance` optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n\n```python\ndocs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n```\n\n## Basic Vectorstore Operations\u200b\n\n```python\ndb = HanaDB(\n    connection=connection, embedding=embeddings, table_name=\"LANGCHAIN_DEMO_BASIC\"\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\n```\n\nWe can add simple text documents to the existing table.\n\n```python\ndocs = [Document(page_content=\"Some text\"), Document(page_content=\"Other docs\")]\ndb.add_documents(docs)\n```\n\nAdd documents with metadata.\n\n```python\ndocs = [\n    Document(\n        page_content=\"foo\",\n        metadata={\"start\": 100, \"end\": 150, \"doc_name\": \"foo.txt\", \"quality\": \"bad\"},\n    ),\n    Document(\n        page_content=\"bar\",\n        metadata={\"start\": 200, \"end\": 250, \"doc_name\": \"bar.txt\", \"quality\": \"good\"},\n    ),\n]\ndb.add_documents(docs)\n```\n\nQuery documents with specific metadata.\n\n```python\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\n# With filtering on \"quality\"==\"bad\", only one document should be returned\nfor doc in docs:\n    print(\"-\" * 80)\n    print(doc.page_content)\n    print(doc.metadata)\n```\n\nDelete documents with specific metadata.\n\n```python\ndb.delete(filter={\"quality\": \"bad\"})\n\n# Now the similarity search with the same filter will return no results\ndocs = db.similarity_search(\"foobar\", k=2, filter={\"quality\": \"bad\"})\nprint(len(docs))\n```\n\n## Advanced filtering\u200b\n\nIn addition to the basic value-based filtering capabilities, it is possible to use more advanced filtering.\nThe table below shows the available filter operators.\n\n| Operator | Semantic |\n| ---- | ---- |\n| $eq | Equality (==) |\n| $ne | Inequality (!=) |\n| $lt | Less than (<) |\n| $lte | Less than or equal (<=) |\n| $gt | Greater than (>) |\n| $gte | Greater than or equal (>=) |\n| $in | Contained in a set of given values  (in) |\n| $nin | Not contained in a set of given values  (not in) |\n| $between | Between the range of two boundary values |\n| $like | Text equality based on the \"LIKE\" semantics in SQL (using \"%\" as wildcard) |\n| $and | Logical \"and\", supporting 2 or more operands |\n| $or | Logical \"or\", supporting 2 or more operands |\n\n```python\n# Prepare some test documents\ndocs = [\n    Document(\n        page_content=\"First\",\n        metadata={\"name\": \"adam\", \"is_active\": True, \"id\": 1, \"height\": 10.0},\n    ),\n    Document(\n        page_content=\"Second\",\n        metadata={\"name\": \"bob\", \"is_active\": False, \"id\": 2, \"height\": 5.7},\n    ),\n    Document(\n        page_content=\"Third\",\n        metadata={\"name\": \"jane\", \"is_active\": True, \"id\": 3, \"height\": 2.4},\n    ),\n]\n\ndb = HanaDB(\n    connection=connection,\n    embedding=embeddings,\n    table_name=\"LANGCHAIN_DEMO_ADVANCED_FILTER\",\n)\n\n# Delete already existing documents from the table\ndb.delete(filter={})\ndb.add_documents(docs)\n\n# Helper function for printing filter results\ndef print_filter_result(result):\n    if len(result) == 0:\n        print(\"<empty result>\")\n    for doc in result:\n        print(doc.metadata)\n```\n\nFiltering with `$ne`, `$gt`, `$gte`, `$lt`, `$lte`\n\n```python\nadvanced_filter = {\"id\": {\"$ne\": 1}}\nprint(f\"Filter: {advanced_filter}\")\nprint_filter_result(db.similarity_search(\"just testing\", k=5, filter=advanced_filter))", "type": "Document"}, {"metadata": {"language": "en", "description": "* Embeddings", "changefreq": "weekly", "source": "https://python.langchain.com/docs/concepts/vectorstores/", "title": "Vector stores | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "lastmod": null, "content_type": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/concepts/vectorstores/", "uuid": "cc5a4723-034b-54a0-bed6-2b3f90ac4d39"}, "page_content": "### Metadata filtering\u200b\n\nWhile vectorstore implement a search algorithm to efficiently search over _all_ the embedded documents to find the most similar ones, many also support filtering on metadata.\nThis allows structured filters to reduce the size of the similarity search space. These two concepts work well together:\n\n1. **Semantic search**: Query the unstructured data directly, often using via embedding or keyword similarity.\n\n2. **Metadata search**: Apply structured query to the metadata, filering specific documents.\n\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n\nHere is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.\n\n```python\nvectorstore.similarity_search(\n    \"LangChain provides abstractions to make working with LLMs easy\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n```\n\nFurther reading- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.\n\n- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.\n\n## Advanced search and retrieval techniques\u200b\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches.\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.\n\n| Name | When to use | Description |\n| ---- | ---- | ---- |\n| Hybrid search | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Paper. |\n| Maximal Marginal Relevance (MMR) | When needing to diversify search results. | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents. |\n\n#### Was this page helpful?\n\n- [Overview](#overview)\n\n- [Integrations](#integrations)\n\n- [Interface](#interface)\n\n- [Initialization](#initialization)\n\n- [Adding documents](#adding-documents)\n\n- [Delete](#delete)\n\n- [Search](#search)- [Similarity metrics](#similarity-metrics)\n\n- [Similarity search](#similarity-search)\n\n- [Metadata filtering](#metadata-filtering)\n\n- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)", "type": "Document"}, {"metadata": {"description": "DingoDB is a distributed multi-mode vector database, which combines the characteristics of data lakes and vector databases, and can store data of any type and size (Key-Value, PDF, audio, video, etc.). It has real-time low-latency processing capabilities to achieve rapid insight and response, and can efficiently conduct instant analysis and process multi-modal data.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "title": "DingoDB | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/dingo/", "uuid": "193b1aeb-b676-5e0d-acdc-1eb3c5677295"}, "page_content": "```output\nquery=' ' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.25033575, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'genre': '\"science fiction\", \"thriller\"', 'rating': {'value': 9.9}}),\n Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.26431882, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}})]\n```\n\n```python\n# This example specifies a query and a filter\nretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\n```\n\n```output\nquery='women' filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Greta Gerwig') limit=None\ncomparator=<Comparator.EQ: 'eq'> attribute='director' value='Greta Gerwig'\n```\n\n```output\n[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'id': 1183189172623, 'text': 'A bunch of normal-sized women are supremely wholesome and some men pine after them', 'score': 0.19482517, 'year': {'value': 2019}, 'director': 'Greta Gerwig', 'rating': {'value': 8.3}})]\n```\n\n```python\n# This example specifies a composite filter\nretriever.invoke(\"What's a highly rated (above 8.5) science fiction film?\")\n```\n\n```output\nquery='science fiction' filter=Comparison(comparator=<Comparator.GT: 'gt'>, attribute='rating', value=8.5) limit=None\ncomparator=<Comparator.GT: 'gt'> attribute='rating' value=8.5\n```\n\n```output\n[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'id': 1183189148854, 'text': 'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', 'score': 0.19805312, 'year': {'value': 2006}, 'director': 'Satoshi Kon', 'rating': {'value': 8.6}}),\n Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'id': 1183189220159, 'text': 'Three men walk into the Zone, three men walk out of the Zone', 'score': 0.225586, 'year': {'value': 1979}, 'director': 'Andrei Tarkovsky', 'rating': {'value': 9.9}, 'genre': '\"science fiction\", \"thriller\"'})]\n```\n\n```python\n# This example specifies a query and composite filter\nretriever.invoke(\n    \"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated\"\n)\n```\n\n```output\nquery='toys' filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]) limit=None\noperator=<Operator.AND: 'and'> arguments=[Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GT: 'gt'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2005)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='animated')]\n```\n\n```output\n[Document(page_content='Toys come alive and have a blast doing so', metadata={'id': 1183189196391, 'text': 'Toys come alive and have a blast doing so', 'score': 0.133829, 'year': {'value': 1995}, 'genre': 'animated'})]\n```\n\n## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.", "type": "Document"}, {"metadata": {"description": "Milvus is a database that stores, indexes, and manages massive embedding vectors generated by deep neural networks and other machine learning (ML) models.", "language": "en", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "title": "Milvus | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/milvus_self_query/", "uuid": "0360b0d8-65a1-5602-9631-7c76b07e7809"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vector_store,\n    document_content_description,\n    metadata_field_info,\n    verbose=True,\n    enable_limit=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"What are two movies about dinosaurs?\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'action'}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'rating': 9.3, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Creating a Milvus vectorstore](#creating-a-milvus-vectorstore)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}, {"metadata": {"language": "en", "description": "Supabase is an open-source Firebase alternative.", "changefreq": "weekly", "source": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "title": "Supabase (Postgres) | \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain", "content_type": null, "lastmod": null, "priority": "0.5", "loc": "https://python.langchain.com/docs/integrations/retrievers/self_query/supabase_self_query/", "uuid": "00739de2-6b1d-517a-8020-0791ce7668f0"}, "page_content": "## Filter k\u200b\n\nWe can also use the self query retriever to specify `k`: the number of documents to fetch.\n\nWe can do this by passing `enable_limit=True` to the constructor.\n\n```python\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectorstore,\n    document_content_description,\n    metadata_field_info,\n    enable_limit=True,\n    verbose=True,\n)\n```\n\n```python\n# This example only specifies a relevant query\nretriever.invoke(\"what are two movies about dinosaurs\")\n```\n\n```output\nquery='dinosaur' filter=None limit=2\n```\n\n```output\n[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'genre': 'science fiction', 'rating': 7.7}),\n Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'})]\n```\n\n#### Was this page helpful?\n\n- [Setup Supabase Database](#setup-supabase-database)\n\n- [Creating a Supabase vector store](#creating-a-supabase-vector-store)\n\n- [Creating our self-querying retriever](#creating-our-self-querying-retriever)\n\n- [Testing it out](#testing-it-out)\n\n- [Filter k](#filter-k)", "type": "Document"}], "steps": []}
{"output": "respond"}
{"messages": [{"content": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-581b5b8b-4fee-444d-b8df-01bf3913d89a", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "num_response_attempts": 1}
{"generations": [{"text": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-581b5b8b-4fee-444d-b8df-01bf3913d89a", "tool_calls": [], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"messages": [{"content": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "additional_kwargs": {}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-581b5b8b-4fee-444d-b8df-01bf3913d89a", "example": false, "tool_calls": [], "invalid_tool_calls": []}], "answer": "To retrieve all relevant queries when looking for metadata filters in self-query retrieval, you should set the value of `k` to a sufficiently large number that exceeds the expected number of relevant documents. However, if you want to ensure that you retrieve all possible relevant documents without a specific limit, you can set `k` to a very high value or use a method that does not impose a limit on the number of results returned.\n\nFor example, you can specify `k` like this:\n\n```python\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 1000})  # or any large number\n```\n\nThis approach will help ensure that you capture all relevant documents based on your query.", "num_response_attempts": 1}
{"output": "end"}
{"output": {"binary_score": "yes"}}
{"generations": [{"text": "", "generation_info": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ChatGeneration", "message": {"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "AIMessage"], "kwargs": {"content": "", "additional_kwargs": {"tool_calls": [{"index": 0, "id": "call_Sp0Xtd2Sh4gt8UVds9V3avAR", "function": {"arguments": "{\"binary_score\":\"yes\"}", "name": "GradeHallucinations"}, "type": "function"}]}, "response_metadata": {"finish_reason": "stop", "model_name": "gpt-4o-mini-2024-07-18", "system_fingerprint": "fp_0705bf87c0"}, "type": "ai", "id": "run-fdce58eb-76fc-4fa2-956a-a7a0f35464b3", "tool_calls": [{"name": "GradeHallucinations", "args": {"binary_score": "yes"}, "id": "call_Sp0Xtd2Sh4gt8UVds9V3avAR", "type": "tool_call"}], "invalid_tool_calls": []}}}], "llm_output": null, "run": null, "type": "LLMResult"}
{"output": {"binary_score": "yes"}}
